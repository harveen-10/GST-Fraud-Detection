# -*- coding: utf-8 -*-
"""B2C_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dXvFMhjMWgz0SqdhxdLTAN93Auw6HOzD

## Steps for preprocessing

Calculate the sum of taxable_value for the sales data and the sum of deposit_amount for the bank data.

Merge the two aggregated DataFrames on the company_id.

Create the discrepancy column by subtracting the total deposit_amount from the total taxable_value for each company.
"""

# connecting my drive to this google colab nb

from google.colab import drive
drive.mount('/content/drive')

# Getting the deposit amount for the B2C sales for each company from the Bank Statements


import pandas as pd
import os
from google.colab import drive


bank_statements_folder = '/content/drive/My Drive/Capstone/B2C/bank_statements1_preprocessed'
# IMPORTANT: Update this path to your B2C Sales folder
b2c_sales_folder = '/content/drive/My Drive/Capstone/B2C/b2c_sales_fraud_cleaned'


# Processing Bank Statements
bank_data_list = []
print(f"Processing files in: {bank_statements_folder}")
try:
    for filename in os.listdir(bank_statements_folder):
        if filename.endswith('.csv'):
            file_path = os.path.join(bank_statements_folder, filename)
            company_id = filename.split('_')[0]
            try:
                df = pd.read_csv(file_path)
                b2c_transactions = df[~df['Narration'].astype(str).str.contains('(B2B)', regex=False)]
                total_b2c_deposit = b2c_transactions['Deposit Amt.'].sum()
                bank_data_list.append({'company_id': company_id, 'Deposit Amount': total_b2c_deposit})
            except Exception as e:
                print(f"Could not process bank file {filename}. Error: {e}")
except FileNotFoundError:
    print(f"Error: The folder path '{bank_statements_folder}' was not found.")

deposits_df = pd.DataFrame(bank_data_list)


# Processing B2C Sales Files
sales_data_list = []
print(f"\nProcessing files in: {b2c_sales_folder}")
try:
    for filename in os.listdir(b2c_sales_folder):
        if filename.endswith('.csv'):
            file_path = os.path.join(b2c_sales_folder, filename)
            company_id = filename.split('_')[0]
            try:
                df = pd.read_csv(file_path)
                total_taxable_value = df['Taxable Value'].sum()
                sales_data_list.append({'company_id': company_id, 'Taxable Value': total_taxable_value})
            except Exception as e:
                print(f"Could not process sales file {filename}. Error: {e}")
except FileNotFoundError:
    print(f"Error: The folder path '{b2c_sales_folder}' was not found.")

taxable_value_df = pd.DataFrame(sales_data_list)


# Finalization: Merge Datasets and Save
if not deposits_df.empty and not taxable_value_df.empty:
    final_dataset = pd.merge(deposits_df, taxable_value_df, on='company_id', how='outer')

    # Handle missing values after merge
    # If a company is in one dataset but not the other, fill the missing value with 0.
    final_dataset['Taxable Value'] = final_dataset['Taxable Value'].fillna(0)
    final_dataset['Deposit Amount'] = final_dataset['Deposit Amount'].fillna(0)

    # Calculate the Discrepancy column
    final_dataset['Discrepancy'] = final_dataset['Deposit Amount'] - final_dataset['Taxable Value']

    # Round tiny floating point discrepancies to zero
    # Any discrepancy with an absolute value less than 1e-6 will be set to 0.
    final_dataset.loc[final_dataset['Discrepancy'].abs() < 1e-6, 'Discrepancy'] = 0

    print("\n--- Combined B2C Dataset ---")
    print(final_dataset.head())

    # Save your new dataset to a CSV file
    output_path = '/content/drive/My Drive/Capstone/B2C/b2c_preprocessed.csv'
    final_dataset.to_csv(output_path, index=False)
    print(f"\nDataset saved to: {output_path}")

else:
    print("\nCould not create final dataset. One or both of the source folders were empty or could not be processed.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, precision_recall_curve,
    average_precision_score, confusion_matrix
)

"""# 1. Load Data

"""

import os

dataset_path = '/content/drive/MyDrive/Capstone/B2C'
os.listdir(dataset_path)

DATA_PATH = "/content/drive/My Drive/Capstone/B2C/b2c_preprocessed.csv"
df = pd.read_csv(DATA_PATH)
print("Data shape:", df.shape)

"""# 2. Use 'Discrepancy' column as ground truth (binary)

"""

if "Discrepancy" not in df.columns:
    raise ValueError("Expected 'Discrepancy' column not found in dataset!")

# Convert discrepancy into binary labels: 0 = normal, >0 = anomaly
y = (df["Discrepancy"] > 0).astype(int)

n_fraud = y.sum()
n_total = len(y)
fraud_pct = n_fraud / n_total
contamination = fraud_pct  # for Isolation Forest

print(f"\nNumber of fraud companies: {n_fraud}")
print(f"Total companies: {n_total}")
print(f"Fraud percentage: {fraud_pct:.2%}")
print("Discrepancy label distribution:\n", y.value_counts())

"""# 3. Feature preparation

"""

# Feature engineering
df['Log_Deposit'] = np.log1p(df['Deposit Amount'])
df['Log_Taxable'] = np.log1p(df['Taxable Value'])
df['Discrepancy_to_Deposit'] = np.where(
    df['Deposit Amount'] > 0,
    df['Discrepancy'] / df['Deposit Amount'],
    0
)
# Features for Isolation Forest (exclude target)
features = df[['Deposit Amount', 'Taxable Value', 'Log_Deposit', 'Log_Taxable', 'Discrepancy_to_Deposit']]

# 4. Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(features)

"""# 5. Train Isolation Forest"""

model = IsolationForest(
    n_estimators=200,
    max_samples="auto",
    contamination=contamination,
    random_state=42
    # bootstrap=True
)
model.fit(X_scaled)

"""# 6. Predict & Score"""

pred_raw = model.predict(X_scaled)        # -1 = anomaly, 1 = normal
y_pred = (pred_raw == -1).astype(int)   # 1 = anomaly
scores = -model.score_samples(X_scaled)

df_results = df.copy()
df_results["predicted_anomaly"] = y_pred
df_results["anomaly_score"] = scores

"""# 7. Evaluation"""

acc = accuracy_score(y, y_pred)
prec = precision_score(y, y_pred, zero_division=0)
rec = recall_score(y, y_pred, zero_division=0)
f1 = f1_score(y, y_pred, zero_division=0)

print("\nConfusion Matrix:\n", confusion_matrix(y, y_pred))
print(f"Accuracy: {acc:.2f}")
print(f"Precision: {prec:.2f}")
print(f"Recall: {rec:.2f}")
print(f"F1 Score: {f1:.2f}")

# Precision-Recall Curve
prec_vals, rec_vals, _ = precision_recall_curve(y, scores)
plt.figure(figsize=(6,5))
plt.plot(rec_vals, prec_vals, label=f"AP = {ap:.3f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.grid(True)
plt.show()

"""# 8. Display anomalous companies


"""

anomalous_companies = df_results[df_results["predicted_anomaly"] == 1]
print("\nTotal anomalous companies detected:", len(anomalous_companies))

pd.set_option("display.max_rows", None)
pd.set_option("display.max_columns", None)
print("\nAnomalous Companies:\n")
print(anomalous_companies)

# 8. Display anomalous companies

anomalous_companies = df_results[df_results["predicted_anomaly"] == 1].copy()

# Add Risk Level
anomalous_companies["Risk_Level"] = anomalous_companies["Taxable Value"].apply(
    lambda x: "Urgent" if x == 0 else "Investigate"
)

print("\nTotal anomalous companies detected:", len(anomalous_companies))


print(f"\n--- The model detected the following {len(anomalous_companies)} companies as anomalous: ---")
display(anomalous_companies[[
    "company_id", "Deposit Amount", "Taxable Value", "Discrepancy",
    "anomaly_score", "Risk_Level"
]])

# Trying other anomaly detection models and checking their performance metrics

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, precision_recall_curve,
    average_precision_score, confusion_matrix
)
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.covariance import EllipticEnvelope


# 1. Load Data
DATA_PATH = "/content/drive/MyDrive/Capstone/B2C/b2c_preprocessed.csv"
try:
    df = pd.read_csv(DATA_PATH)
    print("Data shape:", df.shape)
except FileNotFoundError:
    print(f"File not found at {DATA_PATH}. Please check the path.")
    # Fallback dummy data for execution
    np.random.seed(42)
    df = pd.DataFrame({
        'Deposit Amount': np.random.exponential(1000, 1000),
        'Taxable Value': np.random.exponential(900, 1000),
        'Discrepancy': np.concatenate([np.zeros(950), np.random.exponential(500, 50)])
    })


# 2. Ground truth
y = (df["Discrepancy"] > 0).astype(int)
n_fraud = y.sum()
n_total = len(y)
contamination = n_fraud / n_total

print(f"\nNumber of fraud companies: {n_fraud}")
print(f"Total companies: {n_total}")
print(f"Fraud percentage: {contamination:.2%}")


# 3. Feature preparation
df['Log_Deposit'] = np.log1p(df['Deposit Amount'])
df['Log_Taxable'] = np.log1p(df['Taxable Value'])
df['Discrepancy_to_Deposit'] = np.where(
    df['Deposit Amount'] > 0,
    df['Discrepancy'] / df['Deposit Amount'],
    0
)
features = df[['Deposit Amount', 'Taxable Value', 'Log_Deposit', 'Log_Taxable', 'Discrepancy_to_Deposit']]


# 4. Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(features)


# 5. Define all 4 models
models = {
    "Isolation Forest": IsolationForest(
        contamination=contamination, random_state=42
    ),
    "Local Outlier Factor": LocalOutlierFactor(
        n_neighbors=50, contamination=contamination, novelty=True
    ),
    "One-Class SVM": OneClassSVM(
        kernel='rbf', gamma='scale', nu=contamination
    ),
    "Elliptic Envelope": EllipticEnvelope(
        contamination=contamination, random_state=42
    )
}


# 6. Train, predict & evaluate
# Dictionary to store data for the final plot
plot_data = {}

for name, model in models.items():
    print(f"\n--- {name} ---")

    model.fit(X_scaled)

    # Calculate Scores

    # We negate scores so that higher values = more anomalous (fraud)
    if name == "Local Outlier Factor":
        pred_raw = model.predict(X_scaled)
        scores = -model.decision_function(X_scaled)
    elif name == "One-Class SVM" or name == "Elliptic Envelope":
        pred_raw = model.predict(X_scaled)
        scores = -model.score_samples(X_scaled)
    else:  # Isolation Forest
        pred_raw = model.predict(X_scaled)
        scores = -model.score_samples(X_scaled)

    # Convert predictions: -1 (outlier) -> 1, 1 (inlier) -> 0
    y_pred = (pred_raw == -1).astype(int)


    # Evaluation
    acc = accuracy_score(y, y_pred)
    prec = precision_score(y, y_pred, zero_division=0)
    rec = recall_score(y, y_pred, zero_division=0)
    f1 = f1_score(y, y_pred, zero_division=0)
    ap = average_precision_score(y, scores)

    print("Confusion Matrix:\n", confusion_matrix(y, y_pred))
    print(f"Accuracy: {acc:.2f}")
    print(f"Precision: {prec:.2f}")
    print(f"Recall: {rec:.2f}")
    print(f"F1 Score: {f1:.2f}")
    print(f"PR AUC (AP): {ap:.4f}")

    # Store scores for the collective PR plot later
    plot_data[name] = {'scores': scores, 'ap': ap}


# 7. Precision-Recall Curve
plt.figure(figsize=(10, 8))

for name, data in plot_data.items():
    scores = data['scores']
    ap_val = data['ap']

    # Calculate PR curve points
    prec_vals, rec_vals, _ = precision_recall_curve(y, scores)

    # Plot line
    plt.plot(rec_vals, prec_vals, label=f"{name} (AP = {ap_val:.3f})")

# Add baseline (random guess would be the contamination rate)
plt.plot([0, 1], [contamination, contamination], 'k--', label=f'Baseline ({contamination:.2f})')

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Collective Precision-Recall Curve")
plt.legend(loc='best')
plt.grid(True, alpha=0.3)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])

# Show the final collective plot
plt.show()

"""# Model Validation using StratifiedKFold

### Ensures that the number of fraud companies in each fold remains the same

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.covariance import EllipticEnvelope
from sklearn.metrics import f1_score
import warnings

warnings.filterwarnings('ignore')

# --- 1. Load Data & Preprocessing ---
file_path = '/content/drive/MyDrive/Capstone/B2C/Model/Other models/b2c_preprocessed.csv'

try:
    df = pd.read_csv(file_path)

    # Feature Engineering
    min_positive_taxable = df.loc[df['Taxable Value'] > 0, 'Taxable Value'].min()
    if pd.isna(min_positive_taxable): min_positive_taxable = 1

    adjusted_taxable = df['Taxable Value'].apply(lambda x: x if x > 0 else min_positive_taxable)
    df['Discrepancy_to_Deposit'] = np.where(df['Deposit Amount'] > 0, df['Discrepancy'] / df['Deposit Amount'], 0)

    # Create Labels (Ground Truth)
    rule1 = (df['Taxable Value'] == 0) & (df['Deposit Amount'] > 1e7)
    rule2 = (df['Deposit Amount'] > 0) & (df['Discrepancy'] / df['Deposit Amount'] > 0.1)
    df['true_label'] = (rule1 | rule2).astype(int)

    df['Log_Deposit'] = np.log1p(df['Deposit Amount'])
    df['Log_Taxable'] = np.log1p(df['Taxable Value'])
    df['Log_Discrepancy'] = np.log1p(df['Discrepancy'])

except FileNotFoundError:
    print("File not found. Please check path.")
    raise

# Define Features and Target
feature_cols = [
    'Deposit Amount', 'Taxable Value', 'Discrepancy', 'Log_Deposit',
    'Log_Taxable', 'Log_Discrepancy', 'Discrepancy_to_Deposit'
]

X = df[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)
y = df['true_label']

# --- 2. Setup Analysis Parameters ---
# Determine max k allowed by data (cannot exceed number of fraud cases)
max_possible_k = y.value_counts().min()
k_range = range(2, max_possible_k + 1)
contamination_rate = 0.08  # Based on your fraud rate (8/100)

print(f"Dataset has {max_possible_k} fraud cases.")
print(f"Testing k from 2 to {max_possible_k}...")

# Dictionary to store results
model_names = ["Isolation Forest", "Local Outlier Factor", "One-Class SVM", "Elliptic Envelope"]
results = {name: {'mean_f1': [], 'std_f1': []} for name in model_names}

# --- 3. Iteration Loop (Sensitivity Analysis) ---
for k in k_range:
    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)

    # Temp storage for current k
    fold_scores = {name: [] for name in model_names}

    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # Scaling & PCA (Important for SVM/LOF)
        scaler = StandardScaler()
        X_train_s = scaler.fit_transform(X_train)
        X_test_s = scaler.transform(X_test)

        pca = PCA(n_components=4)
        X_train_p = pca.fit_transform(X_train_s)
        X_test_p = pca.transform(X_test_s)

        # Define Models
        models = {
            "Isolation Forest": IsolationForest(contamination=contamination_rate, n_estimators=200, random_state=42),
            "Local Outlier Factor": LocalOutlierFactor(contamination=contamination_rate, novelty=True, n_neighbors=20),
            "One-Class SVM": OneClassSVM(nu=contamination_rate, kernel="rbf", gamma='scale'),
            "Elliptic Envelope": EllipticEnvelope(contamination=contamination_rate, random_state=42)
        }

        # Train & Score
        for name, model in models.items():
            model.fit(X_train_p)
            y_pred = np.where(model.predict(X_test_p) == -1, 1, 0)
            fold_scores[name].append(f1_score(y_test, y_pred, zero_division=0))

    # Aggregate results for this k
    for name in model_names:
        results[name]['mean_f1'].append(np.mean(fold_scores[name]))
        results[name]['std_f1'].append(np.std(fold_scores[name]))

    print(f"Completed k={k}")

# --- 4. Plotting Results ---
plt.figure(figsize=(12, 8))
colors = ['blue', 'green', 'red', 'purple']
markers = ['o', 's', '^', 'D']

for i, name in enumerate(model_names):
    means = np.array(results[name]['mean_f1'])
    stds = np.array(results[name]['std_f1'])

    # Plot Mean Line
    plt.plot(k_range, means, marker=markers[i], label=name, color=colors[i], linewidth=2)

    # Plot Variance (Shaded Area) - Indicates Stability
    plt.fill_between(k_range, means - stds, means + stds, color=colors[i], alpha=0.1)

plt.title(f'Optimal k Analysis (Elbow Method) for All Models', fontsize=16)
plt.xlabel('Number of Folds (k)', fontsize=14)
plt.ylabel('Mean F1 Score', fontsize=14)
plt.xticks(k_range)
plt.legend(loc='lower right', fontsize=12)
plt.grid(True, alpha=0.3)

plt.show()

# k=5 is the optimal choice because it provides the most reliable balance between training data sufficiency and testing robustness

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.neighbors import LocalOutlierFactor
from sklearn.covariance import EllipticEnvelope
from sklearn.metrics import accuracy_score, precision_score, f1_score

# Load dataset
df = pd.read_csv('/content/drive/MyDrive/Capstone/B2C/Model/Other models/b2c_preprocessed.csv')

# Feature engineering
min_positive_taxable = df.loc[df['Taxable Value'] > 0, 'Taxable Value'].min()
adjusted_taxable = df['Taxable Value'].apply(lambda x: x if x > 0 else min_positive_taxable)
df['Ratio'] = df['Deposit Amount'] / adjusted_taxable
df['Log_Deposit'] = np.log1p(df['Deposit Amount'])
df['Log_Taxable'] = np.log1p(df['Taxable Value'])
df['Log_Discrepancy'] = np.log1p(df['Discrepancy'])
df['Discrepancy_to_Deposit'] = np.where(
    df['Deposit Amount'] > 0, df['Discrepancy'] / df['Deposit Amount'], 0
)

# Business rule flags for hybrid
rule1 = (df['Taxable Value'] == 0) & (df['Deposit Amount'] > 1e7)
rule2 = (df['Deposit Amount'] > 0) & (df['Discrepancy'] / df['Deposit Amount'] > 0.1)
df['underreport_flag'] = rule1 | rule2

# Use hybrid flag as ground truth
df['true_label'] = df['underreport_flag'].astype(int)

# 1. Setup Data and Features
feature_cols = [
    'Deposit Amount', 'Taxable Value', 'Discrepancy', 'Log_Deposit',
    'Log_Taxable', 'Log_Discrepancy', 'Discrepancy_to_Deposit'
]

# Ensure no NaNs or Infinite values
X = df[feature_cols].fillna(0)
X = X.replace([np.inf, -np.inf], 0)

# The target is the 'true_label' generated in previous steps
y = df['true_label']

# 2. Define Models
contamination_rate = 0.08

models = {
    "IsolationForest": IsolationForest(
        contamination=contamination_rate,
        n_estimators=200,
        random_state=42
    ),
    "OneClassSVM": OneClassSVM(
        nu=contamination_rate,
        kernel="rbf",
        gamma='scale'
    ),
    "EllipticEnvelope": EllipticEnvelope(
        contamination=contamination_rate,
        support_fraction=0.9,
        random_state=42
    ),
    "LocalOutlierFactor": LocalOutlierFactor(
        n_neighbors=20,
        contamination=contamination_rate,
        novelty=True  # Crucial for prediction on new data (folds)
    )
}

# 3. Initialize K-Fold
k = 5
skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)

# Dictionary to store final summary data
final_summary = []

print(f"Starting {k}-Fold Cross-Validation on {len(models)} models...")

# 4. Loop through Models
for model_name, model in models.items():
    print(f"\nEvaluating Model: {model_name}")
    print("="*40)

    fold_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}
    fold_idx = 1

    # Inner Loop: Folds
    for train_index, test_index in skf.split(X, y):
        # A. Split Data
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # B. Preprocessing (Standardization)
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # C. PCA
        pca = PCA(n_components=4)
        X_train_pca = pca.fit_transform(X_train_scaled)
        X_test_pca = pca.transform(X_test_scaled)

        # D. Train Model
        model.fit(X_train_pca)

        # E. Predict
        # Returns 1 for inliers (normal), -1 for outliers (fraud)
        y_pred_raw = model.predict(X_test_pca)

        # Map: -1 -> 1 (Fraud), 1 -> 0 (Normal)
        y_pred = np.where(y_pred_raw == -1, 1, 0)

        # F. Calculate Metrics
        acc = accuracy_score(y_test, y_pred)
        prec = precision_score(y_test, y_pred, zero_division=0)
        rec = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)

        # Store for averaging
        fold_metrics['accuracy'].append(acc)
        fold_metrics['precision'].append(prec)
        fold_metrics['recall'].append(rec)
        fold_metrics['f1'].append(f1)

        # Print Individual Fold Result
        print(f"Fold {fold_idx}: Accuracy={acc*100:.2f}%, Precision={prec*100:.2f}%, Recall={rec*100:.2f}%, F1={f1*100:.2f}%")

        fold_idx += 1

    # Calculate Averages
    mean_acc = np.mean(fold_metrics['accuracy'])
    mean_prec = np.mean(fold_metrics['precision'])
    mean_rec = np.mean(fold_metrics['recall'])
    mean_f1 = np.mean(fold_metrics['f1'])

    # Print Average Section
    print("\n" + "="*40)
    print(f"Average Performance for {model_name} across {k} Folds")
    print("="*40)
    print(f"Mean Accuracy:  {mean_acc*100:.2f}%")
    print(f"Mean Precision: {mean_prec*100:.2f}%")
    print(f"Mean Recall:    {mean_rec*100:.2f}%")
    print(f"Mean F1 Score:  {mean_f1*100:.2f}%")

    # Save to summary list
    final_summary.append({
        'Model': model_name,
        'Mean Accuracy': mean_acc,
        'Mean Precision': mean_prec,
        'Mean Recall': mean_rec,
        'Mean F1 Score': mean_f1
    })

# 5. Final Overall Summary Block
print("\n" + "#"*50)
print("Overall Model Performance Summary")
print("#"*50)

for item in final_summary:
    print(f"\n--- {item['Model']} ---")
    print(f"Mean Accuracy: {item['Mean Accuracy']*100:.2f}%")
    print(f"Mean Precision: {item['Mean Precision']*100:.2f}%")
    print(f"Mean Recall: {item['Mean Recall']*100:.2f}%")
    print(f"Mean F1 Score: {item['Mean F1 Score']*100:.2f}%")

# Save detailed results to CSV
results_df = pd.DataFrame(final_summary)
results_df.to_csv('/content/drive/MyDrive/Capstone/B2C/model_comparison_results.csv', index=False)

"""# Checking sensitivity by altering Contamination level"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from sklearn.metrics import precision_score, recall_score, f1_score
import warnings

warnings.filterwarnings('ignore')

# 1. Setup Data
feature_cols = [
    'Deposit Amount', 'Taxable Value', 'Discrepancy', 'Log_Deposit',
    'Log_Taxable', 'Log_Discrepancy', 'Discrepancy_to_Deposit'
]

# Handle NaNs and Infs just in case
X = df[feature_cols].fillna(0)
X = X.replace([np.inf, -np.inf], 0)
y = df['true_label']

# 2. Define Parameters
# We test contamination from 1% to 20%
contamination_range = [0.01, 0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.15, 0.20]
k = 5
skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)

sensitivity_results = []

print(f"Starting {k}-Fold Sensitivity Analysis on Isolation Forest...")
print(f"{'Contam':<10} | {'Mean F1':<10} | {'Mean Recall':<12} | {'Mean Prec':<10}")
print("-" * 50)

# 3. Iterate through Contamination Rates (Sensitivity Loop)
for c in contamination_range:

    # Lists to store scores for the current contamination 'c' across 5 folds
    fold_f1s = []
    fold_recs = []
    fold_precs = []

    # 4. Iterate through Folds (Validation Loop)
    for train_index, test_index in skf.split(X, y):
        # A. Split
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # B. Scale
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # C. PCA (Optional: keeping consistent with your previous setup)
        pca = PCA(n_components=4)
        X_train_pca = pca.fit_transform(X_train_scaled)
        X_test_pca = pca.transform(X_test_scaled)

        # D. Model
        model = IsolationForest(
            n_estimators=200,
            contamination=c,
            random_state=42
        )
        model.fit(X_train_pca)

        # E. Predict & Map (-1 to 1)
        y_pred_raw = model.predict(X_test_pca)
        y_pred = np.where(y_pred_raw == -1, 1, 0)

        # F. Score
        fold_f1s.append(f1_score(y_test, y_pred, zero_division=0))
        fold_recs.append(recall_score(y_test, y_pred, zero_division=0))
        fold_precs.append(precision_score(y_test, y_pred, zero_division=0))

    # 5. Average the results across the k folds
    mean_f1 = np.mean(fold_f1s)
    mean_rec = np.mean(fold_recs)
    mean_prec = np.mean(fold_precs)

    sensitivity_results.append({
        'Contamination': c,
        'Mean F1': mean_f1,
        'Mean Recall': mean_rec,
        'Mean Precision': mean_prec
    })

    print(f"{c:<10.2f} | {mean_f1:<10.2f} | {mean_rec:<12.2f} | {mean_prec:<10.2f}")

# 6. Plotting the Validation Curve
df_res = pd.DataFrame(sensitivity_results)

plt.figure(figsize=(12, 6))

# Plot Lines
plt.plot(df_res['Contamination'], df_res['Mean F1'], marker='o', label='Mean F1 Score', color='red', linewidth=2)
plt.plot(df_res['Contamination'], df_res['Mean Recall'], marker='^', label='Mean Recall', color='green', linestyle='--')
plt.plot(df_res['Contamination'], df_res['Mean Precision'], marker='s', label='Mean Precision', color='blue', linestyle=':')

# Add vertical line for actual fraud rate reference
plt.axvline(x=0.08, color='gray', linestyle='-', label='Target Rate (0.08)')

plt.title(f'K-Fold Sensitivity Analysis (Average of {k} Folds)')
plt.xlabel('Contamination Parameter')
plt.ylabel('Score')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

"""#REPORT

"""

!pip install WeasyPrint

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import json
import base64
from io import BytesIO
from weasyprint import HTML

print("--- B2C Final Report Generator (PDF) Running ---")

# --- 1. Define File Paths ---
# This is the file saved by your B2C_model.ipynb notebook
RESULTS_FILE = '/content/drive/MyDrive/Capstone Preprocessing/B2C/b2c_underreporting_results_with_eval.csv'
# This is where the new PDF report will be saved
FINAL_REPORT_FILE = "/content/drive/My Drive/Capstone/B2C/B2C_Sales_Suppression_Report.pdf"
REPORT_DIR = os.path.dirname(FINAL_REPORT_FILE)

# Create directories if they don't exist
os.makedirs(REPORT_DIR, exist_ok=True)

# --- 2. Helper Functions for Plotting & HTML ---

def fig_to_base64(fig):
    """Converts a matplotlib figure to a base64 encoded string."""
    buf = BytesIO()
    fig.savefig(buf, format='png', bbox_inches='tight')
    plt.close(fig)
    return base64.b64encode(buf.getvalue()).decode('utf-8')

def plot_priority_pie_chart(high_count, medium_count, low_count, normal_count):
    """Creates and returns a base64 string for the priority pie chart."""
    labels = []
    sizes = []
    colors = []

    if normal_count > 0:
        labels.append(f'Normal ({normal_count})')
        sizes.append(normal_count)
        colors.append('#2ca02c') # Green
    if low_count > 0:
        labels.append(f'Low Priority ({low_count})')
        sizes.append(low_count)
        colors.append('#1f77b4') # Blue
    if medium_count > 0:
        labels.append(f'Medium Priority ({medium_count})')
        sizes.append(medium_count)
        colors.append('#ff7f0e') # Orange
    if high_count > 0:
        labels.append(f'High Priority ({high_count})')
        sizes.append(high_count)
        colors.append('#d62728') # Red

    if not sizes:
        return ""

    fig, ax = plt.subplots(figsize=(8, 6))

    wedges, texts, autotexts = ax.pie(
        sizes,
        labels=labels,
        colors=colors,
        autopct='%1.1f%%',
        startangle=90,
        pctdistance=0.85,
        textprops=dict(color="black")
    )

    centre_circle = plt.Circle((0,0),0.70,fc='white')
    fig.gca().add_artist(centre_circle)

    ax.axis('equal')
    plt.title('Breakdown of All Analyzed Companies', pad=20, fontsize=14)
    plt.tight_layout()

    return fig_to_base64(fig)

def plot_horizontal_barchart(df, value_col, label_col, top_n, color, title):
    """Creates and returns a base64 string for a horizontal bar chart."""

    if df.empty or value_col not in df.columns or label_col not in df.columns:
        return ""

    df_plot = df.nlargest(top_n, value_col).sort_values(value_col, ascending=True)

    if df_plot.empty:
        return ""

    fig_height = max(5, top_n * 0.8)
    fig, ax = plt.subplots(figsize=(10, fig_height))

    bars = ax.barh(df_plot[label_col], df_plot[value_col], color=color)

    # Add labels to bars if it's not a percentage
    if "Ratio" not in title and "Percent" not in title:
         ax.bar_label(bars, fmt='â‚¹{:,.0f}', padding=3, fontsize=9)
    else:
         ax.bar_label(bars, fmt='{:.1%}', padding=3, fontsize=9)

    ax.set_xlabel('Value')
    ax.set_ylabel('Company ID')
    ax.set_title(title, fontsize=14)

    # Hide X-axis labels if we added bar labels, to avoid clutter
    ax.get_xaxis().set_visible(False)

    plt.tight_layout()
    return fig_to_base64(fig)

def get_html_style():
    """Returns basic CSS for a professional-looking report."""
    return """
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; line-height: 1.6; }
        .container { max-width: 1200px; margin: 20px auto; padding: 20px; }
        h1, h2, h3, h4 { color: #333; border-bottom: 2px solid #f0f0f0; padding-bottom: 5px; }
        h1 { font-size: 2.5em; text-align: center; color: #1a1a1a; border-bottom: 4px solid #d62728; }
        h2 { font-size: 2em; color: #d62728; }
        h3 { font-size: 1.75em; color: #1f77b4; }
        h4 { font-size: 1.5em; }

        table.report-table {
            border-collapse: collapse;
            width: 100%;
            margin-top: 20px;
            box-shadow: 0 2px 3px rgba(0,0,0,0.1);
            page-break-inside: auto;
            font-size: 0.8em;
            table-layout: fixed;
        }
        table.report-table tr {
            page-break-inside: avoid;
            page-break-after: auto;
        }
        table.report-table th, table.report-table td {
            border: 1px solid #ddd;
            padding: 5px;
            text-align: left;
            word-wrap: break-word;
            overflow-wrap: break-word;
        }

        table.report-table th {
            background-color: #f7f7f7; font-weight: bold;
        }
        table.report-table tr:nth-child(even) { background-color: #fdfdfd; }
        img { max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; margin-top: 15px; }
        .section {
            margin-top: 30px; padding-top: 20px; border-top: 1px solid #eee;
            page-break-inside: avoid;
        }
        .summary-box {
            background-color: #f9f9f9; border: 1px solid #e0e0e0;
            padding: 20px; border-radius: 5px; margin-bottom: 20px;
        }
        .summary-box li { list-style-type: none; font-size: 1.1em; }
        .summary-box li strong { color: #d62728; }
    </style>
    """

# --- 3. Load and Process Data ---
print("Loading B2C results...")
try:
    df = pd.read_csv(RESULTS_FILE)
except FileNotFoundError:
    print(f"CRITICAL: {RESULTS_FILE} not found. Ensure the B2C model notebook has been run.")
    raise SystemExit("B2C results file is missing.")

# Filter for only the flagged companies
df_flagged = df[df['underreport_flag'] == True].copy()

# --- 4. Define Risk Priority and Reasons ---
def get_priority_and_reason(row):
    """
    Assigns a priority and a text reason based on the rules
    from the notebook.
    """
    reasons = []
    priority = 4 # 1=High, 2=Medium, 3=Low, 4=Normal

    # Rule 1: Zero Sales Declared (HIGH PRIORITY)
    if (row['Taxable Value'] == 0) and (row['Deposit Amount'] > 1e7):
        reasons.append('Zero Sales Declared')
        priority = 1

    # Rule 2: High Discrepancy Ratio (MEDIUM PRIORITY)
    if (row['Deposit Amount'] > 0) and (row['Discrepancy'] / row['Deposit Amount'] > 0.1):
        reasons.append('High Discrepancy (>10%)')
        priority = min(priority, 2) # Can be high if also Rule 1

    # Rule 3: ML Anomaly (LOW PRIORITY)
    if row['anomaly'] == 1:
        reasons.append('ML Anomaly')
        priority = min(priority, 3) # Can be high/medium if also Rule 1/2

    # Format strings
    reason_str = ', '.join(sorted(list(set(reasons))))
    priority_map = {1: '1 - High', 2: '2 - Medium', 3: '3 - Low'}

    return priority_map[priority], reason_str

# Apply the function to all flagged companies
df_flagged[['Risk_Priority', 'Reason_for_Flag']] = df_flagged.apply(
    get_priority_and_reason,
    axis=1,
    result_type='expand'
)

# Sort by the new priority
df_flagged.sort_values(by='Risk_Priority', ascending=True, inplace=True)

# --- 5. Calculate Summary Counts ---
print("Calculating summaries...")
count_total_analyzed = len(df)
count_total_flagged = len(df_flagged)
count_normal = count_total_analyzed - count_total_flagged

count_high = len(df_flagged[df_flagged['Risk_Priority'] == '1 - High'])
count_medium = len(df_flagged[df_flagged['Risk_Priority'] == '2 - Medium'])
count_low = len(df_flagged[df_flagged['Risk_Priority'] == '3 - Low'])

total_discrepancy_amount = df_flagged['Discrepancy'].sum()

# --- 6. Generate Plots (as base64 strings) ---
print("Generating visualizations...")

priority_pie_chart_base64 = plot_priority_pie_chart(
    count_high,
    count_medium,
    count_low,
    count_normal
)

bar_discrepancy_base64 = plot_horizontal_barchart(
    df_flagged,
    'Discrepancy',
    'company_id',
    top_n=10,
    color='#d62728',
    title='Top 10 Companies by Discrepancy Amount (â‚¹)'
)

bar_ratio_base64 = plot_horizontal_barchart(
    df_flagged,
    'Discrepancy_to_Deposit',
    'company_id',
    top_n=10,
    color='#1f77b4',
    title='Top 10 Companies by Discrepancy Ratio (as % of Deposits)'
)


# --- 7. Build the HTML Report String ---
print("Building HTML report...")
report_html = f"""
<html>
<head>
    <title>B2C Sales Suppression Report</title>
    {get_html_style()}
</head>
<body>
    <div class="container">
        <h1>ðŸ“ˆ B2C Sales Suppression Report</h1>
        <p style="text-align: center;">
            Report Generated: {pd.to_datetime('now', utc=True).strftime('%Y-%m-%d %H:%M:%S UTC')}<br>
            Models Used: Hybrid Rules & Isolation Forest (Sales vs. Deposits)<br>
            Data Analyzed: {count_total_analyzed} B2C Companies
        </p>

        <div class="section">
            <h2>1. Executive Summary</h2>

            <div class="summary-box">
                <ul>
                    <li><strong>Total Companies Analyzed:</strong> {count_total_analyzed}</li>
                    <li><strong>Total Companies Flagged:</strong> {count_total_flagged}</li>
                    <li><strong>Total Discrepancy Amount:</strong> â‚¹{total_discrepancy_amount:,.2f}</li>
                    <li style="margin-top: 10px;"><strong>High Priority (Zero Sales):</strong> {count_high}</li>
                    <li><strong>Medium Priority (High Ratio):</strong> {count_medium}</li>
                    <li><strong>Low Priority (ML Anomaly Only):</strong> {count_low}</li>
                </ul>
            </div>

            <div style="text-align: center; margin-top: 20px;">
                <img src="data:image/png;base64,{priority_pie_chart_base64}" alt="Priority Pie Chart" style="max-width: 650px;">
            </div>
        </div>

        <div class="section">
            <h2>2. Key Visualizations</h2>

            <h4>Top 10 Companies by Discrepancy Amount</h4>
            <p>Highlights companies with the largest absolute monetary discrepancy.</p>
            <img src="data:image/png;base64,{bar_discrepancy_base64}" alt="Top 10 by Discrepancy Amount">

            <h4>Top 10 Companies by Discrepancy Ratio</h4>
            <p>Highlights companies with the largest discrepancy relative to their total deposits.</p>
            <img src="data:image/png;base64,{bar_ratio_base64}" alt="Top 10 by Discrepancy Ratio">
        </div>

        <div class="section">
            <h2>3. Combined B2C Fraud Roster</h2>
            <p>This is the master list of all suspicious companies, prioritized by risk.</p>
"""

# --- 8. Format and Add the Main Table ---
report_table_cols = [
    'company_id',
    'Risk_Priority',
    'Reason_for_Flag',
    'Discrepancy',
    'Discrepancy_to_Deposit',
    'Deposit Amount',
    'Taxable Value'
]
df_table = df_flagged[report_table_cols].copy()

# Format for display
df_table['Discrepancy'] = df_table['Discrepancy'].map('â‚¹{:,.2f}'.format)
df_table['Deposit Amount'] = df_table['Deposit Amount'].map('â‚¹{:,.2f}'.format)
df_table['Taxable Value'] = df_table['Taxable Value'].map('â‚¹{:,.2f}'.format)
df_table['Discrepancy_to_Deposit'] = (df_table['Discrepancy_to_Deposit'] * 100).round(2).astype(str) + '%'

df_table.reset_index(drop=True, inplace=True)
df_table.index.name = 'Rank'
df_table.index = df_table.index + 1
report_html += df_table.to_html(classes='report-table', border=0)


# --- 9. Close HTML and Save as PDF ---
report_html += """
    </div>
</body>
</html>
"""

# Save as PDF
try:
    HTML(string=report_html).write_pdf(FINAL_REPORT_FILE)

    print(f"\nSuccessfully generated B2C Fraud Report as PDF!")
    print(f"Report saved to: {FINAL_REPORT_FILE}")
except Exception as e:
    print(f"Error saving final PDF report: {e}")
    print("If you see an error about 'cairo', you may need to run:")
    print("!sudo apt-get install build-essential python3-dev python3-pip python3-setuptools python3-wheel python3-cffi libcairo2 libpango-1.0-0 libpangocairo-1.0-0 libgdk-pixbuf2.0-0 libffi-dev shared-mime-info")

print("--- B2C Final Report Generator Finished ---")