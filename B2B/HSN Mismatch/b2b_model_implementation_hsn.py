# -*- coding: utf-8 -*-
"""B2B_Model_implementation1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19YEKu09B51brruXvRVZi6id9kw_22USY

# HSN/SAC code mismatch

## Using Isolation Forest

We know that the companies in the fraudulent_transaction.csv are doing fraud since there is a mismatch in the HSN/SAC codes that the company usually deals with the the codes that the company is doing transaction in.

But to prioritize and rank them based on severity, we are using Isolation Forest.
"""

# connecting my drive to this google colab nb

from google.colab import drive
drive.mount('/content/drive')

"""###Preprocessing required for the model

Now we can start transforming this raw data into meaningful signals to detect the HSN/SAC misuse fraud (Fraudulent ITC claims)

A company can claim ITC only on the goods which it deals with. Fraudulent ITC claims is when a company buys products for its own personal use but tries to claim ITC on those products also.
"""

# HSN/SAC code misuse(Fraudulent ITC claims)
# Checking if the HSN/SAC codes the companies are doing transaction in matches the HSN/SAC codes that the company usually deals with

import os
import pandas as pd

# Define dataset paths
company_hsn_path = '/content/drive/My Drive/Capstone/B2B_CircularTrading/company hsn codes cleaned'
transaction_hsn_path = '/content/drive/My Drive/Capstone/B2B_CircularTrading/company-transaction hsn codes cleaned'
output_file = '/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_transactions.csv'

# Create a dictionary to store company HSN codes (excluding "0000")
company_hsn_lookup = {}

# Load company HSN codes
for file_name in os.listdir(company_hsn_path):
    if file_name.endswith('.csv'):
        company_id = os.path.splitext(file_name)[0]  # Extract company ID from filename
        file_path = os.path.join(company_hsn_path, file_name)
        df = pd.read_csv(file_path, dtype={"HSN": str, "SAC": str})

        # Filter out "7777" (null values) and collect valid HSN and SAC codes
        valid_hsn = df['HSN'][df['HSN'] != '7777'].tolist()
        valid_sac = df['SAC'][df['SAC'] != '7777'].tolist()
        company_hsn_lookup[company_id] = set(valid_hsn + valid_sac)  # Combine HSN and SAC into one set

# Process transaction files and detect mismatches
mismatch_data = []

for file_name in os.listdir(transaction_hsn_path):
    if file_name.endswith('.csv'):
        company_id = os.path.splitext(file_name)[0]  # Extract company ID from filename
        file_path = os.path.join(transaction_hsn_path, file_name)
        df = pd.read_csv(file_path, dtype={"HSN": str})

        # Get the company's valid HSN codes
        registered_hsn = company_hsn_lookup.get(company_id, set())

        # # Print the registered HSN codes for debugging
        # print(f"Company ID: {company_id}, Registered HSN/SAC codes: {registered_hsn}")

        # Check each transaction's HSN code
        for _, row in df.iterrows():
            hsn_code = str(row['HSN'])
            total_quantity = row['Total Quantity']
            total_value = row['Total Value']

            # Check if the HSN code is in the company's registered HSN codes
            if hsn_code not in registered_hsn:
                mismatch_data.append({
                    'company_id': company_id,
                    'Mismatch HSN Code': hsn_code,
                    'Quantity': total_quantity,
                    'Total Amount': total_value
                })

# Create a DataFrame from mismatched transactions
if mismatch_data:
    fraudulent_df = pd.DataFrame(mismatch_data)
    # Ensure HSN code remains a string
    fraudulent_df['Mismatch HSN Code'] = fraudulent_df['Mismatch HSN Code'].astype(str)
    # Save to CSV
    fraudulent_df.to_csv(output_file, index=False)
    print(f"Fraudulent transactions detected and saved to {output_file}")
else:
    print("No fraudulent transactions detected.")

"""## Getting the ground truth labels"""

# Importing all the necessary files
import pandas as pd
import os
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from google.colab import drive


# Define File Paths
all_companies_folder = '/content/drive/My Drive/Capstone/B2B_CircularTrading/b2b transactions cleaned'
fraud_file_path = '/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_transactions.csv'

# Building a profile for all the companies in the dataset.
# Finding the average transaction value for all the companies by dividing the total Invoice Value by the total number of transactions.

all_profiles_list = []
try:
    all_files = os.listdir(all_companies_folder)
    csv_files = [f for f in all_files if f.endswith('.csv')]
    for filename in csv_files:
        company_id = os.path.splitext(filename)[0]
        filepath = os.path.join(all_companies_folder, filename)
        temp_df = pd.read_csv(filepath)
        total_revenue = temp_df['Invoice Value'].sum()
        total_transactions = len(temp_df)
        if total_transactions > 0:
            avg_value = total_revenue / total_transactions
            all_profiles_list.append({
                'company_id': company_id, 'avg_transaction_value': avg_value,
                'total_revenue': total_revenue, 'total_transactions': total_transactions
            })
    master_df = pd.DataFrame(all_profiles_list)
    print(f"--- Created profiles for all {len(master_df)} companies. ---")
    display(master_df.head())
except FileNotFoundError:
    print(f"Error: The directory '{all_companies_folder}' was not found.")

# Finding the ground truth values --> Finding the number of fraudulent companies and their names from the pre processing, which finds the mismatch HSN/SAC code
# for the companies.
# Created a true label column which is labelling the data as fraudulent or not
# By doing this in a way we are manually labelling the data based on the discrepencies in the HSN codes.

try:
    fraud_df = pd.read_csv(fraud_file_path)
    # Create fraud summary
    fraud_summary = fraud_df.groupby('company_id').agg(
        mismatch_count=('Mismatch HSN Code', 'count'),
        mismatch_total_amount=('Total Amount', 'sum')
    ).reset_index()

    known_fraud_count = len(fraud_summary)
    print(f"Found {known_fraud_count} unique fraudulent companies in the file.")

    # Add fraud features to master profile
    master_df = pd.merge(master_df, fraud_summary, on='company_id', how='left')

    # Use direct assignment to safely fill NaN values
    master_df['mismatch_count'] = master_df['mismatch_count'].fillna(0)
    master_df['mismatch_total_amount'] = master_df['mismatch_total_amount'].fillna(0)

    # Create the 'true_label' column
    known_fraud_ids = set(fraud_df['company_id'])
    master_df['true_label'] = np.where(master_df['company_id'].isin(known_fraud_ids), 1, 0)
    print("--- Added features and ground truth labels. ---")
    display(master_df.head())
except FileNotFoundError:
    print(f"Error: The file '{fraud_file_path}' was not found.")

"""###Model Implementation

We now want to identify and rank companies that exhibit suspicious transaction patterns.
"""

# Running the Isolation Forest code

import numpy as np


# 1. Feature Engineering

print("--- Starting Feature Engineering ---")

# We use np.log1p (log(1+x)) to handle any values that might be 0.
master_df['log_avg_transaction_value'] = np.log1p(master_df['avg_transaction_value'])
master_df['log_total_revenue'] = np.log1p(master_df['total_revenue'])
master_df['log_total_transactions'] = np.log1p(master_df['total_transactions'])
master_df['log_mismatch_count'] = np.log1p(master_df['mismatch_count'])
master_df['log_mismatch_total_amount'] = np.log1p(master_df['mismatch_total_amount'])

# Create new ratio features
# What percentage of a company's revenue is from mismatched HSN codes?
master_df['mismatch_revenue_ratio'] = (
    master_df['mismatch_total_amount'] / master_df['total_revenue']
)
# What percentage of a company's transactions are mismatched?
master_df['mismatch_count_ratio'] = (
    master_df['mismatch_count'] / master_df['total_transactions']
)

# Handle potential "divide by zero" issues (if total_revenue or total_transactions was 0)
# We replace infinity with 0 (or you could choose 1, or another value)
master_df['mismatch_revenue_ratio'] = master_df['mismatch_revenue_ratio'].replace(
    [np.inf, -np.inf], 0
).fillna(0)
master_df['mismatch_count_ratio'] = master_df['mismatch_count_ratio'].replace(
    [np.inf, -np.inf], 0
).fillna(0)

print("New features created:")
print(master_df[['log_total_revenue', 'mismatch_revenue_ratio', 'mismatch_count_ratio']].head())



# 2. Model Training

# Define the NEW features for the model
# We now use our engineered features instead of the raw ones
feature_cols = [
    'log_avg_transaction_value',
    'log_total_revenue',
    'log_total_transactions',
    'log_mismatch_count',
    'log_mismatch_total_amount',
    'mismatch_revenue_ratio',
    'mismatch_count_ratio'
]
features = master_df[feature_cols]

# Calculate contamination level dynamically (this code is unchanged)
total_companies = len(master_df)
# Assumes 'known_fraud_count' is in memory from your previous cell
contamination_level = known_fraud_count / total_companies
print(f"\nTo get {known_fraud_count} outliers from {total_companies} companies, contamination is set to: {contamination_level:.4f}")

# Initialize and fit the Isolation Forest model (unchanged)
iso_forest = IsolationForest(contamination=contamination_level, random_state=42)
iso_forest.fit(features)

# Add the model's predictions to the DataFrame (unchanged)
master_df['anomaly_score'] = iso_forest.decision_function(features)
master_df['is_anomaly'] = iso_forest.predict(features)
print("--- Ran Isolation Forest on new features to get predictions. ---")


# 3. Model Results

# Displaying the fraudulent companies
anomalous_companies_df = master_df[master_df['is_anomaly'] == -1]
ranked_anomalies = anomalous_companies_df.sort_values(by='anomaly_score')

print(f"\n--- The model detected the following {len(ranked_anomalies)} companies as anomalous: ---")
display(ranked_anomalies)

# Model evaluation
y_true = master_df['true_label']
y_pred = (master_df['is_anomaly'] == -1).astype(int)

# Calculate the metrics
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print("\n--- Model Performance Metrics (with new features) ---")
print(f'Accuracy: {accuracy:.2%}')
print(f'Precision: {precision:.2%}')
print(f'Recall: {recall:.2%}')
print(f'F1 Score: {f1:.2%}')

"""
## Elliptic Envelope

Elliptic Envelope is a statistical method for detecting outliers. Its core idea is based on a key assumption: that the vast majority of your "normal" data points follow a Gaussian (bell-curve) distribution.
Any point found far outside this normal distribution is considered an outlier.

Since for normal companies we dont have any mismatch (hence it is 0), so these companies form a very dense, tight cluster around the origin. Fraudulent companies which have some mismatch are pushed further away from this central cluster. This enables it to easily flag the outliers. This makes it an excellent model for finding global anomalies that are far from the main group."""

# Elliptic Envelope implementation

from sklearn.covariance import EllipticEnvelope
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize and fit the model
# We use the same contaimation and random_state value as used for the isolation forest.
ee_model = EllipticEnvelope(contamination=contamination_level, random_state=42)
ee_predictions = ee_model.fit_predict(features)

# Get the model's predictions (convert -1/1 to 1/0)
y_pred_ee = (ee_predictions == -1).astype(int)

# 3. Calculate and display performance metrics
accuracy_ee = accuracy_score(y_true, y_pred_ee)
precision_ee = precision_score(y_true, y_pred_ee)
recall_ee = recall_score(y_true, y_pred_ee)
f1_ee = f1_score(y_true, y_pred_ee)

print("\n--- Elliptic Envelope Performance ---")
print(f'Accuracy: {accuracy_ee:.2%}')
print(f'Precision: {precision_ee:.2%}')
print(f'Recall: {recall_ee:.2%}')
print(f'F1 Score: {f1_ee:.2%}')

"""## One Class SVM"""

from sklearn.preprocessing import StandardScaler

# Define the features for the model
features_df = master_df[['avg_transaction_value', 'total_revenue', 'total_transactions',
                         'mismatch_count', 'mismatch_total_amount']]

# 1. Scale the features
# StandardScaler rescales every feature so that it has a mean of 0 and a standard deviation of 1.
# This puts all features on a level playing field, ensuring the model treats them with equal importance during training.
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_df)

# 2. Separate data for training (on normal companies only)
# The model will be trained only on the normal companies data - the ones which dont have a mismatch in the HSN codes, hence its true label showed one
# when we were making the labels for the isolation forest.
X_train = features_scaled[master_df['true_label'] == 0]

# The test data is all the companies tho- the fraudulent and the normal companies together
X_test = features_scaled
y_test = master_df['true_label'].values

print(f"Training data shape (normal companies only): {X_train.shape}")

from sklearn.svm import OneClassSVM
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

print("--- Running One-Class SVM Model ---")

# 1. Initialize and fit the model
# It doesn't use contamination directly, but we'll need it for evaluation later.
# 'nu' is similar to contamination; it's an estimate of the outlier fraction.
oc_svm_model = OneClassSVM(nu=contamination_level, kernel="rbf", gamma="auto")
oc_svm_predictions = oc_svm_model.fit_predict(features_scaled)

# 2. Get anomaly scores for the PR Curve
# The score is the signed distance to the separating hyperplane. Lower is more anomalous.
oc_svm_scores = -oc_svm_model.score_samples(features_scaled)

# 3. Get binary predictions and evaluate
y_pred_oc_svm = (oc_svm_predictions == -1).astype(int)

print("\n--- One-Class SVM Performance ---")
print(f'Accuracy: {accuracy_score(y_true, y_pred_oc_svm):.2%}')
print(f'Precision: {precision_score(y_true, y_pred_oc_svm):.2%}')
print(f'Recall: {recall_score(y_true, y_pred_oc_svm):.2%}')
print(f'F1 Score: {f1_score(y_true, y_pred_oc_svm):.2%}')

"""## LOF"""

from sklearn.neighbors import LocalOutlierFactor

print("\n--- Running Local Outlier Factor (LOF) Model ---")

# 1. Initialize and fit the model
# We set contamination so it knows how many outliers to expect.
lof_model = LocalOutlierFactor(n_neighbors=20, contamination=contamination_level)
lof_predictions = lof_model.fit_predict(features_scaled)

# 2. Get anomaly scores for the PR Curve
# The score is the negative of the Local Outlier Factor. Lower is more anomalous.
lof_scores = -lof_model.negative_outlier_factor_

# 3. Get binary predictions and evaluate
y_pred_lof = (lof_predictions == -1).astype(int)

print("\n--- Local Outlier Factor Performance ---")
print(f'Accuracy: {accuracy_score(y_true, y_pred_lof):.2%}')
print(f'Precision: {precision_score(y_true, y_pred_lof):.2%}')
print(f'Recall: {recall_score(y_true, y_pred_lof):.2%}')
print(f'F1 Score: {f1_score(y_true, y_pred_lof):.2%}')

"""## Model Evaluation"""

from sklearn.metrics import precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt

# --- This cell assumes all your models have been trained in previous cells ---
# --- It uses your existing fitted models and data variables. ---

print("--- Generating Combined Precision-Recall Curve ---")

# 1. Get the ground truth labels
y_true = master_df['true_label']

# 2. Calculate the anomaly score for each model using the correct feature set

# For models trained on UNSCALED data
iso_scores = -iso_forest.decision_function(features)
ee_scores = -ee_model.decision_function(features)
# knn_scores is already calculated from unscaled features in your code

# For models trained on SCALED data
oc_svm_scores = -oc_svm_model.score_samples(features_scaled)
lof_scores = -lof_model.negative_outlier_factor_
# ae_scores is already calculated from scaled features in your code


# 3. Calculate PR curve points and AP for all models
precision_iso, recall_iso, _ = precision_recall_curve(y_true, iso_scores)
ap_iso = average_precision_score(y_true, iso_scores)

precision_oc_svm, recall_oc_svm, _ = precision_recall_curve(y_true, oc_svm_scores)
ap_oc_svm = average_precision_score(y_true, oc_svm_scores)

precision_lof, recall_lof, _ = precision_recall_curve(y_true, lof_scores)
ap_lof = average_precision_score(y_true, lof_scores)

precision_ee, recall_ee, _ = precision_recall_curve(y_true, ee_scores)
ap_ee = average_precision_score(y_true, ee_scores)



# 4. Plot all PR curves on one graph
plt.figure(figsize=(12, 8))

plt.plot(recall_iso, precision_iso, lw=2, label=f'Isolation Forest (AP = {ap_iso:.2f})')
plt.plot(recall_oc_svm, precision_oc_svm, lw=2, label=f'One-Class SVM (AP = {ap_oc_svm:.2f})')
plt.plot(recall_ee, precision_ee, lw=2, label=f'Elliptic Envelope (AP = {ap_ee:.2f})')
plt.plot(recall_lof, precision_lof, lw=2, label=f'LOF (AP = {ap_lof:.2f})')


# Add a no-skill line for reference
no_skill = len(y_true[y_true==1]) / len(y_true)
plt.plot([0, 1], [no_skill, no_skill], linestyle='--', lw=2, label='No-Skill Classifier')

plt.xlabel('Recall (True Positive Rate)')
plt.ylabel('Precision')
plt.title('Comparison of Precision-Recall Curves for All Models')
plt.legend(loc="best")
plt.grid(True)
plt.show()

"""# Model Validation using StratifiedKFold

### Ensures that the number of fraud companies in each fold remains the same
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.covariance import EllipticEnvelope

# --- 1. Data Preparation ---
# (Ensuring variables from your previous cells are ready)
feature_cols = [
    'log_avg_transaction_value',
    'log_total_revenue',
    'log_total_transactions',
    'log_mismatch_count',
    'log_mismatch_total_amount',
    'mismatch_revenue_ratio',
    'mismatch_count_ratio'
]

if 'master_df' not in globals():
    print("Error: Please ensure 'master_df' contains your data.")
else:
    X_unscaled = master_df[feature_cols]
    y = master_df['true_label']

    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_unscaled)

    # Parameters
    # Range of K to test (2 to 15)
    k_range = range(2, 16)

    # Using the contamination level (calculated dynamically or fixed at 0.1)
    contamination_level = 0.10

    # Storage for results
    model_names = ["Isolation Forest", "Local Outlier Factor", "One-Class SVM", "Elliptic Envelope"]
    results = {name: {'mean_f1': [], 'std_f1': []} for name in model_names}

    print(f"--- Starting K-Fold Sensitivity Analysis (k={k_range.start} to {k_range.stop - 1}) ---")

    # --- 2. Iteration Loop ---
    for k in k_range:

        # Create the splitter
        # We use StratifiedKFold to ensure we don't end up with folds having 0 fraud cases
        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)

        # Temporary storage for this specific k
        fold_scores = {name: [] for name in model_names}

        try:
            for train_index, test_index in skf.split(X_scaled, y):
                X_train, X_test = X_scaled[train_index], X_scaled[test_index]
                y_train, y_test = y.iloc[train_index], y.iloc[test_index]

                # --- Define Models ---
                # Note: LOF requires novelty=True to predict on new data (X_test)
                models = {
                    "Isolation Forest": IsolationForest(contamination=contamination_level, random_state=42),
                    "Local Outlier Factor": LocalOutlierFactor(contamination=contamination_level, novelty=True),
                    "One-Class SVM": OneClassSVM(nu=contamination_level, kernel="rbf", gamma="auto"),
                    "Elliptic Envelope": EllipticEnvelope(contamination=contamination_level, random_state=42)
                }

                # --- Train & Predict ---
                for name, model in models.items():
                    model.fit(X_train)
                    # Predict anomalies (-1 becomes 1, 1 becomes 0)
                    y_pred = (model.predict(X_test) == -1).astype(int)

                    # Calculate metric
                    score = f1_score(y_test, y_pred, zero_division=0)
                    fold_scores[name].append(score)

            # --- Aggregate Results for this k ---
            for name in model_names:
                results[name]['mean_f1'].append(np.mean(fold_scores[name]))
                results[name]['std_f1'].append(np.std(fold_scores[name]))

        except ValueError:
            # This handles cases where k is too high for the number of minority samples
            print(f"Skipping k={k}: Not enough fraud samples to populate every fold.")
            for name in model_names:
                results[name]['mean_f1'].append(np.nan)
                results[name]['std_f1'].append(np.nan)

    print("Analysis Complete. Generating Plot...")

    # --- 3. Plotting the Results ---
    plt.figure(figsize=(14, 8))

    colors = ['blue', 'green', 'red', 'orange']

    for i, name in enumerate(model_names):
        means = np.array(results[name]['mean_f1'])
        stds = np.array(results[name]['std_f1'])

        # Plot Mean F1
        plt.plot(k_range, means, marker='o', label=name, color=colors[i], linewidth=2)

        # Fill standard deviation area (Stability Indicator)
        plt.fill_between(k_range, means - stds, means + stds, color=colors[i], alpha=0.1)

    plt.title(f'Effect of K-Folds on Model Performance (F1 Score)\n(Shaded area = Standard Deviation/Instability)', fontsize=16)
    plt.xlabel('Number of Folds (k)', fontsize=14)
    plt.ylabel('Mean F1 Score', fontsize=14)
    plt.xticks(k_range)
    plt.legend(loc='lower right', fontsize=12)
    plt.grid(True, alpha=0.5)

    # Draw a vertical line at k=5 for reference
    plt.axvline(x=5, color='black', linestyle='--', label='Current k=5')

    plt.show()

# Based on the sensitivity analysis, k=5 is the optimal choice because it achieves the best balance between predictive accuracy and model stability given the dataset's size.
# The performance graph demonstrates that at k=5, the Isolation Forest model hits its peak Mean F1 score (approaching 1.0),
# slightly outperforming $k=4$ while maintaining a notably narrow standard deviation, which indicates consistent and robust behavior across different data splits.


# finding mean for all the models

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Import all the models
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.covariance import EllipticEnvelope


# 1. Prepare Data


feature_cols = [
    'log_avg_transaction_value',
    'log_total_revenue',
    'log_total_transactions',
    'log_mismatch_count',
    'log_mismatch_total_amount',
    'mismatch_revenue_ratio',
    'mismatch_count_ratio'
]

# Ensure the columns exist before proceeding
if not all(col in master_df.columns for col in feature_cols + ['true_label']):
    print("Error: 'master_df' is missing required columns.")
    # You might need to re-run your feature engineering cells
else:
    X_unscaled = master_df[feature_cols]
    y = master_df['true_label']

    # CRITICAL: Scale features for a fair comparison
    # OCSVM and LOF require this, and it makes the benchmark standard.
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_unscaled)


    # 2. Configure K-Fold

    k_folds = 5
    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)

    # Get the contamination level (from your previous code)
    known_fraud_count = y.sum()
    total_companies = len(y)
    # contamination_level = known_fraud_count / total_companies
    contamination_level = 0.10

    # Dictionary to store the metrics for each model
    model_metrics = {
        "Isolation Forest": {'acc': [], 'prec': [], 'rec': [], 'f1': []},
        "Local Outlier Factor": {'acc': [], 'prec': [], 'rec': [], 'f1': []},
        "One-Class SVM": {'acc': [], 'prec': [], 'rec': [], 'f1': []},
        "Elliptic Envelope": {'acc': [], 'prec': [], 'rec': [], 'f1': []}
    }

    print(f"Starting {k_folds}-Fold Stratified Cross-Validation...")
    print(f"Contamination set to: {contamination_level:.4f}")
    print("-" * 40)


    # 3. Run the Cross-Validation Loop

    fold_no = 1
    for train_index, test_index in skf.split(X_scaled, y):
        # Split data for this fold
        X_train, X_test = X_scaled[train_index], X_scaled[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # --- Initialize, Fit, and Predict for all models ---

        # 1. Isolation Forest
        model_if = IsolationForest(contamination=contamination_level, random_state=42)
        model_if.fit(X_train)
        y_pred_if = (model_if.predict(X_test) == -1).astype(int)

        # 2. Local Outlier Factor (LOF)
        # Note: LOF must be fit on training data and predicted on test data separately.
        # We use 'novelty=True' to enable the 'predict' method.
        model_lof = LocalOutlierFactor(contamination=contamination_level, novelty=True)
        model_lof.fit(X_train)
        y_pred_lof = (model_lof.predict(X_test) == -1).astype(int)

        # 3. One-Class SVM
        model_ocsvm = OneClassSVM(nu=contamination_level, kernel="rbf", gamma="auto")
        # OCSVM is technically "unsupervised" but we fit on train, predict on test
        model_ocsvm.fit(X_train)
        y_pred_ocsvm = (model_ocsvm.predict(X_test) == -1).astype(int)

        # 4. Elliptic Envelope
        model_ee = EllipticEnvelope(contamination=contamination_level, random_state=42)
        model_ee.fit(X_train)
        y_pred_ee = (model_ee.predict(X_test) == -1).astype(int)

        # --- Store Metrics ---
        models_preds = {
            "Isolation Forest": y_pred_if,
            "Local Outlier Factor": y_pred_lof,
            "One-Class SVM": y_pred_ocsvm,
            "Elliptic Envelope": y_pred_ee
        }

        print(f"\nFold {fold_no}:")
        for model_name, y_pred in models_preds.items():
            acc = accuracy_score(y_test, y_pred)
            prec = precision_score(y_test, y_pred, zero_division=0)
            rec = recall_score(y_test, y_pred, zero_division=0)
            f1 = f1_score(y_test, y_pred, zero_division=0)

            model_metrics[model_name]['acc'].append(acc)
            model_metrics[model_name]['prec'].append(prec)
            model_metrics[model_name]['rec'].append(rec)
            model_metrics[model_name]['f1'].append(f1)
            print(f"  {model_name:<22}: F1={f1:.2%}")

        fold_no += 1


    # 4. Display Final Average Results

    print("\n" + "="*50)
    print(f"Average Performance across {k_folds} Folds")
    print("="*50)

    for model_name, metrics in model_metrics.items():
        print(f"\n--- {model_name} ---")
        print(f"  Mean Accuracy:  {np.mean(metrics['acc']):.2%}")
        print(f"  Mean Precision: {np.mean(metrics['prec']):.2%}")
        print(f"  Mean Recall:    {np.mean(metrics['rec']):.2%}")
        print(f"  Mean F1 Score:  {np.mean(metrics['f1']):.2%}")

# new pr curve

import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.preprocessing import StandardScaler

# --- This cell assumes 'master_df' is loaded and has 'true_label' ---

print("--- Generating B2B PR Curve (Behavioral Features Only) ---")


# 1. Define the Features

# We are REMOVING all mismatch-related features to prevent leakage.
behavioral_feature_cols = [
    'log_avg_transaction_value',
    'log_total_revenue',
    'log_total_transactions'
]

# (This assumes you have run the log-transform cells in your notebook)
X_behavioral = master_df[behavioral_feature_cols]
y = master_df['true_label']

# Scale these features (just as you do in your k-fold script)
scaler = StandardScaler()
X_behavioral_scaled = scaler.fit_transform(X_behavioral)

# Get contamination level (from your previous code)
contamination_level = y.sum() / len(y)


# 2. Create a single, stratified train/test split

X_train, X_test, y_train, y_test = train_test_split(
    X_behavioral_scaled, y, test_size=0.2, stratify=y, random_state=42
)


# 3. Fit all models on this new training set

# model_if = IsolationForest(contamination=contamination_level, random_state=42).fit(X_train)
model_lof = LocalOutlierFactor(contamination=contamination_level, novelty=True).fit(X_train)
model_ocsvm = OneClassSVM(nu=contamination_level, kernel="rbf", gamma="auto").fit(X_train)
model_ee = EllipticEnvelope(contamination=contamination_level, random_state=42).fit(X_train)


# 4. Get the ANOMALY SCORES for the test set

# scores_if = -model_if.decision_function(X_test)
iso_scores = -iso_forest.decision_function(features)
scores_lof = -model_lof.score_samples(X_test)
scores_ocsvm = -model_ocsvm.score_samples(X_test)
scores_ee = -model_ee.decision_function(X_test)


# 5. Calculate PR curve points and AP for all models

precision_iso, recall_iso, _ = precision_recall_curve(y_true, iso_scores)
ap_iso = average_precision_score(y_true, iso_scores)

precision_lof, recall_lof, _ = precision_recall_curve(y_test, scores_lof)
ap_lof = average_precision_score(y_test, scores_lof)

precision_ocsvm, recall_ocsvm, _ = precision_recall_curve(y_test, scores_ocsvm)
ap_ocsvm = average_precision_score(y_test, scores_ocsvm)

precision_ee, recall_ee, _ = precision_recall_curve(y_test, scores_ee)
ap_ee = average_precision_score(y_test, scores_ee)


# 6. Plot all PR curves on one graph

plt.figure(figsize=(12, 8))

plt.plot(recall_iso, precision_iso, lw=2, label=f'Isolation Forest (AP = {ap_iso:.3f})')
plt.plot(recall_lof, precision_lof, lw=2, label=f'Local Outlier Factor (AP = {ap_lof:.3f})')
plt.plot(recall_ocsvm, precision_ocsvm, lw=2, label=f'One-Class SVM (AP = {ap_ocsvm:.3f})')
plt.plot(recall_ee, precision_ee, lw=2, label=f'Elliptic Envelope (AP = {ap_ee:.3f})')

# Add a no-skill line
no_skill = len(y_test[y_test==1]) / len(y_test)
plt.plot([0, 1], [no_skill, no_skill], linestyle='--', lw=2, label=f'No-Skill (AP = {no_skill:.3f})')

plt.xlabel('Recall (True Positive Rate)')
plt.ylabel('Precision')
plt.title('Comparison of Precision-Recall Curves (B2B HSN Misuse - Behavioral Features)')
plt.legend(loc="best")
plt.grid(True)
plt.savefig('b2b_hsn_pr_curve_behavioral.png') # Save the figure
plt.show()

print("PR Curve saved as 'b2b_hsn_pr_curve_behavioral.png'")

"""# Checking sensitivity by altering Contamination level"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, precision_score, recall_score
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import os


# 1. Prepare Data

# This assumes 'master_df' is already loaded from your HSN Misuse notebook
if 'master_df' not in globals():
    print("Error: 'master_df' is not defined.")
    # Placeholder for demo purposes
    # master_df = pd.DataFrame(np.random.rand(100, 7), columns=[
    #     'log_avg_transaction_value', 'log_total_revenue', 'log_total_transactions',
    #     'log_mismatch_count', 'log_mismatch_total_amount', 'mismatch_revenue_ratio', 'mismatch_count_ratio'
    # ])
    # master_df['true_label'] = np.random.choice([0, 1], 100, p=[0.90, 0.10])
else:
    print("Using loaded 'master_df'.")

feature_cols = [
    'log_avg_transaction_value',
    'log_total_revenue',
    'log_total_transactions',
    'log_mismatch_count',
    'log_mismatch_total_amount',
    'mismatch_revenue_ratio',
    'mismatch_count_ratio'
]

if not all(col in master_df.columns for col in feature_cols + ['true_label']):
    print("Error: 'master_df' is missing required columns.")
else:
    X_unscaled = master_df[feature_cols]
    y = master_df['true_label']

    # CRITICAL: Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_unscaled)


    # 2. Configure Sensitivity Analysis

    k_folds = 5
    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)

    # Define the range of contamination values to test
    contamination_range = np.linspace(0.05, 0.15, 11)

    true_contamination = 0.1


    # Dictionary to store the final results
    sensitivity_results = {
        "contamination": [],
        "mean_f1": [],
        "mean_precision": [],
        "mean_recall": []
    }

    print(f"Starting Contamination Sensitivity Analysis...")
    print("-" * 40)

    # 3. Run the Nested Cross-Validation Loop

    for contam_value in contamination_range:

        fold_f1_scores = []
        fold_precision_scores = []
        fold_recall_scores = []

        for fold_no, (train_index, test_index) in enumerate(skf.split(X_scaled, y)):

            X_train, X_test = X_scaled[train_index], X_scaled[test_index]
            y_train, y_test = y.iloc[train_index], y.iloc[test_index]

            # Initialize model
            model_if = IsolationForest(
                contamination=contam_value,
                random_state=42
            )
            model_if.fit(X_train)

            y_pred_if = (model_if.predict(X_test) == -1).astype(int)

            # Store Metrics
            f1 = f1_score(y_test, y_pred_if, zero_division=0)
            precision = precision_score(y_test, y_pred_if, zero_division=0)
            recall = recall_score(y_test, y_pred_if, zero_division=0)

            fold_f1_scores.append(f1)
            fold_precision_scores.append(precision)
            fold_recall_scores.append(recall)

        # Calculate Mean Metrics
        mean_f1 = np.mean(fold_f1_scores)
        mean_precision = np.mean(fold_precision_scores)
        mean_recall = np.mean(fold_recall_scores)

        print(f"Contam = {contam_value:.3f} | F1: {mean_f1:.2%} | Precision: {mean_precision:.2%} | Recall: {mean_recall:.2%}")

        sensitivity_results['contamination'].append(contam_value)
        sensitivity_results['mean_f1'].append(mean_f1)
        sensitivity_results['mean_precision'].append(mean_precision)
        sensitivity_results['mean_recall'].append(mean_recall)



    # 4. Display Final Plot

    print("\n" + "="*50)
    print(f"Sensitivity Analysis Complete")
    print("="*50)

    # Find the best result based on F1-Score
    best_f1 = max(sensitivity_results['mean_f1'])
    best_idx = np.argmax(sensitivity_results['mean_f1'])
    best_contam = sensitivity_results['contamination'][best_idx]

    print(f"Peak F1 Score: {best_f1:.2%} at 'contamination' param: {best_contam:.3f}")

    # Plot the results
    plt.figure(figsize=(12, 7))

    # Plot Mean F1-Score
    plt.plot(
        sensitivity_results['contamination'],
        sensitivity_results['mean_f1'],
        marker='o',
        linestyle='--',
        label=f'Mean F1 Score (Peak: {best_f1:.2%})'
    )

    # Plot Mean Precision
    plt.plot(
        sensitivity_results['contamination'],
        sensitivity_results['mean_precision'],
        marker='s',
        linestyle=':',
        label=f'Mean Precision'
    )

    # Plot Mean Recall
    plt.plot(
        sensitivity_results['contamination'],
        sensitivity_results['mean_recall'],
        marker='^',
        linestyle='-.',
        label=f'Mean Recall'
    )

    # Highlight the PEAK performance
    plt.axvline(
        x=best_contam,
        color='green',
        linestyle='--',
        label=f'Optimal Contamination ({best_contam:.3f})'
    )

    # Highlight the HARDCODED TRUE contamination (0.10)
    plt.axvline(
        x=true_contamination,
        color='red',
        linestyle=':',
        label=f'True Fraud Rate ({true_contamination:.3f})'
    )

    plt.title('Validation of Contamination Hyperparameter (HSN Misuse)', fontsize=16)
    plt.xlabel('Contamination Parameter Value', fontsize=12)
    plt.ylabel(f'Mean Score (across {k_folds} folds)', fontsize=12)
    plt.legend(loc='best')
    plt.grid(True, linestyle=':')
    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))

    # plt.savefig('iforest_sensitivity_validation.png', dpi=300, bbox_inches='tight')
    plt.show()

"""## Creation of final Fraud Report"""

import pandas as pd
import numpy as np
import os


print("--- HSN Mismatch Export Script Running ---")

# Define the output path for the intermediate results
HSN_RESULTS_FILE = "/content/drive/My Drive/Capstone/B2B_CircularTrading/hsn_fraud_results.csv"
HSN_OUTPUT_DIR = os.path.dirname(HSN_RESULTS_FILE)

# Ensure the output directory exists
os.makedirs(HSN_OUTPUT_DIR, exist_ok=True)

# --- Feature Engineering (to match your latest features) ---
# This ensures the ratio columns exist, even if the cell wasn't run
if 'mismatch_revenue_ratio' not in master_df.columns:
    print("Calculating feature ratios...")
    # Calculate Mismatch-to-Revenue Ratio
    master_df['mismatch_revenue_ratio'] = (
        master_df['mismatch_total_amount'] / master_df['total_revenue']
    )
    # Calculate Mismatch-to-Transaction Count Ratio
    master_df['mismatch_count_ratio'] = (
        master_df['mismatch_count'] / master_df['total_transactions']
    )

    # Handle potential "divide by zero" issues (if total_revenue or total_transactions was 0)
    master_df['mismatch_revenue_ratio'] = master_df['mismatch_revenue_ratio'].replace(
        [np.inf, -np.inf], 0
    ).fillna(0)
    master_df['mismatch_count_ratio'] = master_df['mismatch_count_ratio'].replace(
        [np.inf, -np.inf], 0
    ).fillna(0)

# --- Select and Filter Data ---
# Select only the columns needed for the final report
columns_to_export = [
    'company_id',
    'anomaly_score',
    'mismatch_count',
    'mismatch_total_amount',
    'total_revenue',
    'mismatch_revenue_ratio'
]

# Check if all columns exist
if not all(col in master_df.columns for col in columns_to_export):
    print("Error: Not all required columns were found in master_df.")
    print(f"Missing: {[col for col in columns_to_export if col not in master_df.columns]}")
else:
    # Filter for companies that are either flagged OR have any mismatch
    # This gives the next script all relevant data
    # --- FIX: Changed filter to ONLY include companies flagged by the model ---
    hsn_fraud_df = master_df[
        (master_df['is_anomaly'] == -1)
    ][columns_to_export].copy()

    # --- Save to Intermediate File ---
    try:
        hsn_fraud_df.to_csv(HSN_RESULTS_FILE, index=False)
        print(f"Successfully saved HSN fraud results to {HSN_RESULTS_FILE}")
        print(f"Found and saved {len(hsn_fraud_df)} companies with HSN mismatches.")
    except Exception as e:
        print(f"Error saving HSN results: {e}")

print("--- HSN Mismatch Export Script Finished ---")