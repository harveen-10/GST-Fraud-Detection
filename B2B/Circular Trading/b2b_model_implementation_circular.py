# -*- coding: utf-8 -*-
"""B2B_Model_implementation2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15EEak2voGiW686GIfY_RaPmt6BRxh9TA

# For Circular Trading
"""

# connecting my drive to this google colab nb

from google.colab import drive
drive.mount('/content/drive')

"""# Preprocessing"""

import os
import re
import csv
import json
import math
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional

import pandas as pd
import networkx as nx
from tqdm import tqdm

# FILE SYSTEM CONFIGURATION

DATA_FOLDER = "/content/drive/My Drive/Capstone/B2B_CircularTrading/b2b_latest_transactions_circular_time_cleaned"
OUT_FOLDER = "/content/drive/My Drive/Capstone/B2B_CircularTrading/circular_detection_reports"

# This switch determines how companies (nodes) are identified in the graph.
# True: Use 'GSTIN' for unique IDs. False: Fall back to using 'Receiver Name'/'Sender Name'.
USE_GSTIN_AS_NODES = True


#  CYCLE DETECTION PARAMETERS

# These settings fine-tune the fraud detection algorithm itself.

# Hard cap to stop the search early on very dense graphs
MAX_CYCLES_PER_GRAPH = 5000
# The maximum number of "hops" to look for in a cycle (e.g., A->B->C->A is length 3)
MAX_CYCLE_LENGTH = 6
# All invoices in a "fraudulent" cycle must have occurred within this many days
TIME_WINDOW_DAYS = 30
# All invoice values in a cycle must be within this tolerance (e.g., 0.15 = ±15%)
VALUE_SIMILARITY_TOL = 0.15
# Ignores any transactions with a value below this amount
MIN_EDGE_VALUE = 0.0

# CSV reading
ENCODINGS_TO_TRY = ["utf-8", "utf-8-sig", "cp1252"]  # windows-friendly fallback
SNIFFER_BYTES = 2048  # bytes to sniff delimiter

# A company must meet or exceed this risk score to be included in the final report
RISK_THRESHOLD = 30

DATE_FORMATS = [
    "%Y-%m-%d %H:%M:%S",
    "%Y-%m-%d",
    "%d-%m-%Y %H:%M:%S",
    "%d-%m-%Y",
    "%d/%m/%Y %H:%M:%S",
    "%d/%m/%Y",
    "%Y/%m/%d %H:%M:%S",
    "%Y/%m/%d",
    "%d-%b-%Y",
]

ISO_PATTERN = re.compile(r"^\d{4}-\d{2}-\d{2}(?:[ T]\d{2}:\d{2}:\d{2})?$")
DMY_PATTERN = re.compile(r"^\d{1,2}[-/]\d{1,2}[-/]\d{2,4}")

def try_parse_date(s: Any) -> Optional[pd.Timestamp]:
    if pd.isna(s):
        return None
    if isinstance(s, (datetime, pd.Timestamp)):
        return pd.to_datetime(s, errors="coerce")

    s = str(s).strip()
    if not s:
        return None

    # 1) Unambiguous ISO-like → parse with dayfirst=False
    if ISO_PATTERN.match(s):
        return pd.to_datetime(s, errors="coerce", dayfirst=False)

    # 2) DMY-like → parse with dayfirst=True
    if DMY_PATTERN.match(s):
        return pd.to_datetime(s, errors="coerce", dayfirst=True)

    # 3) Try explicit strptime formats
    for fmt in DATE_FORMATS:
        try:
            return pd.to_datetime(datetime.strptime(s, fmt), errors="coerce")
        except Exception:
            pass

    # 4) Fallback infer (no dayfirst to avoid warnings)
    return pd.to_datetime(s, errors="coerce")

def sniff_delimiter(path: str, encodings=ENCODINGS_TO_TRY) -> str:
    """Detect delimiter as comma/tab/semicolon/pipe; default to comma."""
    for enc in encodings:
        try:
            with open(path, "r", encoding=enc, newline="") as f:
                sample = f.read(SNIFFER_BYTES)
                if not sample:
                    continue
                try:
                    dialect = csv.Sniffer().sniff(sample, delimiters=[",", "\t", ";", "|"])
                    return dialect.delimiter
                except Exception:
                    pass
        except Exception:
            continue
    ext = os.path.splitext(path)[1].lower()
    return "\t" if ext == ".tsv" else ","


def read_company_df(path: str) -> pd.DataFrame:
    """Robust reader: detect delimiter/encoding; ensure required columns; coerce types; drop bad rows."""
    delimiter = sniff_delimiter(path)
    last_err = None
    df = None

    for enc in ENCODINGS_TO_TRY:
        try:
            df = pd.read_csv(path, sep=delimiter, encoding=enc)
            break
        except Exception as e:
            last_err = e
            continue

    if df is None:
        raise RuntimeError(f"Failed to read {os.path.basename(path)}: {last_err}")

    # Strip column names and normalize whitespace
    df.columns = [str(c).strip() for c in df.columns]

    # Determine columns for nodes and required fields
    required_cols = {
        "sender_name": "Sender Name",
        "receiver_name": "Receiver Name",
        "invoice_value": "Invoice Value",
        "invoice_date": "Invoice date",
    }
    gstin_rec_col = "GSTIN/UIN of Recipient"

    # Check required existence
    for _, col in required_cols.items():
        if col not in df.columns:
            raise ValueError(f"Missing required column '{col}' in {os.path.basename(path)}")

    # Clean basic fields
    df[required_cols["sender_name"]] = df[required_cols["sender_name"]].astype(str).str.strip()
    df[required_cols["receiver_name"]] = df[required_cols["receiver_name"]].astype(str).str.strip()

    # Parse value
    def to_float(x):
        try:
            return float(str(x).replace(",", "").strip())
        except Exception:
            return math.nan

    df[required_cols["invoice_value"]] = df[required_cols["invoice_value"]].apply(to_float)

    # Parse dates
    df[required_cols["invoice_date"]] = df[required_cols["invoice_date"]].apply(try_parse_date)

    # Drop rows without sender/receiver or date/value
    df = df.dropna(subset=[
        required_cols["sender_name"],
        required_cols["receiver_name"],
        required_cols["invoice_date"],
        required_cols["invoice_value"]
    ])
    df = df[df[required_cols["invoice_value"]] >= MIN_EDGE_VALUE]

    # Light rename map for ease
    df = df.rename(columns={
        required_cols["sender_name"]: "SenderName",
        required_cols["receiver_name"]: "ReceiverName",
        required_cols["invoice_value"]: "InvoiceValue",
        required_cols["invoice_date"]: "InvoiceDate"
    })

    # Optional helpers
    if "Invoice Number" in df.columns:
        df["InvoiceNumber"] = df["Invoice Number"].astype(str).str.strip()
    else:
        df["InvoiceNumber"] = ""

    # Pass-through columns if present
    pass_cols = {
        "Rate": "Rate",
        "Taxable Value": "TaxableValue",
        "Place Of Supply": "PlaceOfSupply",
        "Reverse Charge": "ReverseCharge",
        "Invoice Type": "InvoiceType",
        "E-Commerce GSTIN": "ECommGSTIN",
    }
    for src, dst in pass_cols.items():
        df[dst] = df[src] if src in df.columns else None

    # GSTIN logic (optional as node ID fallback)
    if gstin_rec_col in df.columns:
        df["RecipientGSTIN"] = df[gstin_rec_col].astype(str).str.strip()
    else:
        df["RecipientGSTIN"] = ""

    return df.reset_index(drop=True)

# graph creation

def pick_node_ids(df: pd.DataFrame, use_gstin=USE_GSTIN_AS_NODES):
    """Prefer GSTIN sender/receiver columns when available; else use names."""
    sender_field = "SenderName"
    receiver_field = "ReceiverName"

    sender_gstin_candidates = [c for c in df.columns if "GSTIN" in c and "Sender" in c]
    receiver_gstin_candidates = [c for c in df.columns if "GSTIN" in c and ("Receiver" in c or "Recipient" in c)]

    if use_gstin and sender_gstin_candidates and receiver_gstin_candidates:
        s_col, r_col = sender_gstin_candidates[0], receiver_gstin_candidates[0]
        if s_col in df.columns and r_col in df.columns:
            sender_field, receiver_field = s_col, r_col

    return sender_field, receiver_field


def build_digraph(df: pd.DataFrame) -> nx.MultiDiGraph:
    """MultiDiGraph keeps multiple invoices between same parties."""
    G = nx.MultiDiGraph()
    sender_field, receiver_field = pick_node_ids(df)

    for _, row in df.iterrows():
        s = str(row[sender_field]).strip()
        r = str(row[receiver_field]).strip()
        if not s or not r:
            continue

        attrs = {
            "value": float(row["InvoiceValue"]),
            "date": row["InvoiceDate"],
            "invoice_number": str(row.get("InvoiceNumber", "")),
            "rate": row.get("Rate", None),
            "taxable_value": row.get("TaxableValue", None),
            "place_of_supply": row.get("PlaceOfSupply", None),
            "reverse_charge": row.get("ReverseCharge", None),
            "invoice_type": row.get("InvoiceType", None),
            "ecommerce_gstin": row.get("ECommGSTIN", None),
            "recipient_gstin": row.get("RecipientGSTIN", None),
        }
        G.add_edge(s, r, **attrs)

    return G

# cycles

def edges_for_step(G: nx.MultiDiGraph, u: str, v: str) -> List[Dict[str, Any]]:
    result = []
    if G.has_edge(u, v):
        for k in G[u][v]:
            d = G[u][v][k]
            result.append({"u": u, "v": v, **d})
    return result


def choose_edge_instance(edge_list: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    """Choose invoice closest to median date; fallback to highest value."""
    if not edge_list:
        return None
    dates = []
    for e in edge_list:
        dt = e.get("date")
        if isinstance(dt, (pd.Timestamp, datetime)) and pd.notna(dt):
            dates.append(pd.to_datetime(dt))
    if dates:
        median = pd.Series(dates).sort_values().iloc[len(dates)//2]
        return min(edge_list, key=lambda e: abs(pd.to_datetime(e.get("date")) - median)
                   if e.get("date") is not None else pd.Timedelta.max)
    return max(edge_list, key=lambda e: e.get("value", 0.0) if math.isfinite(e.get("value", 0.0)) else -1)


def cycle_with_attrs(G: nx.MultiDiGraph, cycle_nodes: List[str]) -> List[Dict[str, Any]]:
    edges = []
    L = len(cycle_nodes)
    for i in range(L):
        u = cycle_nodes[i]
        v = cycle_nodes[(i + 1) % L]
        candidates = edges_for_step(G, u, v)
        if not candidates:
            return []
        pick = choose_edge_instance(candidates)
        if pick is None:
            return []
        edges.append(pick)
    return edges


def within_time_window(edges: List[Dict[str, Any]], max_days: int) -> bool:
    dates = [pd.to_datetime(e.get("date")) for e in edges]
    if any(pd.isna(d) for d in dates):
        return False
    span = (max(dates) - min(dates)).days
    return span <= max_days


def values_roundtrip_similarity(edges: List[Dict[str, Any]], tol: float) -> float:
    vals = [float(e.get("value", 0.0)) for e in edges]
    if not vals or any(not math.isfinite(v) for v in vals):
        return 0.0
    med = float(pd.Series(vals).median())
    if med <= 0:
        return 0.0
    ok = sum(1 for v in vals if abs(v - med) <= tol * med)
    return ok / len(vals)


def reciprocity_ratio(G: nx.MultiDiGraph) -> float:
    if G.number_of_edges() == 0:
        return 0.0
    seen = set()
    bi = 0
    for u, v in G.edges():
        if (u, v) in seen or (v, u) in seen:
            continue
        seen.add((u, v))
        if G.has_edge(v, u):
            bi += 1
    return bi / max(1, len(seen))


def company_risk_score(cycles: List[Dict[str, Any]], G: nx.MultiDiGraph) -> float:
    if not cycles:
        return round(100 * (0.4 * reciprocity_ratio(G)), 2)

    lengths = [c["cycle_length"] for c in cycles]
    sims = [c["value_similarity"] for c in cycles]
    edges_in_cycles = set()
    for c in cycles:
        for e in c["edges"]:
            edges_in_cycles.add((e["u"], e["v"], e.get("invoice_number", "")))
    share = len(edges_in_cycles) / max(1, G.number_of_edges())
    rec = reciprocity_ratio(G)

    score = (
        0.35 * min(1.0, len(cycles) / 20.0) +
        0.20 * (sum(sims) / max(1, len(sims))) +
        0.20 * (sum(lengths) / max(1, len(lengths)) / 6.0) +
        0.15 * share +
        0.10 * rec
    )
    return round(100 * score, 2)

# per company analysis

def analyze_company_file(path: str) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    df = read_company_df(path)
    if df.empty:
        return pd.DataFrame(), {
            "file": os.path.basename(path),
            "status": "empty_or_invalid",
            "risk_score": 0.0
        }

    G = build_digraph(df)

    n_nodes = G.number_of_nodes()
    n_edges = G.number_of_edges()

    # Use a condensed simple DiGraph for cycle enumeration (faster)
    simple_G = nx.DiGraph()
    simple_G.add_edges_from([(u, v) for u, v in G.edges()])

    cycles_found: List[Dict[str, Any]] = []
    count = 0

    for cyc in nx.simple_cycles(simple_G):
        L = len(cyc)
        if L < 2 or L > MAX_CYCLE_LENGTH:
            continue

        chosen_edges = cycle_with_attrs(G, cyc)
        if not chosen_edges:
            continue

        if not within_time_window(chosen_edges, TIME_WINDOW_DAYS):
            continue

        sim = values_roundtrip_similarity(chosen_edges, VALUE_SIMILARITY_TOL)
        if sim <= 0.5:  # require >=50% edges close to median
            continue

        total_val = sum(float(e.get("value", 0.0)) for e in chosen_edges)
        dates = [pd.to_datetime(e["date"]) for e in chosen_edges]
        min_date = min(dates)
        max_date = max(dates)

        cycles_found.append({
            "cycle_nodes": cyc,
            "cycle_length": L,
            "value_similarity": round(sim, 3),
            "total_value": round(total_val, 2),
            "start_date": str(min_date.date()),
            "end_date": str(max_date.date()),
            "days_span": (max_date - min_date).days,
            "edges": chosen_edges,
        })

        count += 1
        if count >= MAX_CYCLES_PER_GRAPH:
            break

    # Flatten cycles to a row-per-edge report
    rows = []
    for c in cycles_found:
        for idx, e in enumerate(c["edges"], 1):
            rows.append({
                "cycle_id": f"{hash(tuple(c['cycle_nodes']))}_{idx}",
                "hop": idx,
                "from": e["u"],
                "to": e["v"],
                "invoice_number": e.get("invoice_number", ""),
                "invoice_date": str(pd.to_datetime(e.get("date")).date()) if e.get("date") else "",
                "invoice_value": e.get("value", ""),
                "cycle_length": c["cycle_length"],
                "value_similarity": c["value_similarity"],
                "total_cycle_value": c["total_value"],
                "cycle_start_date": c["start_date"],
                "cycle_end_date": c["end_date"],
                "cycle_days_span": c["days_span"],
            })
    cycles_df = pd.DataFrame(rows)

    summary = {
        "file": os.path.basename(path),
        "nodes": n_nodes,
        "edges": n_edges,
        "cycles_detected": len(cycles_found),
        "max_cycle_length_checked": MAX_CYCLE_LENGTH,
        "time_window_days": TIME_WINDOW_DAYS,
        "value_similarity_tolerance": VALUE_SIMILARITY_TOL,
        "reciprocity_ratio": round(reciprocity_ratio(G), 4),
        "risk_score": company_risk_score(cycles_found, G),
        "notes": "Higher risk_score indicates tighter/more cycles and reciprocated flows."
    }

    return cycles_df, summary

def run_detection():
    os.makedirs(OUT_FOLDER, exist_ok=True)
    files = sorted([
        os.path.join(DATA_FOLDER, f) for f in os.listdir(DATA_FOLDER)
        if os.path.isfile(os.path.join(DATA_FOLDER, f)) and f.lower().endswith((".csv", ".tsv"))
    ])

    if not files:
        print(f"No CSV/TSV files found in {DATA_FOLDER}")
        return

    for path in tqdm(files, desc="Analyzing companies", unit="file"):
        base = os.path.splitext(os.path.basename(path))[0]
        report_path = os.path.join(OUT_FOLDER, f"{base}_report.csv")
        summary_path = os.path.join(OUT_FOLDER, f"{base}_summary.json")

        try:
            cycles_df, summary = analyze_company_file(path)

            # Ensure consistent header even if empty
            if cycles_df.empty:
                cycles_df = pd.DataFrame(columns=[
                    "cycle_id","hop","from","to","invoice_number","invoice_date","invoice_value",
                    "cycle_length","value_similarity","total_cycle_value",
                    "cycle_start_date","cycle_end_date","cycle_days_span"
                ])

            cycles_df.to_csv(report_path, index=False, encoding="utf-8")

            with open(summary_path, "w", encoding="utf-8") as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)

        except Exception as ex:
            with open(summary_path, "w", encoding="utf-8") as f:
                json.dump({
                    "file": os.path.basename(path),
                    "status": "error",
                    "error": str(ex)
                }, f, ensure_ascii=False, indent=2)



if __name__ == "__main__":
    # 1) Run detection for all companies
    run_detection()

"""# Model Implementation

### GraphSAGE + Louvain and Leiden for cluster detection and LOF for anomaly detection
"""

!pip install python-igraph leidenalg torch_geometric

!pip install python-louvain

# detect_fraud_fullbatch.py
# Unsupervised fraud flagging using Louvain/Leiden + full-batch GraphSAGE (no GraphSAINT deps)
# Input: folder "b2b_latest_transactions_circular_time_cleaned" with CSVs
# Output: prints 'fraudulent_companies' list and writes fraudulent_companies.csv

import os
import glob
import math
import warnings
from datetime import datetime

import pandas as pd
import numpy as np
import networkx as nx

# Communities
import igraph as ig
import leidenalg as la
import community.community_louvain as community_louvain # python-louvain

# ML / GNN
import torch
from torch import nn
import torch.nn.functional as F
from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import StandardScaler

from torch_geometric.data import Data
from torch_geometric.nn import SAGEConv

warnings.filterwarnings("ignore", category=FutureWarning)

FOLDER = "/content/drive/My Drive/Capstone/B2B_CircularTrading/b2b_latest_transactions_circular_time_cleaned"

EXPECTED_COLS = [
    "GSTIN/UIN of Recipient","Receiver Name","Sender Name","Invoice Number",
    "Invoice date","Invoice Value","Place Of Supply","Reverse Charge",
    "Applicable % of Tax Rate","Invoice Type","E-Commerce GSTIN",
    "Rate","Taxable Value","Cess Amount"
]

def parse_invoice_dates(series: pd.Series) -> pd.Series:
    # Try strict ISO first
    s1 = pd.to_datetime(series, errors="coerce", format="%Y-%m-%d")
    # Where it failed, try day-first (handles 02/01/2025, 2-1-25, etc.)
    mask = s1.isna()
    if mask.any():
        s2 = pd.to_datetime(series[mask], errors="coerce", dayfirst=True)
        s1.loc[mask] = s2
    return s1

def read_all_csvs(folder):
    files = sorted(glob.glob(os.path.join(folder, "*.csv")))
    if not files:
        raise FileNotFoundError(f"No CSV files found in: {folder}")
    for fp in files:
        try:
            df = pd.read_csv(fp, dtype=str)
        except Exception:
            df = pd.read_csv(fp, low_memory=False, dtype=str)

        # Normalize columns
        for c in EXPECTED_COLS:
            if c not in df.columns:
                df[c] = np.nan
        df = df[EXPECTED_COLS].copy()

        # Numerics
        for col in ["Invoice Value","Applicable % of Tax Rate","Rate","Taxable Value","Cess Amount"]:
            s = (df[col].astype(str)
                       .str.replace(",", "", regex=False)
                       .str.extract(r"([-+]?\d*\.?\d+)", expand=False))
            df[col] = pd.to_numeric(s, errors="coerce")

        # Dates
        df["Invoice date"] = parse_invoice_dates(df["Invoice date"])
        yield os.path.basename(fp), df

def build_graph():
    G = nx.DiGraph()
    node_feats = {}
    gstin_to_receiver_name = {}
    sender_nodes_to_company = {}
    min_date, max_date = None, None

    for fname, df in read_all_csvs(FOLDER):
        df["Sender Name"] = df["Sender Name"].fillna("").str.strip()
        df["GSTIN/UIN of Recipient"] = df["GSTIN/UIN of Recipient"].fillna("").str.strip()
        df["Receiver Name"] = df["Receiver Name"].fillna("").str.strip()
        df["Invoice Number"] = df["Invoice Number"].fillna("").str.strip()
        df["Place Of Supply"] = df["Place Of Supply"].fillna("").str.strip()
        df["Invoice Type"] = df["Invoice Type"].fillna("").str.strip()
        df["Reverse Charge"] = df["Reverse Charge"].astype(str).str.upper().str.strip()

        dmin = df["Invoice date"].min()
        dmax = df["Invoice date"].max()
        if pd.notnull(dmin): min_date = dmin if min_date is None else min(min_date, dmin)
        if pd.notnull(dmax): max_date = dmax if max_date is None else max(max_date, dmax)

        for _, r in df.iterrows():
            sender = r["Sender Name"] or "UNKNOWN_SENDER"
            recip_gstin = r["GSTIN/UIN of Recipient"] or "UNKNOWN_GSTIN"
            recv_name = r["Receiver Name"] or ""
            inv_date = r["Invoice date"]
            inv_val = float(r["Invoice Value"]) if pd.notnull(r["Invoice Value"]) else 0.0
            rate = float(r["Rate"]) if pd.notnull(r["Rate"]) else np.nan
            taxable = float(r["Taxable Value"]) if pd.notnull(r["Taxable Value"]) else np.nan
            cess = float(r["Cess Amount"]) if pd.notnull(r["Cess Amount"]) else 0.0
            rev_chg = str(r["Reverse Charge"])
            rev_chg_bool = 1.0 if rev_chg in {"Y","YES","1","TRUE"} else 0.0
            pos = r["Place Of Supply"]
            inv_type = r["Invoice Type"] or ""
            ecomm_gstin = r["E-Commerce GSTIN"] or ""

            s_node = f"SENDER::{sender}"
            t_node = f"GSTIN::{recip_gstin}"

            if recip_gstin:
                gstin_to_receiver_name[recip_gstin] = recv_name
            sender_nodes_to_company[s_node] = sender

            if s_node not in G:
                G.add_node(s_node, ntype="sender")
            if t_node not in G:
                G.add_node(t_node, ntype="recipient")

            edge_attr = {
                "invoice_value": inv_val,
                "rate": 0.0 if math.isnan(rate) else rate,
                "taxable": 0.0 if math.isnan(taxable) else taxable,
                "cess_amount": cess,
                "reverse_charge": rev_chg_bool,
                "place_of_supply": pos,
                "invoice_type": inv_type,
                "ecomm_gstin": ecomm_gstin,
                "ts": pd.Timestamp(inv_date).value if pd.notnull(inv_date) else np.nan
            }
            G.add_edge(s_node, t_node, **edge_attr)

            for nid, is_sender in [(s_node, True), (t_node, False)]:
                d = node_feats.get(nid, {
                    "sum_value_sent": 0.0, "sum_value_recv": 0.0,
                    "n_invoices_sent": 0,  "n_invoices_recv": 0,
                    "sum_rate_sent": 0.0,  "sum_rate_recv": 0.0,
                    "sum_taxable_sent": 0.0, "sum_taxable_recv": 0.0,
                    "sum_cess_sent": 0.0,  "sum_cess_recv": 0.0,
                    "sum_revchg_sent": 0.0, "sum_revchg_recv": 0.0,
                    "partners": set(), "pos": set(), "types": set(),
                    "first_ts": None, "last_ts": None
                })
                if is_sender:
                    d["sum_value_sent"] += inv_val
                    d["n_invoices_sent"] += 1
                    d["sum_rate_sent"] += 0.0 if math.isnan(rate) else rate
                    d["sum_taxable_sent"] += 0.0 if math.isnan(taxable) else taxable
                    d["sum_cess_sent"] += cess
                    d["sum_revchg_sent"] += rev_chg_bool
                    d["partners"].add(t_node)
                else:
                    d["sum_value_recv"] += inv_val
                    d["n_invoices_recv"] += 1
                    d["sum_rate_recv"] += 0.0 if math.isnan(rate) else rate
                    d["sum_taxable_recv"] += 0.0 if math.isnan(taxable) else taxable
                    d["sum_cess_recv"] += cess
                    d["sum_revchg_recv"] += rev_chg_bool
                    d["partners"].add(s_node)
                if pos: d["pos"].add(pos)
                if inv_type: d["types"].add(inv_type)
                ts = edge_attr["ts"]
                if not math.isnan(ts):
                    d["first_ts"] = ts if d["first_ts"] is None else min(d["first_ts"], ts)
                    d["last_ts"]  = ts if d["last_ts"]  is None else max(d["last_ts"], ts)
                node_feats[nid] = d

    return G, node_feats, sender_nodes_to_company, gstin_to_receiver_name, min_date, max_date

def finalize_node_features(G, node_feats):
    nodes = sorted(G.nodes())
    feats = []
    for nid in nodes:
        d = node_feats.get(nid, {})
        sent = d.get("n_invoices_sent", 0)
        recv = d.get("n_invoices_recv", 0)
        partners = len(d.get("partners", set()))
        pos_ct = len(d.get("pos", set()))
        types_ct = len(d.get("types", set()))
        dur_days = 0.0
        if d.get("first_ts") is not None and d.get("last_ts") is not None:
            dur_days = (d["last_ts"] - d["first_ts"]) / (1e9 * 60 * 60 * 24)

        in_deg = G.in_degree(nid)
        out_deg = G.out_degree(nid)
        tot_deg = in_deg + out_deg

        avg_rate_sent = d.get("sum_rate_sent", 0.0) / max(1, sent)
        avg_rate_recv = d.get("sum_rate_recv", 0.0) / max(1, recv)
        avg_taxable_sent = d.get("sum_taxable_sent", 0.0) / max(1, sent)
        avg_taxable_recv = d.get("sum_taxable_recv", 0.0) / max(1, recv)
        avg_value_sent = d.get("sum_value_sent", 0.0) / max(1, sent)
        avg_value_recv = d.get("sum_value_recv", 0.0) / max(1, recv)
        revchg_ratio_sent = d.get("sum_revchg_sent", 0.0) / max(1, sent)
        revchg_ratio_recv = d.get("sum_revchg_recv", 0.0) / max(1, recv)

        feats.append([
            sent, recv, partners, pos_ct, types_ct, dur_days,
            in_deg, out_deg, tot_deg,
            d.get("sum_value_sent", 0.0), d.get("sum_value_recv", 0.0),
            avg_value_sent, avg_value_recv,
            avg_rate_sent, avg_rate_recv,
            avg_taxable_sent, avg_taxable_recv,
            d.get("sum_cess_sent", 0.0), d.get("sum_cess_recv", 0.0),
            revchg_ratio_sent, revchg_ratio_recv
        ])
    X = np.array(feats, dtype=np.float32)
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    return nodes, torch.tensor(X, dtype=torch.float)

def run_louvain_labels(G, nodes):
    UG = G.to_undirected()
    part = community_louvain.best_partition(UG)
    labels = torch.tensor([part.get(n, -1) for n in nodes], dtype=torch.long)
    if (labels < 0).any():
        maxlab = labels.max().item() if labels.numel() else 0
        labels[labels < 0] = maxlab + 1
    return labels

def run_leiden_partition(G, nodes):
    UG = G.to_undirected()
    mapping = {n:i for i,n in enumerate(UG.nodes())}
    edges = [(mapping[u], mapping[v]) for u,v in UG.edges()]
    ig_g = ig.Graph(n=len(mapping), edges=edges, directed=False)
    part = la.find_partition(ig_g, la.RBConfigurationVertexPartition)
    inv_mapping = {i:n for n,i in mapping.items()}
    node_to_com = {}
    for com_id, members in enumerate(part):
        for i in members:
            node_to_com[inv_mapping[i]] = com_id
    labels = torch.tensor([node_to_com.get(n, -1) for n in nodes], dtype=torch.long)
    if (labels < 0).any():
        maxlab = labels.max().item() if labels.numel() else 0
        labels[labels < 0] = maxlab + 1
    return labels

class SAGE(nn.Module):
    def __init__(self, in_dim, hid=128, out_dim=128, n_classes=None):
        super().__init__()
        self.conv1 = SAGEConv(in_dim, hid)
        self.conv2 = SAGEConv(hid, out_dim)
        self.classifier = nn.Linear(out_dim, n_classes) if n_classes is not None else None

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index); x = F.relu(x)
        x = self.conv2(x, edge_index); z = F.relu(x)
        if self.classifier is not None:
            yhat = self.classifier(z)
            return z, yhat
        return z, None

def train_fullbatch(data, pseudo_labels, epochs=80, lr=1e-3, weight_decay=1e-4):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    data = data.to(device)
    y = pseudo_labels.to(device)
    n_classes = int(y.max().item()) + 1

    model = SAGE(in_dim=data.x.size(1), hid=128, out_dim=128, n_classes=n_classes).to(device)
    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

    model.train()
    for _ in range(epochs):
        z, yhat = model(data.x, data.edge_index)
        loss = F.cross_entropy(yhat, y)
        opt.zero_grad(); loss.backward(); opt.step()

    model.eval()
    with torch.no_grad():
        z, _ = model(data.x, data.edge_index)
    return z.cpu().numpy()

def lof_anomaly_scores(embeddings, n_neighbors=25):
    k = min(n_neighbors, max(5, embeddings.shape[0]-1))
    lof = LocalOutlierFactor(n_neighbors=k, novelty=False)
    lof.fit(embeddings)
    return -lof.negative_outlier_factor_

def aggregate_company_scores(nodes, sender_nodes_to_company, node_scores):
    comp_to_scores = {}
    for idx, nid in enumerate(nodes):
        if nid.startswith("SENDER::"):
            comp = sender_nodes_to_company.get(nid, nid.replace("SENDER::",""))
            comp_to_scores.setdefault(comp, []).append(float(node_scores[idx]))
    comp_rank = []
    for comp, arr in comp_to_scores.items():
        arr = np.array(arr, dtype=float)
        score = 0.7*np.max(arr) + 0.3*np.median(arr)
        comp_rank.append((comp, score))
    comp_rank.sort(key=lambda x: x[1], reverse=True)
    return comp_rank

def main():
    print("[*] Building graph from CSVs (no master dataset on disk)...")
    G, node_feats, sender_nodes_to_company, gstin_to_receiver_name, dmin, dmax = build_graph()
    print(f"[*] Graph: |V|={G.number_of_nodes()} |E|={G.number_of_edges()}")
    if dmin is not None and dmax is not None:
        print(f"[*] Date span: {pd.to_datetime(dmin).date()} to {pd.to_datetime(dmax).date()}")

    print("[*] Finalizing node features...")
    nodes, X = finalize_node_features(G, node_feats)

    print("[*] Converting to PyG Data...")
    nidx = {n:i for i,n in enumerate(nodes)}
    ei_src, ei_dst = [], []
    for u, v in G.edges():
        ei_src.append(nidx[u]); ei_dst.append(nidx[v])
    edge_index = torch.tensor([ei_src, ei_dst], dtype=torch.long)
    pyg_data = Data(x=X, edge_index=edge_index)

    print("[*] Running Louvain & Leiden communities...")
    louvain_labels = run_louvain_labels(G, nodes)
    leiden_labels  = run_leiden_partition(G, nodes)

    print("[*] Training GraphSAGE full-batch (pseudo-labels from Louvain)...")
    embeddings = train_fullbatch(pyg_data, louvain_labels, epochs=80, lr=0.001)

    print("[*] Scoring anomalies via LOF on embeddings + consensus penalty...")
    node_scores = lof_anomaly_scores(embeddings, n_neighbors=25)

    UG = G.to_undirected()
    louv_map = {n:int(l) for n,l in zip(nodes, louvain_labels.tolist())}
    leid_map = {n:int(l) for n,l in zip(nodes, leiden_labels.tolist())}

    consensus_penalty = np.zeros(len(nodes), dtype=float)
    for i, n in enumerate(nodes):
        nbrs = list(UG.neighbors(n))
        if not nbrs:
            consensus_penalty[i] = 0.2
            continue
        l_mismatch = sum(1 for nb in nbrs if louv_map.get(nb, -2) != louv_map.get(n, -1)) / len(nbrs)
        d_mismatch = sum(1 for nb in nbrs if leid_map.get(nb, -2) != leid_map.get(n, -1)) / len(nbrs)
        consensus_penalty[i] = 0.5*(l_mismatch + d_mismatch)

    final_node_score = node_scores + consensus_penalty

    print("[*] Aggregating node anomalies to companies (Sender Name)...")
    comp_rank = aggregate_company_scores(nodes, sender_nodes_to_company, final_node_score)

    if comp_rank:
        scores = np.array([s for _, s in comp_rank])
        q1, q3 = np.percentile(scores, [25, 75])
        iqr = q3 - q1
        robust_cut = q3 + 2.0*iqr
        topk_cut = np.percentile(scores, 95)
        thresh = max(robust_cut, topk_cut)
        fraudulent_companies = [c for c, s in comp_rank if s >= thresh]
    else:
        fraudulent_companies = []

    print("\n=== Fraudulent Companies (flagged) ===")
    print(fraudulent_companies)

    out_df = pd.DataFrame({
        "Company": [c for c,_ in comp_rank],
        "AnomalyScore": [float(s) for _,s in comp_rank],
        "FlaggedAsFraudulent": [c in set(fraudulent_companies) for c,_ in comp_rank],
    })
    out_df.to_csv("/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_companies_graphSAGE.csv", index=False)
    print("\nSaved: grpfraudulent_companies.csv")

if __name__ == "__main__":
    main()

# cycle_detection.py
# MODIFIED SCRIPT:
# Reads a fraud summary CSV, finds all associated cycles from
# heuristic reports, and then ADDS cycle membership columns
# back to the original fraud summary CSV.

import os
import pandas as pd

# === Configuration ===

# This is the folder where the *_report.csv files (from the heuristic script) are.
OUT_FOLDER = "/content/drive/My Drive/Capstone/B2B_CircularTrading/circular_detection_reports"

# This is the exact path to your GraphSAGE (or other GNN) output.
# THIS FILE WILL BE READ AND THEN OVERWRITTEN with new cycle columns.
FRAUD_SUMMARY_CSV = "/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_companies_graphSAGE.csv"

# === Helper Functions ===

def _group_key(cycle_id: str) -> str:
    """Extracts the base ID from a unique hop ID."""
    return cycle_id.rsplit("_", 1)[0] if isinstance(cycle_id, str) and "_" in cycle_id else str(cycle_id)

def _reconstruct_nodes(rows: pd.DataFrame) -> list:
    """Rebuilds a cycle path from a DataFrame of its edges."""
    # Prefer ordered hops if present
    if "hop" in rows.columns and rows["hop"].notna().all():
        hops = rows.sort_values("hop")
        seq = hops["from"].tolist()
        last_to = hops["to"].iloc[-1]
        if not seq or last_to != seq[0]:
            seq.append(last_to)
        return seq

    # Fallback adjacency stitch (if 'hop' column is missing)
    edges = list(zip(rows["from"], rows["to"]))
    if not edges:
        return []

    forward = {u: v for u, v in edges}
    tos = {v for _, v in edges}
    start = next((u for u, _ in edges if u not in tos), edges[0][0])

    seq = [start]
    seen = {start}
    while len(seq) <= len(edges) + 1:
        nxt = forward.get(seq[-1])
        if not nxt:
            break
        seq.append(nxt)
        if nxt in seen:
            break
        seen.add(nxt)
    return seq

def _canonical_cycle(nodes: list) -> tuple:
    """Returns a canonical tuple for a cycle to de-duplicate by rotation and direction."""
    if not nodes:
        return tuple()

    # Drop closing node if present (e.g., A->B->A becomes [A, B])
    if len(nodes) >= 2 and nodes[0] == nodes[-1]:
        nodes = nodes[:-1]

    if not nodes:
        return tuple()

    # Get all rotations
    rots = [tuple(nodes[i:] + nodes[:i]) for i in range(len(nodes))]

    # Get all reversed rotations
    r = list(reversed(nodes))
    r_rots = [tuple(r[i:] + r[:i]) for i in range(len(r))]

    # Return the lexicographically smallest version
    return min(rots + r_rots)

# === Main Execution (MODIFIED) ===

def main():
    """
    Reads the GNN output (FRAUD_SUMMARY_CSV), finds associated cycles,
    and then ADDS new 'In_Cycle_X' columns back to that same CSV.
    """
    print("\n" + "="*30)
    print("  RUNNING CYCLE DETECTOR & CSV UPDATER  ")
    print("="*30)

    if not os.path.exists(FRAUD_SUMMARY_CSV):
        print(f"Cycle Updater: Could not find {FRAUD_SUMMARY_CSV}")
        print("Run the GNN detection script first to create this file.")
        return

    # --- 1. Load the Fraud Summary CSV ---
    try:
        # We load this as 'df_to_update' because we will modify and re-save it
        df_to_update = pd.read_csv(FRAUD_SUMMARY_CSV)

        # Check for required columns
        if "Company" not in df_to_update.columns or "FlaggedAsFraudulent" not in df_to_update.columns:
             print(f"Cycle Updater: Error! {FRAUD_SUMMARY_CSV} is missing the")
             print("required 'Company' or 'FlaggedAsFraudulent' columns.")
             return

        # Ensure 'Company' column is string type for matching
        df_to_update["Company"] = df_to_update["Company"].astype(str)

        # Create a set of all companies where FlaggedAsFraudulent is True
        fraud_nodes = set(df_to_update[
            df_to_update["FlaggedAsFraudulent"] == True
        ]["Company"].dropna())

        if not fraud_nodes:
            print("Cycle Updater: No companies were flagged as fraudulent in the input file.")
            print("No cycles will be searched for.")
            return

        print(f"Cycle Updater: Loaded {len(fraud_nodes)} fraudulent companies from {FRAUD_SUMMARY_CSV}.")

    except Exception as e:
        print(f"Cycle Updater: Error reading {FRAUD_SUMMARY_CSV}: {e}")
        return

    # --- 2. Locate and scan all heuristic cycle reports ---
    report_paths = []
    if not os.path.exists(OUT_FOLDER):
        print(f"Cycle Updater: Report folder {OUT_FOLDER} not found.")
        print("Please ensure the heuristic script has run and saved reports here.")
        return

    for f in os.listdir(OUT_FOLDER):
        if f.endswith("_report.csv"):
            report_paths.append(os.path.join(OUT_FOLDER, f))

    if not report_paths:
        print(f"Cycle Updater: No '_report.csv' files found in {OUT_FOLDER}.")
        print("Cannot find any cycles to add to the summary file.")
        return

    # --- 3. Build the list of fraudulent cycles ---
    seen = set()
    cycles_out = []

    print(f"Cycle Updater: Scanning {len(report_paths)} report files for cycles...")
    for report_csv in report_paths:
        try:
            df = pd.read_csv(report_csv, dtype={"cycle_id": str, "from": str, "to": str})
        except Exception as e:
            continue

        if df.empty or not {"cycle_id", "from", "to"}.issubset(df.columns):
            continue

        df["cycle_group"] = df["cycle_id"].apply(_group_key)

        for _, gdf in df.groupby("cycle_group", sort=False):
            nodes = _reconstruct_nodes(gdf)

            if len(nodes) >= 2 and nodes[0] == nodes[-1]:
                nodes = nodes[:-1]

            # Keep cycles (len 3+) where ALL members are fraud nodes
            if len(nodes) >= 3 and all(n in fraud_nodes for n in nodes):
                key = _canonical_cycle(nodes)
                if key and key not in seen:
                    seen.add(key)
                    cycles_out.append(list(key))

    # Sort by length (longest first), then alphabetically
    cycles_out.sort(key=lambda c: (-len(c), c))

    if not cycles_out:
        print("Cycle Updater: No associated cycles (len 3+) were found.")
        print("CSV file will not be modified.")
        return

    print(f"Cycle Updater: Found {len(cycles_out)} unique cycles associated with flagged companies.")

    # --- 4. Add new cycle columns to the DataFrame ---

    # Remove any old 'In_Cycle_' columns to start fresh
    old_cycle_cols = [col for col in df_to_update.columns if col.startswith("In_Cycle_")]
    if old_cycle_cols:
        print(f"Cycle Updater: Removing {len(old_cycle_cols)} old cycle columns.")
        df_to_update = df_to_update.drop(columns=old_cycle_cols)

    # Add new columns for each cycle found
    for i, cycle_nodes in enumerate(cycles_out):
        col_name = f"In_Cycle_{i+1}"
        cycle_set = set(cycle_nodes)

        # Set column to True if 'Company' is in this cycle set, else False
        df_to_update[col_name] = df_to_update["Company"].isin(cycle_set)

    # --- 5. Save the updated DataFrame back to the CSV file ---
    try:
        df_to_update.to_csv(FRAUD_SUMMARY_CSV, index=False)
        print(f"\nSUCCESS: Updated {FRAUD_SUMMARY_CSV} with {len(cycles_out)} cycle columns.")
        print("\n--- Final CSV Head ---")
        print(df_to_update.head())
        print("------------------------")

    except Exception as e:
        print(f"Cycle Updater: CRITICAL ERROR: Could not save updated CSV to {FRAUD_SUMMARY_CSV}")
        print(f"Error: {e}")


if __name__ == "__main__":
    main()

"""### Graph Convolutional Network (GCN) + Louvain/Leiden for community detection"""

# detect_fraud_robust_nopyg.py
# Robust fraud detection WITHOUT torch_geometric (pure PyTorch GCN).
# FIXED FOR COLAB - No argparse issues

import os, glob, math, warnings, random, sys
from typing import List, Optional, Tuple
os.environ.setdefault("OMP_NUM_THREADS", "1")

import pandas as pd
import numpy as np
import networkx as nx
import igraph as ig
import leidenalg as la
import community.community_louvain as community_louvain # python-louvain

import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors
from sklearn.ensemble import IsolationForest

warnings.filterwarnings("ignore", category=FutureWarning)

# ============ CONFIG (Replace argparse for Colab) ============
class Config:
    # Data paths
    data_folder = "/content/drive/My Drive/Capstone/B2B_CircularTrading/b2b_latest_transactions_circular_time_cleaned"
    save_csv = "/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_companies_gcn.csv"

    # Threshold
    percentile = 94.0  # Top 6% as fraudulent (set None for robust rule)

    # GCN training
    epochs = 100
    lr = 1e-3
    weight_decay = 5e-4
    model_drop = 0.10
    feat_drop = 0.02
    gauss_std = 0.0
    edge_drop = 0.0
    label_smoothing = 0.02
    early_stop = 8
    seed = 42

    # Ensemble scoring
    lof_neighbors = 30
    knn_k = 30
    iforest_trees = 600
    embed_pca_dim = 64
    embed_jitter = 1e-6

args = Config()  # Use this instead of parse_args()

EXPECTED_COLS = [
    "GSTIN/UIN of Recipient","Receiver Name","Sender Name","Invoice Number",
    "Invoice date","Invoice Value","Place Of Supply","Reverse Charge",
    "Applicable % of Tax Rate","Invoice Type","E-Commerce GSTIN",
    "Rate","Taxable Value","Cess Amount"
]

# ------------- Utils -------------
def set_seed(seed: int):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)

def parse_invoice_dates(series: pd.Series) -> pd.Series:
    s1 = pd.to_datetime(series, errors="coerce", format="%Y-%m-%d")
    mask = s1.isna()
    if mask.any():
        s2 = pd.to_datetime(series[mask], errors="coerce", dayfirst=True)
        s1.loc[mask] = s2
    return s1

def read_all_csvs(folder):
    files = sorted(glob.glob(os.path.join(folder, "*.csv")))
    if not files:
        raise FileNotFoundError(f"No CSV files found in: {folder}")
    for fp in files:
        try:
            df = pd.read_csv(fp, dtype=str)
        except Exception:
            df = pd.read_csv(fp, low_memory=False, dtype=str)

        for c in EXPECTED_COLS:
            if c not in df.columns:
                df[c] = np.nan
        df = df[EXPECTED_COLS].copy()

        for col in ["Invoice Value","Applicable % of Tax Rate","Rate","Taxable Value","Cess Amount"]:
            s = (df[col].astype(str)
                       .str.replace(",", "", regex=False)
                       .str.extract(r"([-+]?\d*\.?\d+)", expand=False))
            df[col] = pd.to_numeric(s, errors="coerce")

        df["Invoice date"] = parse_invoice_dates(df["Invoice date"])
        yield os.path.basename(fp), df

# ------------- Graph build -------------
def build_graph(folder):
    G = nx.DiGraph()
    node_feats = {}
    sender_nodes_to_company = {}
    min_date, max_date = None, None

    for _, df in read_all_csvs(folder):
        df["Sender Name"] = df["Sender Name"].fillna("").str.strip()
        df["GSTIN/UIN of Recipient"] = df["GSTIN/UIN of Recipient"].fillna("").str.strip()
        df["Receiver Name"] = df["Receiver Name"].fillna("").str.strip()
        df["Place Of Supply"] = df["Place Of Supply"].fillna("").str.strip()
        df["Invoice Type"] = df["Invoice Type"].fillna("").str.strip()
        df["Reverse Charge"] = df["Reverse Charge"].astype(str).str.upper().str.strip()

        dmin = df["Invoice date"].min(); dmax = df["Invoice date"].max()
        if pd.notnull(dmin): min_date = dmin if min_date is None else min(min_date, dmin)
        if pd.notnull(dmax): max_date = dmax if max_date is None else max(max_date, dmax)

        for _, r in df.iterrows():
            sender = r["Sender Name"] or "UNKNOWN_SENDER"
            recip_gstin = r["GSTIN/UIN of Recipient"] or "UNKNOWN_GSTIN"
            inv_date = r["Invoice date"]
            inv_val = float(r["Invoice Value"]) if pd.notnull(r["Invoice Value"]) else 0.0
            rate = float(r["Rate"]) if pd.notnull(r["Rate"]) else np.nan
            taxable = float(r["Taxable Value"]) if pd.notnull(r["Taxable Value"]) else np.nan
            cess = float(r["Cess Amount"]) if pd.notnull(r["Cess Amount"]) else 0.0
            rev_chg_bool = 1.0 if str(r["Reverse Charge"]) in {"Y","YES","1","TRUE"} else 0.0
            pos = r["Place Of Supply"]; inv_type = r["Invoice Type"] or ""
            ecomm_gstin = r["E-Commerce GSTIN"] or ""

            s_node = f"SENDER::{sender}"
            t_node = f"GSTIN::{recip_gstin}"

            sender_nodes_to_company[s_node] = sender
            if s_node not in G: G.add_node(s_node, ntype="sender")
            if t_node not in G: G.add_node(t_node, ntype="recipient")

            edge_attr = {
                "invoice_value": inv_val,
                "rate": 0.0 if math.isnan(rate) else rate,
                "taxable": 0.0 if math.isnan(taxable) else taxable,
                "cess_amount": cess,
                "reverse_charge": rev_chg_bool,
                "place_of_supply": pos,
                "invoice_type": inv_type,
                "ecomm_gstin": ecomm_gstin,
                "ts": pd.Timestamp(inv_date).value if pd.notnull(inv_date) else np.nan
            }
            G.add_edge(s_node, t_node, **edge_attr)

            for nid, is_sender in [(s_node, True), (t_node, False)]:
                d = node_feats.get(nid, {
                    "sum_value_sent": 0.0, "sum_value_recv": 0.0,
                    "n_invoices_sent": 0,  "n_invoices_recv": 0,
                    "sum_rate_sent": 0.0,  "sum_rate_recv": 0.0,
                    "sum_taxable_sent": 0.0, "sum_taxable_recv": 0.0,
                    "sum_cess_sent": 0.0,  "sum_cess_recv": 0.0,
                    "sum_revchg_sent": 0.0, "sum_revchg_recv": 0.0,
                    "partners": set(), "pos": set(), "types": set(),
                    "first_ts": None, "last_ts": None
                })
                if is_sender:
                    d["sum_value_sent"] += inv_val; d["n_invoices_sent"] += 1
                    d["sum_rate_sent"] += 0.0 if math.isnan(rate) else rate
                    d["sum_taxable_sent"] += 0.0 if math.isnan(taxable) else taxable
                    d["sum_cess_sent"] += cess; d["sum_revchg_sent"] += rev_chg_bool
                    d["partners"].add(t_node)
                else:
                    d["sum_value_recv"] += inv_val; d["n_invoices_recv"] += 1
                    d["sum_rate_recv"] += 0.0 if math.isnan(rate) else rate
                    d["sum_taxable_recv"] += 0.0 if math.isnan(taxable) else taxable
                    d["sum_cess_recv"] += cess; d["sum_revchg_recv"] += rev_chg_bool
                    d["partners"].add(s_node)
                if pos: d["pos"].add(pos)
                if inv_type: d["types"].add(inv_type)
                ts = edge_attr["ts"]
                if not math.isnan(ts):
                    d["first_ts"] = ts if d["first_ts"] is None else min(d["first_ts"], ts)
                    d["last_ts"]  = ts if d["last_ts"]  is None else max(d["last_ts"], ts)
                node_feats[nid] = d

    return G, node_feats, sender_nodes_to_company, min_date, max_date

# ------------- Features -------------
def finalize_node_features(G, node_feats):
    nodes = sorted(G.nodes())
    feats = []
    for nid in nodes:
        d = node_feats.get(nid, {})
        sent = d.get("n_invoices_sent", 0); recv = d.get("n_invoices_recv", 0)
        partners = len(d.get("partners", set()))
        pos_ct = len(d.get("pos", set())); types_ct = len(d.get("types", set()))
        dur_days = 0.0
        if d.get("first_ts") is not None and d.get("last_ts") is not None:
            dur_days = (d["last_ts"] - d["first_ts"]) / (1e9 * 60 * 60 * 24)
        in_deg = G.in_degree(nid); out_deg = G.out_degree(nid); tot_deg = in_deg + out_deg

        avg_rate_sent = d.get("sum_rate_sent", 0.0) / max(1, sent)
        avg_rate_recv = d.get("sum_rate_recv", 0.0) / max(1, recv)
        avg_taxable_sent = d.get("sum_taxable_sent", 0.0) / max(1, sent)
        avg_taxable_recv = d.get("sum_taxable_recv", 0.0) / max(1, recv)
        avg_value_sent = d.get("sum_value_sent", 0.0) / max(1, sent)
        avg_value_recv = d.get("sum_value_recv", 0.0) / max(1, recv)
        revchg_ratio_sent = d.get("sum_revchg_sent", 0.0) / max(1, sent)
        revchg_ratio_recv = d.get("sum_revchg_recv", 0.0) / max(1, recv)

        feats.append([
            sent, recv, partners, pos_ct, types_ct, dur_days,
            in_deg, out_deg, tot_deg,
            d.get("sum_value_sent", 0.0), d.get("sum_value_recv", 0.0),
            avg_value_sent, avg_value_recv,
            avg_rate_sent, avg_rate_recv,
            avg_taxable_sent, avg_taxable_recv,
            d.get("sum_cess_sent", 0.0), d.get("sum_cess_recv", 0.0),
            revchg_ratio_sent, revchg_ratio_recv
        ])
    X = np.array(feats, dtype=np.float32)
    X = StandardScaler().fit_transform(X)
    return nodes, torch.tensor(X, dtype=torch.float)

# ------------- Communities -------------
def run_louvain_labels(G, nodes):
    UG = G.to_undirected()
    part = community_louvain.best_partition(UG)
    labels = torch.tensor([part.get(n, -1) for n in nodes], dtype=torch.long)
    if (labels < 0).any():
        maxlab = labels.max().item() if labels.numel() else 0
        labels[labels < 0] = maxlab + 1
    return labels

def run_leiden_partition(G, nodes):
    UG = G.to_undirected()
    mapping = {n:i for i,n in enumerate(UG.nodes())}
    edges = [(mapping[u], mapping[v]) for u,v in UG.edges()]
    ig_g = ig.Graph(n=len(mapping), edges=edges, directed=False)
    part = la.find_partition(ig_g, la.RBConfigurationVertexPartition)
    inv = {i:n for n,i in mapping.items()}
    node_to_com = {}
    for cid, members in enumerate(part):
        for i in members:
            node_to_com[inv[i]] = cid
    labels = torch.tensor([node_to_com.get(n, -1) for n in nodes], dtype=torch.long)
    if (labels < 0).any():
        maxlab = labels.max().item() if labels.numel() else 0
        labels[labels < 0] = maxlab + 1
    return labels

# ------------- Pure-PyTorch GCN -------------
class GCN(nn.Module):
    def __init__(self, in_dim, hid=128, out_dim=128, n_classes=None, p_drop=0.1):
        super().__init__()
        self.lin1 = nn.Linear(in_dim, hid, bias=False)
        self.lin2 = nn.Linear(hid, out_dim, bias=False)
        self.bn1 = nn.BatchNorm1d(hid)
        self.bn2 = nn.BatchNorm1d(out_dim)
        self.dropout = nn.Dropout(p_drop)
        self.classifier = nn.Linear(out_dim, n_classes) if n_classes is not None else None

    def forward(self, x, adj_norm, training=False):
        x = torch.sparse.mm(adj_norm, x)
        x = self.lin1(x); x = self.bn1(x); x = F.relu(x)
        x = self.dropout(x) if training else x

        x = torch.sparse.mm(adj_norm, x)
        x = self.lin2(x); x = self.bn2(x); z = F.relu(x)
        z = self.dropout(z) if training else z

        if self.classifier is not None:
            yhat = self.classifier(z)
            return z, yhat
        return z, None

def build_sparse_adj(n: int, edges: List[Tuple[int,int]], add_self_loops=True) -> torch.Tensor:
    edges_und = edges + [(j,i) for (i,j) in edges]
    if add_self_loops:
        edges_und += [(i,i) for i in range(n)]
    row = torch.tensor([i for i,j in edges_und], dtype=torch.long)
    col = torch.tensor([j for i,j in edges_und], dtype=torch.long)
    vals = torch.ones(len(edges_und), dtype=torch.float)
    adj = torch.sparse_coo_tensor(torch.stack([row, col]), vals, (n, n)).coalesce()
    deg = torch.sparse.sum(adj, dim=1).to_dense()
    deg_inv_sqrt = torch.pow(deg.clamp(min=1.0), -0.5)
    r, c = adj.indices()
    norm_vals = deg_inv_sqrt[r] * adj.values() * deg_inv_sqrt[c]
    adj_norm = torch.sparse_coo_tensor(adj.indices(), norm_vals, (n, n)).coalesce()
    return adj_norm

def train_gcn_regularized(
    X: torch.Tensor,
    edges: List[Tuple[int,int]],
    pseudo_labels: torch.Tensor,
    epochs=100, lr=1e-3, weight_decay=5e-4,
    p_model_drop=0.10, p_feat_drop=0.02, gauss_std=0.0, p_edge_drop=0.0,
    label_smoothing=0.02, early_stop_patience=8, seed=42
):
    set_seed(seed)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    X = X.to(device)
    y = pseudo_labels.to(device)
    n, in_dim = X.shape
    n_classes = int(y.max().item()) + 1

    model = GCN(in_dim, 128, 128, n_classes=n_classes, p_drop=p_model_drop).to(device)
    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

    X0 = X.clone().detach()
    edges0 = list(edges)

    best_loss = float("inf"); best_state = None; no_improve = 0

    for ep in range(1, epochs+1):
        x = X0.clone()
        if p_feat_drop > 0:
            mask = torch.rand_like(x) < p_feat_drop
            x = x.masked_fill(mask, 0.0)
        if gauss_std > 0:
            x = x + gauss_std * torch.randn_like(x)

        ed = edges0 if p_edge_drop <= 0 else [
            e for e in edges0 if np.random.rand() >= p_edge_drop
        ]
        adj_norm = build_sparse_adj(n, ed).to(device)

        model.train()
        z, yhat = model(x, adj_norm, training=True)
        loss = F.cross_entropy(yhat, y, label_smoothing=label_smoothing)
        opt.zero_grad(); loss.backward(); opt.step()

        cur = float(loss.item())
        if cur < best_loss - 1e-6:
            best_loss = cur
            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
            no_improve = 0
        else:
            no_improve += 1
            if no_improve >= early_stop_patience:
                break

    if best_state is not None:
        model.load_state_dict({k: v for k, v in best_state.items()})

    model.eval()
    with torch.no_grad():
        adj_norm_clean = build_sparse_adj(n, edges0).to(device)
        z, _ = model(X0, adj_norm_clean, training=False)
    return z.cpu().numpy()

# ------------- Scoring -------------
def rank_normalize(x: np.ndarray) -> np.ndarray:
    order = x.argsort()
    ranks = np.empty_like(order, dtype=float)
    ranks[order] = np.linspace(0, 1, len(x), endpoint=True)
    return ranks

def lof_score(emb, n_neighbors=30):
    k = min(max(5, n_neighbors), max(5, emb.shape[0]-1))
    lof = LocalOutlierFactor(n_neighbors=k, novelty=False, metric="euclidean")
    lof.fit(emb)
    return -lof.negative_outlier_factor_

def knn_distance_score(emb, k=30):
    k = min(max(5, k), max(5, emb.shape[0]-1))
    nn = NearestNeighbors(n_neighbors=k, metric="euclidean")
    nn.fit(emb)
    dists, _ = nn.kneighbors(emb)
    return dists[:, 1:].mean(axis=1) if dists.shape[1] > 1 else dists[:, 0]

def iforest_score(emb, trees=600, seed=42):
    iso = IsolationForest(
        n_estimators=trees, max_samples="auto", contamination="auto",
        random_state=seed, n_jobs=1
    )
    iso.fit(emb)
    return -iso.decision_function(emb)

def ensemble_anomaly_scores(emb, pca_dim=64, jitter=1e-6, lof_k=30, knn_k=30, if_trees=600, seed=42):
    d = min(pca_dim, emb.shape[1], max(8, emb.shape[1] // 2))
    pca = PCA(n_components=d, whiten=True, random_state=seed)
    z = pca.fit_transform(emb)
    if jitter and jitter > 0:
        z = z + np.random.default_rng(seed).normal(scale=jitter, size=z.shape)

    s_lof  = lof_score(z, n_neighbors=lof_k)
    s_knn  = knn_distance_score(z, k=knn_k)
    s_if   = iforest_score(z, trees=if_trees, seed=seed)

    s = 0.2*rank_normalize(s_lof) + 0.4*rank_normalize(s_knn) + 0.4*rank_normalize(s_if)
    return s

# ------------- Aggregation -------------
def aggregate_company_scores(nodes, sender_nodes_to_company, node_scores):
    comp_to_scores = {}
    for idx, nid in enumerate(nodes):
        if nid.startswith("SENDER::"):
            comp = sender_nodes_to_company.get(nid, nid.replace("SENDER::",""))
            comp_to_scores.setdefault(comp, []).append(float(node_scores[idx]))
    comp_rank = []
    for comp, arr in comp_to_scores.items():
        arr = np.array(arr, dtype=float)
        score = 0.6*np.max(arr) + 0.4*np.median(arr)
        comp_rank.append((comp, score))
    comp_rank.sort(key=lambda x: x[1], reverse=True)
    return comp_rank

# ------------- Main -------------
def main():
    set_seed(args.seed)

    print("[*] Building graph...")
    G, node_feats, sender_nodes_to_company, dmin, dmax = build_graph(args.data_folder)
    print(f"[*] Graph: |V|={G.number_of_nodes()} |E|={G.number_of_edges()}")
    if dmin is not None and dmax is not None:
        print(f"[*] Date span: {pd.to_datetime(dmin).date()} to {pd.to_datetime(dmax).date()}")

    print("[*] Finalizing features...")
    nodes, X = finalize_node_features(G, node_feats)

    print("[*] Communities (Louvain/Leiden)...")
    louvain_labels = run_louvain_labels(G, nodes)
    leiden_labels  = run_leiden_partition(G, nodes)

    print("[*] Preparing edges...")
    nidx = {n:i for i,n in enumerate(nodes)}
    edges = [(nidx[u], nidx[v]) for u,v in G.edges()]

    print("[*] Training GCN...")
    embeddings = train_gcn_regularized(
        X, edges, louvain_labels,
        epochs=args.epochs, lr=args.lr, weight_decay=args.weight_decay,
        p_model_drop=args.model_drop, p_feat_drop=args.feat_drop,
        gauss_std=args.gauss_std, p_edge_drop=args.edge_drop,
        label_smoothing=args.label_smoothing, early_stop_patience=args.early_stop,
        seed=args.seed
    )

    print("[*] Ensemble anomaly scoring...")
    node_scores_base = ensemble_anomaly_scores(
        embeddings,
        pca_dim=args.embed_pca_dim,
        jitter=args.embed_jitter,
        lof_k=args.lof_neighbors,
        knn_k=args.knn_k,
        if_trees=args.iforest_trees,
        seed=args.seed
    )

    print("[*] Adding consensus penalty...")
    UG = G.to_undirected()
    louv_map = {n:int(l) for n,l in zip(nodes, louvain_labels.tolist())}
    leid_map = {n:int(l) for n,l in zip(nodes, leiden_labels.tolist())}

    consensus_penalty = np.zeros(len(nodes), dtype=float)
    for i, n in enumerate(nodes):
        nbrs = list(UG.neighbors(n))
        if not nbrs:
            consensus_penalty[i] = 0.05
            continue
        l_mismatch = sum(1 for nb in nbrs if louv_map.get(nb, -2) != louv_map.get(n, -1)) / len(nbrs)
        d_mismatch = sum(1 for nb in nbrs if leid_map.get(nb, -2) != leid_map.get(n, -1)) / len(nbrs)
        consensus_penalty[i] = 0.2*(l_mismatch + d_mismatch)

    final_node_score = node_scores_base + consensus_penalty

    print("[*] Aggregating to companies...")
    comp_rank = aggregate_company_scores(nodes, sender_nodes_to_company, final_node_score)

    # Threshold
    if comp_rank:
        scores = np.array([s for _, s in comp_rank], dtype=float)
        if args.percentile is not None:
            thresh = np.percentile(scores, args.percentile)
            print(f"[*] Using percentile threshold: {args.percentile} -> score >= {thresh:.6f}")
        else:
            q1, q3 = np.percentile(scores, [25, 75]); iqr = q3 - q1
            p95 = np.percentile(scores, 95)
            thresh = max(q3 + 2.0*iqr, p95)
            print(f"[*] Using robust threshold: max(Q3+2*IQR, P95) = {thresh:.6f}")
        fraudulent_companies = [c for c, s in comp_rank if s >= thresh]
    else:
        fraudulent_companies = []

    print("\n=== Fraudulent Companies (flagged) ===")
    print(fraudulent_companies)

    out_df = pd.DataFrame({
        "Company": [c for c,_ in comp_rank],
        "AnomalyScore": [float(s) for _,s in comp_rank],
        "FlaggedAsFraudulent": [c in set(fraudulent_companies) for c,_ in comp_rank],
    })
    out_df.to_csv(args.save_csv, index=False)
    print(f"\nSaved: {args.save_csv}")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"[ERROR] {e}")
        import traceback
        traceback.print_exc()

# Run directly in Colab
main()

# cycle_detection_gcn_csv_update.py
# MODIFIED SCRIPT:
# Reads a GCN fraud summary CSV, finds all associated cycles (where AT LEAST ONE
# member is fraudulent), and then ADDS cycle membership columns
# back to the original fraud summary CSV.

import os
import pandas as pd

# === Configuration ===

# This is the folder where the *_report.csv files (from the heuristic script) are.
OUT_FOLDER = "/content/drive/My Drive/Capstone/B2B_CircularTrading/circular_detection_reports"

# This is the exact path to your GCN output.
# THIS FILE WILL BE READ AND THEN OVERWRITTEN with new cycle columns.
FRAUD_SUMMARY_CSV = "/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_companies_gcn.csv"

# === Helper Functions ===

def _group_key(cycle_id: str) -> str:
    """Extracts the base ID from a unique hop ID."""
    return cycle_id.rsplit("_", 1)[0] if isinstance(cycle_id, str) and "_" in cycle_id else str(cycle_id)

def _reconstruct_nodes(rows: pd.DataFrame) -> list:
    """Rebuilds a cycle path from a DataFrame of its edges."""
    # Prefer ordered hops if present
    if "hop" in rows.columns and rows["hop"].notna().all():
        hops = rows.sort_values("hop")
        seq = hops["from"].tolist()
        last_to = hops["to"].iloc[-1]
        if not seq or last_to != seq[0]:
            seq.append(last_to)
        return seq

    # Fallback adjacency stitch (if 'hop' column is missing)
    edges = list(zip(rows["from"], rows["to"]))
    if not edges:
        return []

    forward = {u: v for u, v in edges}
    tos = {v for _, v in edges}
    start = next((u for u, _ in edges if u not in tos), edges[0][0])

    seq = [start]
    seen = {start}
    while len(seq) <= len(edges) + 1:
        nxt = forward.get(seq[-1])
        if not nxt:
            break
        seq.append(nxt)
        if nxt in seen:
            break
        seen.add(nxt)
    return seq

def _canonical_cycle(nodes: list) -> tuple:
    """Returns a canonical tuple for a cycle to de-duplicate by rotation and direction."""
    if not nodes:
        return tuple()

    # Drop closing node if present (e.g., A->B->A becomes [A, B])
    if len(nodes) >= 2 and nodes[0] == nodes[-1]:
        nodes = nodes[:-1]

    if not nodes:
        return tuple()

    # Get all rotations
    rots = [tuple(nodes[i:] + nodes[:i]) for i in range(len(nodes))]

    # Get all reversed rotations
    r = list(reversed(nodes))
    r_rots = [tuple(r[i:] + r[:i]) for i in range(len(r))]

    # Return the lexicographically smallest version
    return min(rots + r_rots)

# === Main Execution (MODIFIED) ===

def main():
    """
    Reads the GNN output (FRAUD_SUMMARY_CSV), finds associated cycles,
    and then ADDS new 'In_Cycle_X' columns back to that same CSV.
    """
    print("\n" + "="*30)
    print("  RUNNING GCN CYCLE DETECTOR & CSV UPDATER  ")
    print("="*30)

    if not os.path.exists(FRAUD_SUMMARY_CSV):
        print(f"Cycle Updater: Could not find {FRAUD_SUMMARY_CSV}")
        print("Run the GCN detection script first to create this file.")
        return

    # --- 1. Load the Fraud Summary CSV ---
    try:
        # We load this as 'df_to_update' because we will modify and re-save it
        df_to_update = pd.read_csv(FRAUD_SUMMARY_CSV)

        # Check for required columns
        if "Company" not in df_to_update.columns or "FlaggedAsFraudulent" not in df_to_update.columns:
             print(f"Cycle Updater: Error! {FRAUD_SUMMARY_CSV} is missing the")
             print("required 'Company' or 'FlaggedAsFraudulent' columns.")
             return

        # Ensure 'Company' column is string type for matching
        df_to_update["Company"] = df_to_update["Company"].astype(str)

        # Create a set of all companies where FlaggedAsFraudulent is True
        fraud_nodes = set(df_to_update[
            df_to_update["FlaggedAsFraudulent"] == True
        ]["Company"].dropna())

        if not fraud_nodes:
            print("Cycle Updater: No companies were flagged as fraudulent in the input file.")
            print("No cycles will be searched for.")
            return

        print(f"Cycle Updater: Loaded {len(fraud_nodes)} fraudulent companies from {FRAUD_SUMMARY_CSV}.")

    except Exception as e:
        print(f"Cycle Updater: Error reading {FRAUD_SUMMARY_CSV}: {e}")
        return

    # --- 2. Locate and scan all heuristic cycle reports ---
    report_paths = []
    if not os.path.exists(OUT_FOLDER):
        print(f"Cycle Updater: Report folder {OUT_FOLDER} not found.")
        print("Please ensure the heuristic script has run and saved reports here.")
        return

    for f in os.listdir(OUT_FOLDER):
        if f.endswith("_report.csv"):
            report_paths.append(os.path.join(OUT_FOLDER, f))

    if not report_paths:
        print(f"Cycle Updater: No '_report.csv' files found in {OUT_FOLDER}.")
        print("Cannot find any cycles to add to the summary file.")
        return

    # --- 3. Build the list of fraudulent cycles ---
    seen = set()
    cycles_out = []

    print(f"Cycle Updater: Scanning {len(report_paths)} report files for cycles...")
    for report_csv in report_paths:
        try:
            df = pd.read_csv(report_csv, dtype={"cycle_id": str, "from": str, "to": str})
        except Exception as e:
            continue

        if df.empty or not {"cycle_id", "from", "to"}.issubset(df.columns):
            continue

        df["cycle_group"] = df["cycle_id"].apply(_group_key)

        for _, gdf in df.groupby("cycle_group", sort=False):
            nodes = _reconstruct_nodes(gdf)

            if len(nodes) >= 2 and nodes[0] == nodes[-1]:
                nodes = nodes[:-1]

            # --- MODIFIED LOGIC ---
            # Keep cycles (len 3+) where AT LEAST ONE member is on the fraud list
            if len(nodes) >= 3 and any(n in fraud_nodes for n in nodes):
            # --- END OF MODIFICATION ---
                key = _canonical_cycle(nodes)
                if key and key not in seen:
                    seen.add(key)
                    cycles_out.append(list(key))

    # Sort by length (longest first), then alphabetically
    cycles_out.sort(key=lambda c: (-len(c), c))

    if not cycles_out:
        print("Cycle Updater: No associated cycles (len 3+) were found.")
        print("CSV file will not be modified.")
        return

    print(f"Cycle Updater: Found {len(cycles_out)} unique cycles associated with flagged companies.")

    # --- 4. Add new cycle columns to the DataFrame ---

    # Remove any old 'In_Cycle_' columns to start fresh
    old_cycle_cols = [col for col in df_to_update.columns if col.startswith("In_Cycle_")]
    if old_cycle_cols:
        print(f"Cycle Updater: Removing {len(old_cycle_cols)} old cycle columns.")
        df_to_update = df_to_update.drop(columns=old_cycle_cols)

    # Add new columns for each cycle found
    for i, cycle_nodes in enumerate(cycles_out):
        col_name = f"In_Cycle_{i+1}"
        cycle_set = set(cycle_nodes)

        # Set column to True if 'Company' is in this cycle set, else False
        df_to_update[col_name] = df_to_update["Company"].isin(cycle_set)

    # --- 5. Save the updated DataFrame back to the CSV file ---
    try:
        df_to_update.to_csv(FRAUD_SUMMARY_CSV, index=False)
        print(f"\nSUCCESS: Updated {FRAUD_SUMMARY_CSV} with {len(cycles_out)} cycle columns.")
        print("\n--- Final CSV Head ---")
        print(df_to_update.head())
        print("------------------------")

    except Exception as e:
        print(f"Cycle Updater: CRITICAL ERROR: Could not save updated CSV to {FRAZUD_SUMMARY_CSV}")
        print(f"Error: {e}")


if __name__ == "__main__":
    main()

"""### EGNN"""

# edge_gnn_circular_trading_detector.py
# Deterministic Edge-GNN for circular trading detection (GST).
# MODIFIED:
# - Now saves all company fraud flags AND risk scores to a CSV.
# - TOP_K set to 7.

import os, glob, math, random
from collections import defaultdict, Counter
from datetime import datetime

import numpy as np
import pandas as pd
import networkx as nx
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler

# ================= DETERMINISM =================
os.environ.setdefault("PYTHONHASHSEED", "0")
os.environ.setdefault("CUBLAS_WORKSPACE_CONFIG", ":16:8")
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.set_default_dtype(torch.float64)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.use_deterministic_algorithms(True)
DEVICE = "cpu"

# ================= CONFIG =====================
DATA_DIR = "/content/drive/My Drive/Capstone/B2B_CircularTrading/b2b_latest_transactions_circular_time_cleaned"

# --- Output File Config ---
# Path to save this script's final output
EGINN_FRAUD_SUMMARY_CSV = "/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_companies_egnn.csv"
# --- End Output Config ---

DATE_COL = "Invoice date"
RECEIVER_GSTIN_COL = "GSTIN/UIN of Recipient"
RECEIVER_NAME_COL = "Receiver Name"
SENDER_NAME_COL = "Sender Name"
INVOICE_VALUE_COL = "Invoice Value"
RATE_COL = "Rate"
TAXABLE_VALUE_COL = "Taxable Value"
CESS_COL = "Cess Amount"
REVERSE_CHARGE_COL = "Reverse Charge"
INVOICE_TYPE_COL = "Invoice Type"
PLACE_OF_SUPPLY_COL = "Place Of Supply"

EPOCHS = 5
LR = 1e-3
HIDDEN_DIM = 64
NEGATIVE_RATIO = 1         # deterministic negatives
TOP_K = 7                  # how many companies to print (CHANGED FROM 6 to 7)
MAX_CYCLE_LEN = 4          # detect 2..4 node directed cycles

# =============== HELPERS ======================
def safe_float(x, default=0.0):
    try:
        if pd.isna(x): return default
        return float(str(x).replace(",", ""))
    except Exception:
        return default

def parse_date(s):
    if pd.isna(s): return None
    s = str(s).strip()
    fmts = [
        "%Y-%m-%d %H:%M:%S",
        "%Y-%m-%d",
        "%d-%m-%Y", "%d/%m/%Y",
        "%Y/%m/%d",
        "%d-%b-%Y", "%d %b %Y",
    ]
    for f in fmts:
        try: return datetime.strptime(s, f)
        except Exception: pass
    try:
        return pd.to_datetime(s, errors="coerce").to_pydatetime()
    except Exception:
        return None

def one_hot_invoice_type(val):
    if pd.isna(val): return [0.0, 0.0]
    return [1.0, 0.0] if "b2b" in str(val).strip().lower() else [0.0, 1.0]

def canonical_key(gstin, name, fallback):
    g = str(gstin).strip() if gstin is not None and str(gstin).strip().lower() not in ["", "nan", "none"] else None
    n = str(name).strip() if name is not None and str(name).strip().lower() not in ["", "nan", "none"] else None
    if g and len(g) == 15 and g.isalnum(): return g
    if n: return n
    return fallback

def majority_name(counter: Counter, fallback: str) -> str:
    if not counter: return fallback
    mx = max(counter.values())
    cands = [k for k, v in counter.items() if v == mx]
    return sorted(cands)[0]

# ============== GRAPH BUILD ===================
def build_graph(folder):
    G = nx.DiGraph()
    edge_agg = defaultdict(lambda: {
        "count": 0,
        "sum_inv_value": 0.0,
        "avg_rate_numer": 0.0,
        "sum_taxable": 0.0,
        "sum_cess": 0.0,
        "reverse_charge_count": 0,
        "min_date": None,
        "max_date": None,
        "place_codes": defaultdict(int),
        "inv_type_vec_sum": np.zeros(2, dtype=np.float64),
        "sum_date_ordinal": 0.0,  # for avg temporal order
    })
    node_stats = defaultdict(lambda: {
        "out_count": 0, "in_count": 0,
        "out_sum": 0.0, "in_sum": 0.0,
        "partners_out": set(), "partners_in": set(),
        "place_codes": defaultdict(int),
    })
    name_votes = defaultdict(Counter)

    files = sorted(glob.glob(os.path.join(folder, "*.csv")))
    if not files:
        raise FileNotFoundError(f"No CSV files in {folder}")

    print(f"Reading {len(files)} CSVs...")
    for path in files:
        file_key = os.path.splitext(os.path.basename(path))[0].strip()
        try:
            df = pd.read_csv(path)
        except Exception:
            try:
                df = pd.read_csv(path, encoding="utf-8", engine="python")
            except Exception as e:
                print(f"Skipping {path}: {e}")
                continue
        df.columns = df.columns.str.strip()

        # Sender representative (deterministic mode)
        s_mode = None
        if SENDER_NAME_COL in df.columns and not df[SENDER_NAME_COL].empty:
            vals = df[SENDER_NAME_COL].dropna().astype(str).str.strip()
            if not vals.empty:
                s_mode = majority_name(Counter(vals.tolist()), None)

        sender_key = canonical_key(None, s_mode, file_key)
        G.add_node(sender_key)
        name_votes[sender_key][s_mode or file_key] += 1

        for _, row in df.iterrows():
            recv_gstin = row.get(RECEIVER_GSTIN_COL)
            recv_name  = row.get(RECEIVER_NAME_COL)
            recv_key = canonical_key(recv_gstin, recv_name,
                                     str(recv_gstin) if pd.notna(recv_gstin) else str(recv_name))
            if not sender_key or not recv_key or sender_key == recv_key:
                continue

            inv = safe_float(row.get(INVOICE_VALUE_COL), 0.0)
            rate = safe_float(row.get(RATE_COL), 0.0)
            tax  = safe_float(row.get(TAXABLE_VALUE_COL), 0.0)
            cess = safe_float(row.get(CESS_COL), 0.0)
            rc   = str(row.get(REVERSE_CHARGE_COL)).strip().lower() if REVERSE_CHARGE_COL in df.columns else "no"
            rc_flag = 1 if rc in ["yes", "y", "true", "1"] else 0
            inv_type_vec = np.array(one_hot_invoice_type(row.get(INVOICE_TYPE_COL)), dtype=np.float64)
            pos = str(row.get(PLACE_OF_SUPPLY_COL)).strip().lower() if PLACE_OF_SUPPLY_COL in df.columns else ""
            d = parse_date(row.get(DATE_COL))

            u, v = sender_key, recv_key
            G.add_node(u); G.add_node(v)

            s_name = row.get(SENDER_NAME_COL)
            if pd.notna(s_name) and str(s_name).strip():
                name_votes[u][str(s_name).strip()] += 1
            r_name = row.get(RECEIVER_NAME_COL)
            if pd.notna(r_name) and str(r_name).strip():
                name_votes[v][str(r_name).strip()] += 1

            b = edge_agg[(u, v)]
            b["count"] += 1
            b["sum_inv_value"] += inv
            b["avg_rate_numer"] += rate
            b["sum_taxable"] += tax
            b["sum_cess"] += cess
            b["reverse_charge_count"] += rc_flag
            if d:
                if b["min_date"] is None or d < b["min_date"]: b["min_date"] = d
                if b["max_date"] is None or d > b["max_date"]: b["max_date"] = d
                b["sum_date_ordinal"] += d.toordinal()
            if pos: b["place_codes"][pos] += 1
            b["inv_type_vec_sum"] += inv_type_vec

            nsu, nsv = node_stats[u], node_stats[v]
            nsu["out_count"] += 1; nsu["out_sum"] += inv; nsu["partners_out"].add(v)
            nsv["in_count"]  += 1; nsv["in_sum"]  += inv; nsv["partners_in"].add(u)
            if pos:
                nsu["place_codes"][pos] += 1
                nsv["place_codes"][pos] += 1

    # finalize edges
    print("Finalizing graph edges...")
    for (u, v), b in sorted(edge_agg.items()):
        c = b["count"]
        avg_rate = b["avg_rate_numer"] / c if c else 0.0
        dur = (b["max_date"] - b["min_date"]).days if b["min_date"] and b["max_date"] else 0.0
        rc_ratio = b["reverse_charge_count"] / c if c else 0.0
        pos_div = float(len(b["place_codes"]))
        inv_type_avg = (b["inv_type_vec_sum"] / max(1, c)).astype(np.float64)
        avg_ord = b["sum_date_ordinal"] / c if c else 0.0

        # edge features: magnitude, tax profile, RC, temporal span, location entropy-ish
        e = np.array([
            math.log1p(b["sum_inv_value"]),
            avg_rate,
            math.log1p(b["sum_taxable"]),
            math.log1p(b["sum_cess"]),
            rc_ratio,
            math.log1p(max(0.0, dur) + 1.0),
            pos_div,
            avg_ord / 36500.0,  # scaled ordinal to keep numeric range moderate
        ], dtype=np.float64)
        e = np.concatenate([e, inv_type_avg]) # Add one-hot avg
        G.add_edge(u, v, edge_feat=e, count=c, inv_avg=b["sum_inv_value"]/max(1,c))

    # node features + stable display names
    print("Finalizing graph nodes...")
    for n, st in node_stats.items():
        outd, ind = st["out_count"], st["in_count"]
        out_sum, in_sum = st["out_sum"], st["in_sum"]
        pratio = len(st["partners_out"]) / (len(st["partners_in"]) + 1.0)
        pdiv = len(st["place_codes"])
        nf = np.array([
            float(outd), float(ind),
            math.log1p(out_sum), math.log1p(in_sum),
            float(pratio), float(pdiv),
        ], dtype=np.float64)
        G.nodes[n]["node_feat"] = nf
        G.nodes[n]["display_name"] = majority_name(name_votes[n], str(n))

    return G

# ============== EDGE-GNN ======================
class EdgeGNNLayer(nn.Module):
    def __init__(self, in_node, in_edge, out_dim):
        super().__init__()
        self.lin = nn.Linear(in_node*2 + in_edge, out_dim)
        self.act = nn.ReLU()
        torch.manual_seed(SEED)
        nn.init.xavier_uniform_(self.lin.weight, gain=1.0)
        nn.init.zeros_(self.lin.bias)
    def forward(self, H, src, dst, E):
        x = torch.cat([H[src], H[dst], E], dim=1)
        m = self.act(self.lin(x))
        out = torch.zeros((H.shape[0], m.shape[1]), dtype=H.dtype, device=H.device)
        out.index_add_(0, dst, m)   # aggregate to dst
        return self.act(out)

class EdgeGNN(nn.Module):
    def __init__(self, node_in, edge_in, hidden):
        super().__init__()
        self.l1 = EdgeGNNLayer(node_in, edge_in, hidden)
        self.l2 = EdgeGNNLayer(hidden, edge_in, hidden)
    def forward(self, X, src, dst, E):
        h1 = self.l1(X, src, dst, E)
        h2 = self.l2(h1, src, dst, E)
        return h2

class EdgeDecoder(nn.Module):
    def __init__(self, hidden_node, edge_in):
        super().__init__()
        self.f1 = nn.Linear(hidden_node*2 + edge_in, hidden_node)
        self.f2 = nn.Linear(hidden_node, 1)
        self.act = nn.ReLU()
        torch.manual_seed(SEED)
        nn.init.xavier_uniform_(self.f1.weight, gain=1.0)
        nn.init.zeros_(self.f1.bias)
        nn.init.xavier_uniform_(self.f2.weight, gain=1.0)
        nn.init.zeros_(self.f2.bias)
    def forward(self, Hu, Hv, E):
        z = torch.cat([Hu, Hv, E], dim=1)
        z = self.act(self.f1(z))
        return self.f2(z).squeeze(-1)

class EdgeGNNModel(nn.Module):
    def __init__(self, node_in, edge_in, hidden):
        super().__init__()
        self.gnn = EdgeGNN(node_in, edge_in, hidden)
        self.dec = EdgeDecoder(hidden, edge_in)
    def forward(self, X, src, dst, E):
        H = self.gnn(X, src, dst, E)
        logits = self.dec(H[src], H[dst], E)
        return logits, H

# ======= GRAPH → TENSORS (stable) ===========
def to_tensors(G):
    nodes = sorted(G.nodes())
    nidx = {n:i for i,n in enumerate(nodes)}

    # Handle nodes with missing features (if any)
    default_nf = np.zeros(6, dtype=np.float64) # 6 is from build_graph
    node_feats = []
    for n in nodes:
        nf = G.nodes[n].get("node_feat")
        if nf is None:
            nf = default_nf
        node_feats.append(nf)
    X = np.stack(node_feats, axis=0)

    edges = sorted(G.edges())
    src = np.array([nidx[u] for u,v in edges], dtype=np.int64)
    dst = np.array([nidx[v] for u,v in edges], dtype=np.int64)

    # Handle edges with missing features (if any)
    default_ef = np.zeros(10, dtype=np.float64) # 8 from array + 2 from one-hot
    edge_feats = []
    if not edges: # Handle graph with no edges
        E = np.empty((0, default_ef.shape[0]), dtype=np.float64)
    else:
        for u,v in edges:
            ef = G[u][v].get("edge_feat")
            if ef is None:
                ef = default_ef
            # Ensure correct shape if some edges missed one-hot
            if len(ef) == 8:
                ef = np.concatenate([ef, [0.0, 0.0]])
            edge_feats.append(ef)
        E = np.stack(edge_feats, axis=0)


    # Check for empty features which would fail scaling
    if X.shape[0] > 0:
        X = StandardScaler().fit_transform(X)
    if E.shape[0] > 0:
        E = StandardScaler().fit_transform(E)

    return {
        "nodes": nodes, "edges": edges, "nidx": nidx,
        "X": torch.tensor(X, dtype=torch.get_default_dtype()),
        "src": torch.tensor(src, dtype=torch.long),
        "dst": torch.tensor(dst, dtype=torch.long),
        "E": torch.tensor(E, dtype=torch.get_default_dtype()),
    }

# ======= DETERMINISTIC NEGATIVES =============
def pick_non_edges_lexi(N, existing, count):
    picked = []
    if count <= 0: return picked
    for u in range(N):
        for v in range(N):
            if u==v: continue
            if (u,v) in existing: continue
            picked.append((u,v))
            if len(picked) >= count:
                return picked
    return picked

# =============== TRAIN ========================
def train_edge_model(T, epochs=EPOCHS, lr=LR, hidden=HIDDEN_DIM, device=DEVICE):
    X, src, dst, E = T["X"].to(device), T["src"].to(device), T["dst"].to(device), T["E"].to(device)
    N, P = X.shape[0], src.shape[0]
    existing = {(int(src[i]), int(dst[i])) for i in range(P)}
    num_neg = max(P * NEGATIVE_RATIO, 1)

    # Handle case where E-GNN has no edges (E.shape[1] is 0)
    edge_dim = E.shape[1]
    if P == 0: # No edges
        edge_dim = 10 # Match default_ef shape
        E = torch.empty((0, edge_dim), dtype=torch.get_default_dtype(), device=device)

    if edge_dim == 0: # Edges exist but no features
        edge_dim = 10
        E = torch.zeros((P, edge_dim), dtype=torch.get_default_dtype(), device=device)


    model = EdgeGNNModel(node_in=X.shape[1], edge_in=edge_dim, hidden=hidden).to(device)
    opt = optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.BCEWithLogitsLoss()

    neg = pick_non_edges_lexi(N, existing, int(num_neg))
    nsrc = torch.tensor([u for u, _ in neg], dtype=torch.long, device=device)
    ndst = torch.tensor([v for _, v in neg], dtype=torch.long, device=device)
    nE   = torch.zeros((len(neg), edge_dim), dtype=E.dtype, device=device)

    bsrc = torch.cat([src, nsrc], dim=0)
    bdst = torch.cat([dst, ndst], dim=0)
    bE   = torch.cat([E, nE], dim=0)
    by   = torch.cat([
        torch.ones(P, dtype=torch.get_default_dtype(), device=device),
        torch.zeros(len(neg), dtype=torch.get_default_dtype(), device=device)
    ], dim=0)

    if bsrc.shape[0] == 0:
        print("No training data (positive or negative edges). Skipping training.")
        return model

    for ep in range(epochs):
        model.train(); opt.zero_grad(set_to_none=True)
        logits, _ = model(X, bsrc, bdst, bE)
        loss = loss_fn(logits, by)
        loss.backward(); opt.step()

        if (ep + 1) % max(1, epochs // 5) == 0:
            with torch.no_grad():
                pred = (torch.sigmoid(logits) > 0.5).to(by.dtype)
                acc = (pred == by).to(torch.float32).mean().item()
            print(f"[{ep+1:03d}/{epochs}] loss={float(loss):.6f} acc={acc:.6f}")

    return model

# =============== INFER ========================
def score_edges(model, T, device=DEVICE):
    model.eval()
    with torch.no_grad():
        X, src, dst, E = T["X"].to(device), T["src"].to(device), T["dst"].to(device), T["E"].to(device)

        edge_dim = E.shape[1]
        if src.shape[0] == 0: # No edges
             return np.array([]), torch.empty((X.shape[0], HIDDEN_DIM)).cpu().numpy()

        if edge_dim == 0:
            edge_dim = 10 # Match training
            E = torch.zeros((src.shape[0], edge_dim), dtype=torch.get_default_dtype(), device=device)

        logits, H = model(X, src, dst, E)
        probs = torch.sigmoid(logits).cpu().numpy()
        return probs, H.cpu().numpy()

# ======= CYCLE MINING & RISK =================
def directed_cycles(G, max_len=4):
    # nx.simple_cycles is deterministic given insertion order + sorted edges we used
    cs = []
    for cyc in nx.simple_cycles(G):
        if 2 <= len(cyc) <= max_len:
            cs.append(tuple(cyc))
    return sorted(set(cs))

def zscore(x):
    x = np.asarray(x, dtype=np.float64)
    mu, sd = np.nanmean(x), np.nanstd(x)
    return np.zeros_like(x) if sd < 1e-12 else (x - mu)/sd

def cycle_edge_amount_variation(G, cyc):
    """Return round-trip consistency: low variation (similar amounts) → higher suspiciousness."""
    vals = []
    for i in range(len(cyc)):
        u = cyc[i]; v = cyc[(i+1) % len(cyc)]
        if G.has_edge(u, v):
            vals.append(G[u][v].get("inv_avg", 0.0))
    vals = np.array(vals, dtype=np.float64)
    if len(vals) == 0: return 0.0
    m = np.nanmean(vals); s = np.nanstd(vals)
    if m <= 0: return 0.0
    # consistency score: 1 - (cv clipped)
    cv = float(s / (m + 1e-9))
    return max(0.0, 1.0 - min(cv, 1.0))

def aggregate_cycle_risk(G, T, edge_probs, cycles, top_k=TOP_K):
    nodes = T["nodes"]; edges = T["edges"]
    edge2p = {(u,v):p for (u,v),p in zip(edges, edge_probs)}

    # Edge anomaly score (lower prob => higher anomaly)
    edge_anom = defaultdict(float)
    for (u,v), p in edge2p.items():
        edge_anom[(u,v)] = 1.0 - float(p)

    # score each cycle by: mean edge anomaly * amount-consistency
    cyc_scores = []
    for cyc in cycles:
        ea = []
        for i in range(len(cyc)):
            u = cyc[i]; v = cyc[(i+1) % len(cyc)]
            ea.append(edge_anom.get((u,v), 0.0))
        if not ea: continue
        edge_component = float(np.mean(ea))
        amt_component = cycle_edge_amount_variation(G, cyc)
        cyc_score = edge_component * (0.5 + 0.5*amt_component)  # weight amounts
        cyc_scores.append((cyc, cyc_score))

    # distribute cycle scores to member nodes
    node_cycle_sum = defaultdict(float)
    for cyc, sc in cyc_scores:
        for n in cyc:
            node_cycle_sum[n] += sc

    # also add degree imbalance as weak signal
    deg_imb = []
    for n in nodes:
        od = G.out_degree(n); idg = G.in_degree(n)
        deg_imb.append(abs(od - idg) / (od + idg + 1.0))
    deg_imb = np.array(deg_imb, dtype=np.float64)

    # assemble vectors aligned to nodes order
    cyc_sum_arr = np.array([node_cycle_sum.get(n, 0.0) for n in nodes], dtype=np.float64)

    risk = (zscore(cyc_sum_arr) + 0.5*zscore(deg_imb)) / 1.5

    # stable Top-K: primary = risk desc, secondary = display_name asc
    display = [G.nodes[n].get("display_name", str(n)) for n in nodes]
    order = np.lexsort((np.array(display, dtype=object), -risk))
    sel = order[:max(1, int(top_k))]
    flagged = [display[i] for i in sel]

    # de-dup (shouldn’t duplicate)
    out, seen = [], set()
    for nm in flagged:
        if nm not in seen:
            seen.add(nm); out.append(nm)

    # --- MODIFICATION: Return all scores and names ---
    return out, risk, display
    # --- END MODIFICATION ---

# ================== MAIN =====================
def main():
    print("PYTHONHASHSEED =", os.environ.get("PYTHONHASHSEED", "unset"))
    print("Building graph from:", DATA_DIR)
    G = build_graph(DATA_DIR)
    print(f"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")

    # --- MODIFICATION: Save empty fraud file ---
    # Get all display names *before* checking graph size
    all_nodes_in_graph = sorted(list(set(G.nodes[n].get("display_name", str(n)) for n in G.nodes())))
    if not all_nodes_in_graph:
        all_nodes_in_graph = [] # handle totally empty graph

    if G.number_of_edges() < 3:
        print("Graph too small. Skipping.")
        data = [{"Company": name, "FlaggedAsFraudulent": False, "Score": 0.0} for name in all_nodes_in_graph]
        df = pd.DataFrame(data, columns=["Company", "FlaggedAsFraudulent", "Score"])
        if df.empty: # Still empty, add columns
             df = pd.DataFrame(columns=["Company", "FlaggedAsFraudulent", "Score"])
        df.to_csv(EGINN_FRAUD_SUMMARY_CSV, index=False)
        print(f"Saved empty/default fraud summary to {EGINN_FRAUD_SUMMARY_CSV}")
        return
    # --- END MODIFICATION ---

    T = to_tensors(G)
    print("Training Edge-GNN (deterministic)…")
    model = train_edge_model(T, epochs=EPOCHS, lr=LR, hidden=HIDDEN_DIM, device=DEVICE)

    print("Scoring edges…")
    edge_probs, _ = score_edges(model, T, device=DEVICE)

    print(f"Mining directed cycles (len 2–{MAX_CYCLE_LEN})…")
    cycles = directed_cycles(G, max_len=MAX_CYCLE_LEN)

    print("Aggregating cycle-centric company risk…")
    # --- MODIFICATION: Capture all scores and names ---
    flagged, all_node_risks, all_node_names = aggregate_cycle_risk(G, T, edge_probs, cycles, top_k=TOP_K)
    # --- END MODIFICATION ---

    print("\nFraudulent companies (flagged):")
    print(flagged)

    # --- MODIFICATION: Save results to CSV with Scores ---
    print("\nSaving fraud summary to CSV...")
    try:
        flagged_set = set(flagged)

        # Create a dictionary mapping node display names to their scores
        # all_node_names and all_node_risks are aligned by T["nodes"]
        node_to_risk = {name: score for name, score in zip(all_node_names, all_node_risks)}

        # Use the canonical list of all names (alphabetical)
        all_nodes_display = all_nodes_in_graph

        data = []
        for name in all_nodes_display:
            data.append({
                "Company": name,
                "FlaggedAsFraudulent": name in flagged_set,
                "Score": node_to_risk.get(name, 0.0) # Add the score
            })

        df = pd.DataFrame(data)
        # Ensure 'Score' column is present even if data is empty
        if 'Score' not in df.columns and "Company" in df.columns:
            df['Score'] = 0.0

        df.to_csv(EGINN_FRAUD_SUMMARY_CSV, index=False)
        print(f"Successfully saved fraud summary (with scores) to {EGINN_FRAUD_SUMMARY_CSV}")

    except Exception as e:
        print(f"ERROR: Failed to save fraud summary CSV: {e}")
    # --- END MODIFICATION ---

if __name__ == "__main__":
    main()

import os
import pandas as pd

# === Configuration ===

# This is the folder where the *_report.csv files (from the heuristic script) are.
HEURISTIC_REPORTS_DIR = "/content/drive/My Drive/Capstone/B2B_CircularTrading/circular_detection_reports"

# This is the CSV file *from* your E-GNN script, which this script will
# read and then *update* with new cycle columns.
EGINN_FRAUD_SUMMARY_CSV = "/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_companies_egnn.csv"


# === Helper Functions (from your code) ===

def _group_key(cycle_id: str) -> str:
    """Extracts the base ID from a unique hop ID."""
    return cycle_id.rsplit("_", 1)[0] if isinstance(cycle_id, str) and "_" in cycle_id else str(cycle_id)

def _reconstruct_nodes(rows: pd.DataFrame) -> list:
    """Rebuilds a cycle path from a DataFrame of its edges."""
    # Prefer ordered hops if present
    if "hop" in rows.columns and rows["hop"].notna().all():
        hops = rows.sort_values("hop")
        seq = hops["from"].tolist()
        last_to = hops["to"].iloc[-1]
        if not seq or last_to != seq[0]:
            seq.append(last_to)
        return seq

    # Fallback adjacency stitch (if 'hop' column is missing)
    edges = list(zip(rows["from"], rows["to"]))
    if not edges: return []

    forward = {u: v for u, v in edges}
    tos = {v for _, v in edges}
    start = next((u for u, _ in edges if u not in tos), edges[0][0])

    seq = [start]
    seen = {start}
    while len(seq) <= len(edges) + 1:
        nxt = forward.get(seq[-1])
        if not nxt:
            break
        seq.append(nxt)
        if nxt in seen:
            break
        seen.add(nxt)
    return seq

def _canonical_cycle(nodes: list) -> tuple:
    """
    Returns a canonical tuple for a cycle to de-duplicate
    based on the unique *set* of companies, regardless of
    rotation or direction.
    """
    if not nodes:
        return tuple()

    # Drop closing node if present (e.g., A->B->A becomes [A, B])
    if len(nodes) >= 2 and nodes[0] == nodes[-1]:
        nodes = nodes[:-1]

    if not nodes:
        return tuple()

    # Sort the nodes to create a canonical key based on the
    # *members* of the cycle, not the path.
    return tuple(sorted(nodes))


# === Main Execution ===

def main():
    """
    Reads the E-GNN fraud summary (EGINN_FRAUD_SUMMARY_CSV) and
    pre-computed cycle reports from HEURISTIC_REPORTS_DIR.
    It then filters for cycles with AT LEAST ONE fraudulent company
    and saves these cycles as new columns back to EGINN_FRAUD_SUMMARY_CSV.
    """
    print("\n" + "="*30)
    print("  RUNNING E-GNN CYCLE UPDATER  ")
    print("="*30)

    if not os.path.exists(EGINN_FRAUD_SUMMARY_CSV):
        print(f"Cycle Updater: Could not find {EGINN_FRAUD_SUMMARY_CSV}")
        print("Please run the 'edge_gnn_circular_trading_detector.py' script first.")
        return

    # Build fraud company node set from the E-GNN script's output
    try:
        fraud_df = pd.read_csv(EGINN_FRAUD_SUMMARY_CSV)

        if "Company" not in fraud_df.columns or "FlaggedAsFraudulent" not in fraud_df.columns:
             print(f"Cycle Updater: Error! {EGINN_FRAUD_SUMMARY_CSV} is missing")
             print("required 'Company' or 'FlaggedAsFraudulent' columns.")
             return

        # Create a set of all companies where FlaggedAsFraudulent is True
        fraud_nodes = set(fraud_df[fraud_df["FlaggedAsFraudulent"] == True]["Company"].dropna().astype(str))

        if not fraud_nodes:
            print("Cycle Updater: No companies were flagged as fraudulent in the input file.")
            print(f"No changes made to {EGINN_FRAUD_SUMMARY_CSV}.")
            return

        print(f"Cycle Updater: Loaded {len(fraud_nodes)} fraudulent companies from E-GNN script to filter by.")

    except Exception as e:
        print(f"Cycle Updater: Error reading {EGINN_FRAUD_SUMMARY_CSV}: {e}")
        return

    # Locate all pre-computed per-company cycle reports
    report_paths = []
    if not os.path.exists(HEURISTIC_REPORTS_DIR):
        print(f"Cycle Updater: Report folder {HEURISTIC_REPORTS_DIR} not found.")
        print("Please ensure the *heuristic* script has been run to generate reports here.")
        return

    for f in os.listdir(HEURISTIC_REPORTS_DIR):
        if f.endswith("_report.csv"):
            report_paths.append(os.path.join(HEURISTIC_REPORTS_DIR, f))

    if not report_paths:
        print(f"Cycle Updater: No '_report.csv' files found in {HEURISTIC_REPORTS_DIR}.")
        print("Note: This script *filters* cycles. You must run the heuristic cycle script")
        print("first to generate these cycle reports.")
        return

    seen = set()
    cycles_out = []

    print(f"Cycle Updater: Scanning {len(report_paths)} report files for cycles...")
    for report_csv in report_paths:
        try:
            df = pd.read_csv(report_csv, dtype={"cycle_id": str})
        except Exception as e:
            print(f"Warning: Skipping {report_csv}, could not read: {e}")
            continue

        if df.empty:
            continue

        if not {"cycle_id", "from", "to"}.issubset(df.columns):
            continue

        df["cycle_group"] = df["cycle_id"].apply(_group_key)

        for _, gdf in df.groupby("cycle_group", sort=False):
            nodes = _reconstruct_nodes(gdf)

            if len(nodes) >= 2 and nodes[0] == nodes[-1]:
                nodes = nodes[:-1]

            # Keep cycles where AT LEAST ONE member is on the fraud list and length is >= 3
            if len(nodes) >= 3 and any(n in fraud_nodes for n in nodes):
                key = _canonical_cycle(nodes)
                if key and key not in seen:
                    seen.add(key)
                    cycles_out.append(list(key)) # Use a list here, not tuple

    # Sort by length (longest first), then alphabetically
    cycles_out.sort(key=lambda c: (-len(c), c))

    if not cycles_out:
        print("\nCycle Updater: No cycles (length 3+) found containing at least one E-GNN-flagged company.")
        print(f"No changes made to {EGINN_FRAUD_SUMMARY_CSV}.")
        return

    print(f"\nCycle Updater: Found {len(cycles_out)} unique cycles associated with flagged companies.")
    print("Updating CSV file with new 'In_Cycle_...' columns...")

    # --- Add new columns to the DataFrame ---

    # Set index for faster lookups
    fraud_df = fraud_df.set_index("Company")

    for i, cycle_nodes in enumerate(cycles_out):
        col_name = f"In_Cycle_{i+1}"
        # Initialize the new column to False
        fraud_df[col_name] = False

        # Find which companies from the cycle are in our DataFrame
        valid_members = [node for node in cycle_nodes if node in fraud_df.index]

        if valid_members:
            # Set True for all companies in this cycle
            fraud_df.loc[valid_members, col_name] = True

        print(f"  -> Added column '{col_name}' for cycle: {', '.join(cycle_nodes)}")

    # Reset index before saving
    fraud_df = fraud_df.reset_index()

    # --- Save the updated DataFrame back to the same file ---
    try:
        fraud_df.to_csv(EGINN_FRAUD_SUMMARY_CSV, index=False)
        print(f"\nSuccessfully updated and saved results to {EGINN_FRAUD_SUMMARY_CSV}")
    except Exception as e:
        print(f"\nERROR: Failed to save updated CSV to {EGINN_FRAUD_SUMMARY_CSV}: {e}")

if __name__ == "__main__":
    main()

"""# Model Evaluation"""

# Generation of ground truth for model evaluation

import os
import pandas as pd


# 1. The folder containing all your company CSVs
# (This is the path you provided)
DATA_DIR = "/content/drive/My Drive/Capstone/B2B_CircularTrading/b2b_latest_transactions_circular_time_cleaned"

# 2. Fraud companies
fraud_companies = ['1IBLJ', 'ZW9XA', 'HT0HL', '5KXVF', 'UJV6O', 'WZS3T']
c1 = fraud_companies[0:4]                                 # Cycle 1 (4-cycle)
c2 = [fraud_companies[0], fraud_companies[4], fraud_companies[5]]  # Cycle 2 (3-cycle a)
c3 = [fraud_companies[1], fraud_companies[4], fraud_companies[3]]  # Cycle 3 (3-cycle b)

# 3. Output file name
OUTPUT_FILE = "/content/drive/My Drive/Capstone/B2B_CircularTrading/ground_truth_circular_trading.csv"


def create_ground_truth():
    """
    Generates a ground truth CSV file from a folder of company CSVs
    and predefined lists of fraudulent/cycle-involved companies.
    """
    print(f"Starting ground truth creation...")

    # Create sets for fast lookup
    set_fraud = set(fraud_companies)
    set_c1 = set(c1)
    set_c2 = set(c2)
    set_c3 = set(c3)

    all_company_ids = []

    # Try to get all company IDs from the folder
    if os.path.exists(DATA_DIR):
        try:
            print(f"Scanning folder: {DATA_DIR}")
            for filename in os.listdir(DATA_DIR):
                if filename.endswith(".csv"):
                    company_id = os.path.splitext(filename)[0]
                    all_company_ids.append(company_id)
            print(f"Found {len(all_company_ids)} companies in folder.")
        except Exception as e:
            print(f"Error reading folder {DATA_DIR}: {e}")
            all_company_ids = []
    else:
        print(f"Warning: Folder not found at {DATA_DIR}.")

    # If folder scan failed or was empty, fall back to the fraud list
    # (This assumes non-fraudulent companies might be missing)
    if not all_company_ids:
        print("Falling back to using only the provided fraud_companies list.")
        # We'll just use the fraud list as the total list
        all_company_ids = list(fraud_companies)
        # Note: If your folder has non-fraudulent companies,
        # they will be missed by this fallback.

    if not all_company_ids:
        print("Error: No company IDs found. Exiting.")
        return

    # Build the ground truth data
    ground_truth_data = []
    for company_id in sorted(list(set(all_company_ids))): # De-duplicate
        data_row = {
            "Company_ID": company_id,
            "Is_Fraudulent": company_id in set_fraud,
            "In_Cycle_1": company_id in set_c1,
            "In_Cycle_2": company_id in set_c2,
            "In_Cycle_3": company_id in set_c3,
        }
        ground_truth_data.append(data_row)

    # Create and save the DataFrame
    df = pd.DataFrame(ground_truth_data)

    try:
        df.to_csv(OUTPUT_FILE, index=False)
        print(f"\nSuccessfully created ground truth file!")
        print(f"Saved as: {os.path.abspath(OUTPUT_FILE)}")
        print("\n--- Data Head ---")
        print(df.head(10))
        print("-----------------")

    except Exception as e:
        print(f"Error saving file: {e}")

if __name__ == "__main__":
    create_ground_truth()

# checking model metrics

import pandas as pd
from sklearn.metrics import precision_score, recall_score, f1_score
import os

# --- CONFIGURATION ---

# 1. Path to your ground truth file
GROUND_TRUTH_FILE = "/content/drive/My Drive/Capstone/B2B_CircularTrading/ground_truth_circular_trading.csv"

# 2. A dictionary of your models and their output CSV files
MODEL_OUTPUT_FILES = {
    "E-GNN": "/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_companies_egnn.csv",
    "GraphSAGE": "/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_companies_graphSAGE.csv",
    "GCN": "/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_companies_gcn.csv",
}

# --- HELPER FUNCTIONS ---

def load_and_standardize(filepath, is_ground_truth=False):
    """
    Loads a CSV, standardizes column names, and extracts all label data.
    """
    if not os.path.exists(filepath):
        print(f"Warning: File not found, skipping: {filepath}")
        return None

    try:
        df = pd.read_csv(filepath)
    except Exception as e:
        print(f"Error reading {filepath}: {e}")
        return None

    # Standardize column names
    if is_ground_truth:
        if "Company_ID" in df.columns:
            df = df.rename(columns={"Company_ID": "Company", "Is_Fraudulent": "Is_Fraud"})
        else:
            print(f"Error: Ground truth file {filepath} is missing 'Company_ID' or 'Is_Fraudulent'.")
            return None
    else:
        if "FlaggedAsFraudulent" in df.columns:
            df = df.rename(columns={"FlaggedAsFraudulent": "Is_Fraud"})
        elif "Is_Fraudulent" in df.columns:
             df = df.rename(columns={"Is_Fraudulent": "Is_Fraud"})

        if "Company" not in df.columns or "Is_Fraud" not in df.columns:
            print(f"Error: Model file {filepath} is missing 'Company' or 'FlaggedAsFraudulent'.")
            return None

    # --- NEW: Extract all label columns ---
    label_cols = [col for col in df.columns if col == "Is_Fraud" or col.startswith("In_Cycle_")]
    labels_df = df[["Company"] + label_cols].copy()

    return labels_df

def evaluate_multi_label_metrics(gt_labels_df, model_labels_df):
    """
    Compares *all* label columns (Is_Fraud, In_Cycle_...)
    using micro-averaging.
    """
    # Merge GT and Model on Company, fill missing (e.g., model missed a company)
    merged = pd.merge(
        gt_labels_df,
        model_labels_df,
        on="Company",
        how="outer",
        suffixes=("_gt", "_model")
    )

    # Get the union of all label columns
    gt_cols = [col for col in merged.columns if col.endswith('_gt')]
    model_cols = [col for col in merged.columns if col.endswith('_model')]

    # Create y_true and y_pred DataFrames
    # Fill NaN with False (if a company or label is missing, it's False)
    y_true = merged[gt_cols].fillna(False)
    y_pred = merged[model_cols].fillna(False)

    # Remove suffixes to ensure columns align
    y_true.columns = [col.replace('_gt', '') for col in gt_cols]
    y_pred.columns = [col.replace('_model', '') for col in model_cols]

    # Ensure both dataframes have the same columns in the same order
    all_labels = sorted(list(set(y_true.columns) | set(y_pred.columns)))
    y_true = y_true.reindex(columns=all_labels, fill_value=False)
    y_pred = y_pred.reindex(columns=all_labels, fill_value=False)

    # Calculate metrics using "micro" averaging
    # This treats every (company, label) pair as a single prediction
    precision = precision_score(y_true, y_pred, zero_division=0, average='micro')
    recall = recall_score(y_true, y_pred, zero_division=0, average='micro')
    f1 = f1_score(y_true, y_pred, zero_division=0, average='micro')

    return {
        "precision": precision,
        "recall": recall,
        "f1": f1,
    }

# --- MAIN EXECUTION ---

def main():
    print("Loading Ground Truth...")
    gt_labels = load_and_standardize(GROUND_TRUTH_FILE, is_ground_truth=True)
    if gt_labels is None:
        print("Failed to load ground truth. Exiting.")
        return

    print("Loaded Ground Truth.")
    print("="*40 + "\n")

    for model_name, model_path in MODEL_OUTPUT_FILES.items():
        print(f"--- Evaluating Model: {model_name} ---")

        model_labels = load_and_standardize(model_path)
        if model_labels is None:
            print("Skipping...\n")
            continue

        print(f"Loaded {model_name} data.")

        # --- 1. Evaluate Combined Metrics ---
        print("\n[Combined Multi-Label Metrics (Fraud + Cycles)]")
        metrics = evaluate_multi_label_metrics(gt_labels, model_labels)
        print(f"  Precision:             {metrics['precision'] * 100:.2f}%")
        print(f"  Recall:                {metrics['recall'] * 100:.2f}%")
        print(f"  F1-Score:              {metrics['f1'] * 100:.2f}%")

        print("\n" + "="*40 + "\n")

if __name__ == "__main__":
    main()

# code for displaying the cycles and the heatmap of the different models

import pandas as pd
from sklearn.metrics import precision_score, recall_score, f1_score
import os
import altair as alt
import networkx as nx
import matplotlib.pyplot as plt
from IPython.display import display, HTML

# --- CONFIGURATION ---

# 1. Path to your ground truth file
GROUND_TRUTH_FILE = "/content/drive/My Drive/Capstone/B2B_CircularTrading/ground_truth_circular_trading.csv"

# 2. A dictionary of your models and their output CSV files
MODEL_OUTPUT_FILES = {
    "E-GNN": "/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_companies_egnn.csv",
    "GraphSAGE": "/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_companies_graphSAGE.csv",
    "GCN": "/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_companies_gcn.csv",
}

# --- HELPER FUNCTIONS ---

def load_and_standardize(filepath, is_ground_truth=False):
    """
    Loads a CSV, standardizes column names, and extracts all label data.
    Converts all label columns to booleans.
    """
    if not os.path.exists(filepath):
        print(f"Warning: File not found, skipping: {filepath}")
        return None

    try:
        df = pd.read_csv(filepath)
    except Exception as e:
        print(f"Error reading {filepath}: {e}")
        return None

    # Standardize column names
    if is_ground_truth:
        if "Company_ID" in df.columns:
            df = df.rename(columns={"Company_ID": "Company", "Is_Fraudulent": "Is_Fraud"})
        elif "Company" in df.columns and "Is_Fraudulent" in df.columns:
             df = df.rename(columns={"Is_Fraudulent": "Is_Fraud"})
        else:
            print(f"Error: Ground truth file {filepath} is missing 'Company' or 'Is_Fraudulent'.")
            return None
    else:
        # Handle model-specific naming variations
        if "FlaggedAsFraudulent" in df.columns:
            df = df.rename(columns={"FlaggedAsFraudulent": "Is_Fraud"})
        elif "Is_Fraudulent" in df.columns:
            df = df.rename(columns={"Is_Fraudulent": "Is_Fraud"})

        # Check for 'Is_Fraud' column after renaming
        if "Company" not in df.columns or "Is_Fraud" not in df.columns:
            print(f"Error: Model file {filepath} is missing 'Company' or 'Is_Fraud' (or 'FlaggedAsFraudulent').")
            return None

    # --- BUG FIX: Extract and Convert all label columns to boolean ---
    label_cols = [col for col in df.columns if col == "Is_Fraud" or col.startswith("In_Cycle_")]

    # Define all possible "True" values
    true_vals = [True, 'True', 'TRUE', 1, 1.0, '1', '1.0']

    for col in label_cols:
        if col in df.columns:
            # .isin() returns a boolean Series. This handles all cases.
            df[col] = df[col].isin(true_vals)
        else:
            print(f"Warning: Expected label column {col} not found in {filepath}")

    # Ensure all expected label columns exist, even if empty
    all_cols = ["Company"] + label_cols
    for col in all_cols:
        if col not in df.columns:
            df[col] = False # Add missing label columns as all-False

    labels_df = df[["Company"] + [c for c in label_cols if c in df.columns]].copy()
    return labels_df

def evaluate_multi_label_metrics(gt_labels_df, model_labels_df):
    """
    Compares *all* label columns (Is_Fraud, In_Cycle_...)
    using micro-averaging.
    """
    # Merge GT and Model on Company, fill missing (e.g., model missed a company)
    merged = pd.merge(
        gt_labels_df,
        model_labels_df,
        on="Company",
        how="outer",
        suffixes=("_gt", "_model")
    )

    # Get the union of all label columns
    gt_cols = [col for col in merged.columns if col.endswith('_gt')]
    model_cols = [col for col in merged.columns if col.endswith('_model')]

    # Create y_true and y_pred DataFrames
    # Fill NaN with False (if a company or label is missing, it's False)
    y_true = merged[gt_cols].fillna(False)
    y_pred = merged[model_cols].fillna(False)

    # Remove suffixes to ensure columns align
    y_true.columns = [col.replace('_gt', '') for col in gt_cols]
    y_pred.columns = [col.replace('_model', '') for col in model_cols]

    # Ensure both dataframes have the same columns in the same order
    all_labels = sorted(list(set(y_true.columns) | set(y_pred.columns)))

    # Check if all_labels is empty
    if not all_labels:
        print("Warning: No label columns (Is_Fraud, In_Cycle_...) found to compare.")
        return {"precision": 0, "recall": 0, "f1": 0}

    y_true = y_true.reindex(columns=all_labels, fill_value=False)
    y_pred = y_pred.reindex(columns=all_labels, fill_value=False)

    # Calculate metrics using "micro" averaging
    precision = precision_score(y_true, y_pred, zero_division=0, average='micro')
    recall = recall_score(y_true, y_pred, zero_division=0, average='micro')
    f1 = f1_score(y_true, y_pred, zero_division=0, average='micro')

    return {
        "precision": precision,
        "recall": recall,
        "f1": f1,
    }

# --- NEW VISUALIZATION FUNCTIONS ---

def visualize_summary_metrics(results_list):
    """
    Creates and returns a grouped bar chart of P/R/F1 for all models.
    """
    if not results_list:
        print("No results to visualize.")
        return None

    df_results = pd.DataFrame(results_list)
    df_melted = pd.melt(df_results, id_vars='Model', var_name='Metric', value_name='Score')

    chart = alt.Chart(df_melted).mark_bar().encode(
        x=alt.X('Metric:N', axis=None),
        y=alt.Y('Score:Q', axis=alt.Axis(format='%', title='Score')),
        color=alt.Color('Metric:N', title="Metric"),
        column=alt.Column('Model:N', header=alt.Header(titleOrient="bottom", labelOrient="bottom", title="Model")),
        tooltip=['Model', 'Metric', alt.Tooltip('Score', format='.1%')]
    ).properties(
        title='Overall Model Performance (All Labels)'
    ).interactive()

    chart_path = 'summary_metrics_barchart.json'
    chart.save(chart_path)
    print(f"Saved summary chart to {chart_path}")
    return chart


def visualize_cycle_comparison(gt_labels_df, model_labels_df, model_name):
    """
    Creates and returns a heatmap comparing cycle membership (TP, FP, FN)
    for a single model vs. ground truth.
    """
    print(f"Generating cycle comparison map for {model_name}...")

    merged = pd.merge(
        gt_labels_df,
        model_labels_df,
        on="Company",
        how="outer",
        suffixes=("_gt", "_model")
    )

    plot_data = []

    # Find all cycle columns present in *either* df
    gt_cycle_cols = {col.replace('_gt', '') for col in merged.columns if col.startswith('In_Cycle_') and col.endswith('_gt')}
    model_cycle_cols = {col.replace('_model', '') for col in merged.columns if col.startswith('In_Cycle_') and col.endswith('_model')}
    all_base_cycles = sorted(list(gt_cycle_cols | model_cycle_cols))

    if not all_base_cycles:
        print(f"No 'In_Cycle_...' columns found to visualize for {model_name}.")
        return None

    for _, row in merged.iterrows():
        company = row['Company']
        for cycle in all_base_cycles:
            gt_col = f'{cycle}_gt'
            model_col = f'{cycle}_model'

            is_gt = row[gt_col] if gt_col in row and pd.notna(row[gt_col]) else False
            is_model = row[model_col] if model_col in row and pd.notna(row[model_col]) else False

            status = "True Negative"
            if is_gt and is_model:
                status = "True Positive"
            elif not is_gt and is_model:
                status = "False Positive"
            elif is_gt and not is_model:
                status = "False Negative"

            plot_data.append({
                "Company": company,
                "Cycle": cycle,
                "Status": status
            })

    plot_df = pd.DataFrame(plot_data)
    plot_df = plot_df[plot_df['Status'] != 'True Negative'] # Filter out TNs

    if plot_df.empty:
        print(f"No cycle activity (TP, FP, or FN) found for {model_name}.")
        return None

    domain = ['True Positive', 'False Positive', 'False Negative']
    range_ = ['#2ca02c', '#d62728', '#ff7f0e'] # Green, Red, Orange

    chart = alt.Chart(plot_df).mark_rect().encode(
        x=alt.X('Cycle:N', title='Cycle'),
        y=alt.Y('Company:N', title='Company'),
        color=alt.Color('Status:N', scale=alt.Scale(domain=domain, range=range_), title="Result"),
        tooltip=['Company', 'Cycle', 'Status']
    ).properties(
        title=f'Cycle Membership Comparison: {model_name} vs. Ground Truth'
    ).interactive()

    chart_path = f'cycle_comparison_{model_name}.json'
    chart.save(chart_path)
    print(f"Saved heatmap to {chart_path}")
    return chart


def visualize_trading_graph(labels_df, title=""):
    """
    Creates and displays a NetworkX graph showing circular trading.
    Nodes = Companies, Edges = Inferred circular transactions
    """
    print(f"Generating circular trading graph for: {title}")

    G = nx.DiGraph()
    cycle_cols = [col for col in labels_df.columns if col.startswith("In_Cycle_")]

    if not cycle_cols:
        print("No 'In_Cycle_' columns found to draw graph.")
        return

    # Use a different color for each cycle
    # Using 'tab10' colormap, which has 10 distinct colors
    cmap = plt.get_cmap('tab10')
    cycle_color_map = {col: cmap(i % 10) for i, col in enumerate(cycle_cols)}

    edge_list = []
    edge_colors = []

    for cycle_name, color in cycle_color_map.items():
        # Get companies in this specific cycle
        companies_in_cycle = labels_df[labels_df[cycle_name] == True]['Company'].tolist()

        if len(companies_in_cycle) > 1:
            # Add nodes
            G.add_nodes_from(companies_in_cycle)

            # Add edges in a circle (e.g., A->B, B->C, C->A)
            for i in range(len(companies_in_cycle)):
                u = companies_in_cycle[i]
                v = companies_in_cycle[(i + 1) % len(companies_in_cycle)] # Wrap around

                edge_list.append((u,v))
                edge_colors.append(color)
        elif len(companies_in_cycle) == 1:
            # Add single node if it's flagged in a "cycle" by itself
            G.add_node(companies_in_cycle[0])

    if G.number_of_nodes() == 0:
        print("No companies found in any cycles to draw graph.")
        return

    # --- Draw the graph ---
    plt.figure(figsize=(10, 7))
    # Using spring_layout for better node separation
    pos = nx.spring_layout(G, k=0.9, iterations=50, seed=42)

    # Draw nodes
    nx.draw_networkx_nodes(G, pos, node_size=2000, node_color='lightblue', alpha=0.8)

    # Draw edges
    nx.draw_networkx_edges(G, pos, edgelist=edge_list, edge_color=edge_colors,
                           width=2, alpha=0.7, arrows=True, arrowsize=20)

    # Draw labels
    nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')

    # Create a legend
    legend_patches = [plt.Line2D([0], [0], color=color, lw=4,
                                 label=cycle_name.replace("In_", ""))
                      for cycle_name, color in cycle_color_map.items()]
    plt.legend(handles=legend_patches, loc='best', title="Cycles")

    plt.title(f"Inferred Circular Trading: {title}", fontsize=16)
    plt.axis('off') # Hide axes
    plt.show() # Display the plot


# --- MAIN EXECUTION ---

def main():
    # Enable Altair's data server
    alt.data_transformers.enable('default')

    print("Loading Ground Truth...")
    gt_labels = load_and_standardize(GROUND_TRUTH_FILE, is_ground_truth=True)
    if gt_labels is None:
        print("Failed to load ground truth. Exiting.")
        return

    print("Loaded Ground Truth.")
    print("="*40 + "\n")

    # --- 1. Visualize Ground Truth Graph ---
    visualize_trading_graph(gt_labels, title="Ground Truth")
    print("\n" + "="*40 + "\n")

    all_results = []
    charts_to_display = {}

    for model_name, model_path in MODEL_OUTPUT_FILES.items():
        print(f"--- Evaluating Model: {model_name} ---")

        model_labels = load_and_standardize(model_path)
        if model_labels is None:
            print("Skipping...\n")
            continue

        print(f"Loaded {model_name} data.")

        # --- 2. Evaluate Combined Metrics ---
        print("\n[Combined Multi-Label Metrics (Fraud + Cycles)]")
        metrics = evaluate_multi_label_metrics(gt_labels, model_labels)
        print(f"  Precision:           {metrics['precision'] * 100:.2f}%")
        print(f"  Recall:              {metrics['recall'] * 100:.2f}%")
        print(f"  F1-Score:            {metrics['f1'] * 100:.2f}%")

        all_results.append({'Model': model_name, **metrics})

        # --- 3. Generate and Store Heatmap Chart ---
        heatmap = visualize_cycle_comparison(gt_labels, model_labels, model_name)
        if heatmap:
            charts_to_display[f'heatmap_{model_name}'] = heatmap

        # --- 4. Generate and Display Model's Trading Graph ---
        visualize_trading_graph(model_labels, title=f"{model_name} Predictions")

        print("\n" + "="*40 + "\n")

    # --- 5. Generate and Store Summary Chart ---
    print("--- Overall Model Performance ---")
    summary_chart = visualize_summary_metrics(all_results)
    if summary_chart:
        charts_to_display['summary'] = summary_chart

    # --- 6. Display all Altair charts at the end ---
    print("\n--- Displaying Altair Charts ---")
    print("Displaying Cycle Comparison Heatmaps...")
    if 'heatmap_E-GNN' in charts_to_display:
        display(charts_to_display['heatmap_E-GNN'])
    if 'heatmap_GraphSAGE' in charts_to_display:
        display(charts_to_display['heatmap_GraphSAGE'])
    if 'heatmap_GCN' in charts_to_display:
        display(charts_to_display['heatmap_GCN'])

    print("Displaying Overall Performance Summary...")
    if 'summary' in charts_to_display:
        display(charts_to_display['summary'])

    print("Done.")

if __name__ == "__main__":
    main()

"""# Model Validation"""

# Model Validation (E-GNN with Edge-Level Split)

import numpy as np
import torch
import torch.optim as optim
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score, average_precision_score

# --- Re-using Config from your script ---
DEVICE = "cpu"
SEED = 42
HIDDEN_DIM = 64
LR = 1e-3
# We define epochs here for the main validation loop
EPOCHS = 100 # Increased from 5, as we now have early stopping
NEG_SAMPLING_RATIO = 1.0


def create_edge_splits(T, val_size=0.1, test_size=0.1, random_state=42):
    """
    Splits the positive edges into train/val/test sets.
    """
    src_pos = T['src'].cpu().numpy()
    dst_pos = T['dst'].cpu().numpy()

    # We must operate on indices, not (src, dst) pairs
    edge_indices = np.arange(len(src_pos))

    # 1. Split indices into train+val and test
    train_val_idx, test_idx = train_test_split(
        edge_indices,
        test_size=test_size,
        random_state=random_state
    )

    # 2. Split train+val into train and val
    # Adjust val_size relative to the reduced train+val set
    relative_val_size = val_size / (1.0 - test_size)
    train_idx, val_idx = train_test_split(
        train_val_idx,
        test_size=relative_val_size,
        random_state=random_state
    )

    return train_idx, val_idx, test_idx


def get_negative_edges(T, edge_indices, N, num_neg_samples):
    """
    Deterministically samples negative edges for a given set of positive edges.
    """
    # Use existing T['nodes'] to get N (total nodes)
    existing_edges = set(zip(T['src'].cpu().numpy(), T['dst'].cpu().numpy()))
    src_pos = T['src'][edge_indices].cpu().numpy()

    # We use a simple hash to make this deterministic but different per positive edge
    # This is a basic deterministic sampling method
    np.random.seed(SEED)
    neg_src = np.random.choice(N, size=num_neg_samples)
    neg_dst = np.random.choice(N, size=num_neg_samples)

    neg_u_list, neg_v_list = [], []
    for i in range(num_neg_samples):
        u, v = neg_src[i], neg_dst[i]
        # Ensure it's not a self-loop and not a real edge
        while u == v or (u, v) in existing_edges:
            u = np.random.choice(N)
            v = np.random.choice(N)
        neg_u_list.append(u)
        neg_v_list.append(v)

    return torch.tensor(neg_u_list, dtype=torch.long), torch.tensor(neg_v_list, dtype=torch.long)


def prepare_data_for_split(T):
    """
    Creates data splits (pos/neg) for train, val, and test.
    """
    N = T['X'].shape[0] # Total number of nodes

    train_idx, val_idx, test_idx = create_edge_splits(T)

    data = {}

    # 1. Training Set
    num_train_pos = len(train_idx)
    data['train_src_pos'] = T['src'][train_idx]
    data['train_dst_pos'] = T['dst'][train_idx]
    data['train_E_pos'] = T['E'][train_idx]

    train_neg_src, train_neg_dst = get_negative_edges(T, train_idx, N, int(num_train_pos * NEG_SAMPLING_RATIO))
    data['train_src_neg'] = train_neg_src
    data['train_dst_neg'] = train_neg_dst
    data['train_E_neg'] = torch.zeros((len(train_neg_src), T['E'].shape[1]), dtype=T['E'].dtype)

    # 2. Validation Set
    data['val_src_pos'] = T['src'][val_idx]
    data['val_dst_pos'] = T['dst'][val_idx]
    data['val_E_pos'] = T['E'][val_idx]

    val_neg_src, val_neg_dst = get_negative_edges(T, val_idx, N, int(len(val_idx) * NEG_SAMPLING_RATIO))
    data['val_src_neg'] = val_neg_src
    data['val_dst_neg'] = val_neg_dst
    data['val_E_neg'] = torch.zeros((len(val_neg_src), T['E'].shape[1]), dtype=T['E'].dtype)

    # 3. Test Set
    data['test_src_pos'] = T['src'][test_idx]
    data['test_dst_pos'] = T['dst'][test_idx]
    data['test_E_pos'] = T['E'][test_idx]

    test_neg_src, test_neg_dst = get_negative_edges(T, test_idx, N, int(len(test_idx) * NEG_SAMPLING_RATIO))
    data['test_src_neg'] = test_neg_src
    data['test_dst_neg'] = test_neg_dst
    data['test_E_neg'] = torch.zeros((len(test_neg_src), T['E'].shape[1]), dtype=T['E'].dtype)

    return data


def train_and_validate_egnn(T, data, device=DEVICE):
    """
    This replaces your original 'train_edge_model' function.
    It includes a full training and validation loop.
    """
    X = T['X'].to(device)

    # We pass the *entire* graph structure (all original edges) to the GNN
    # The GNN needs this to build node embeddings.
    # We will only calculate the *loss* on the training edges.
    full_src, full_dst, full_E = T['src'].to(device), T['dst'].to(device), T['E'].to(device)

    # 1. Combine training edges (pos + neg)
    train_src = torch.cat([data['train_src_pos'], data['train_src_neg']]).to(device)
    train_dst = torch.cat([data['train_dst_pos'], data['train_dst_neg']]).to(device)
    train_E = torch.cat([data['train_E_pos'], data['train_E_neg']]).to(device)
    train_y = torch.cat([
        torch.ones(len(data['train_src_pos'])),
        torch.zeros(len(data['train_src_neg']))
    ]).to(device)

    # 2. Combine validation edges (pos + neg)
    val_src = torch.cat([data['val_src_pos'], data['val_src_neg']]).to(device)
    val_dst = torch.cat([data['val_dst_pos'], data['val_dst_neg']]).to(device)
    val_E = torch.cat([data['val_E_pos'], data['val_E_neg']]).to(device)
    val_y = torch.cat([
        torch.ones(len(data['val_src_pos'])),
        torch.zeros(len(data['val_src_neg']))
    ]).to(device)

    # 3. Combine test edges (pos + neg)
    test_src = torch.cat([data['test_src_pos'], data['test_src_neg']]).to(device)
    test_dst = torch.cat([data['test_dst_pos'], data['test_dst_neg']]).to(device)
    test_E = torch.cat([data['test_E_pos'], data['test_E_neg']]).to(device)
    test_y = torch.cat([
        torch.ones(len(data['test_src_pos'])),
        torch.zeros(len(data['test_src_neg']))
    ]).to(device)

    # --- Model Setup ---
    # Ensure edge_dim is correct even if E is empty
    edge_dim = T['E'].shape[1] if T['E'].shape[0] > 0 else 10 # Default from your script

    model = EdgeGNNModel(node_in=X.shape[1], edge_in=edge_dim, hidden=HIDDEN_DIM).to(device)
    opt = optim.Adam(model.parameters(), lr=LR)
    loss_fn = nn.BCEWithLogitsLoss()

    best_val_f1 = -1
    best_epoch = -1
    best_model_state = None

    print(f"--- Starting E-GNN Validation (Train: {len(train_y)}, Val: {len(val_y)}) ---")

    for ep in range(EPOCHS):
        # --- TRAINING ---
        model.train()
        opt.zero_grad(set_to_none=True)

        # GNN forward pass on the FULL graph to get node embeddings
        H = model.gnn(X, full_src, full_dst, full_E)

        # Decoder pass on just the TRAINING edges
        logits_train = model.dec(H[train_src], H[train_dst], train_E)
        loss = loss_fn(logits_train, train_y)

        loss.backward()
        opt.step()

        # --- VALIDATION ---
        model.eval()
        with torch.no_grad():
            # Get node embeddings from the GNN (using full graph)
            H_val = model.gnn(X, full_src, full_dst, full_E)

            # Decoder pass on just the VALIDATION edges
            logits_val = model.dec(H_val[val_src], H_val[val_dst], val_E)

            # Calculate validation metrics
            probs_val = torch.sigmoid(logits_val).cpu().numpy()
            preds_val = (probs_val > 0.5).astype(int)
            y_true_val = val_y.cpu().numpy()

            val_f1 = f1_score(y_true_val, preds_val, zero_division=0)
            val_auc = roc_auc_score(y_true_val, probs_val)
            val_ap = average_precision_score(y_true_val, probs_val)

        if (ep + 1) % max(1, EPOCHS // 10) == 0:
            print(f"[{ep+1:03d}/{EPOCHS}] Train Loss: {loss.item():.4f} | Val F1: {val_f1:.2%} | Val AUC: {val_auc:.2%}")

        # --- Early Stopping ---
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_epoch = ep
            # Save the model state that gave the best val score
            best_model_state = model.state_dict()

    print(f"\n--- Training Complete ---")
    print(f"Best Model found at Epoch {best_epoch+1} with Validation F1: {best_val_f1:.2%}")

    # --- FINAL TEST ---
    # Load the best model
    model.load_state_dict(best_model_state)
    model.eval()

    with torch.no_grad():
        # Get final embeddings from the best model
        H_test = model.gnn(X, full_src, full_dst, full_E)

        # Evaluate on the TEST set
        logits_test = model.dec(H_test[test_src], H_test[test_dst], test_E)

        probs_test = torch.sigmoid(logits_test).cpu().numpy()
        preds_test = (probs_test > 0.5).astype(int)
        y_true_test = test_y.cpu().numpy()

        test_f1 = f1_score(y_true_test, preds_test, zero_division=0)
        test_auc = roc_auc_score(y_true_test, probs_test)
        test_ap = average_precision_score(y_true_test, probs_test)

    print("\n" + "="*50)
    print("  Final E-GNN Performance on (Hidden) Test Set")
    print("="*50)
    print(f"Test F1-Score:      {test_f1:.2%}")
    print(f"Test AUC Score:     {test_auc:.2%}")
    print(f"Test Avg. Precision: {test_ap:.2%}")
    print("="*50)

    # Return the fully trained model for the risk-scoring step
    return model


def validation_main():
    """
    This new 'main' function runs the validation process
    and then proceeds with your original risk scoring.
    """
    print("PYTHONHASHSEED =", os.environ.get("PYTHONHASHSEED", "unset"))
    print("Building graph from:", DATA_DIR)
    G = build_graph(DATA_DIR)

    # --- (Your 'too small' check is still good) ---
    if G.number_of_edges() < 20: # Increased min for a split
        print("Graph too small for train/val/test split. Skipping.")
        # ... (rest of your small-graph-handling code) ...
        return

    print(f"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")

    T = to_tensors(G)

    # Prepare data splits (pos/neg for train/val/test)
    data_splits = prepare_data_for_split(T)

    print("Training and Validating Edge-GNN...")
    # This function now runs the full train/val/test loop
    model = train_and_validate_egnn(T, data_splits, device=DEVICE)

    # --- Proceed with your original scoring ---
    # Now that we have the best validated model, we use it to
    # score *all* original edges for the cycle-mining step.

    print("\nScoring all graph edges with best model...")
    # This is your original 'score_edges' function's logic
    model.eval()
    with torch.no_grad():
        X, src, dst, E = T["X"].to(DEVICE), T["src"].to(DEVICE), T["dst"].to(DEVICE), T["E"].to(DEVICE)
        edge_dim = E.shape[1]
        if edge_dim == 0:
            edge_dim = 10
            E = torch.zeros((src.shape[0], edge_dim), dtype=torch.get_default_dtype(), device=DEVICE)

        logits, _ = model(X, src, dst, E)
        edge_probs = torch.sigmoid(logits).cpu().numpy()

    print(f"Mining directed cycles (len 2–{MAX_CYCLE_LEN})...")
    cycles = directed_cycles(G, max_len=MAX_CYCLE_LEN)

    print("Aggregating cycle-centric company risk…")
    flagged, all_node_risks, all_node_names = aggregate_cycle_risk(G, T, edge_probs, cycles, top_k=TOP_K)

    print("\nFraudulent companies (flagged):")
    print(flagged)

    # ... (Rest of your CSV saving logic) ...
    print("\n(Your original script would now save the fraud summary CSV...)")
    print(f"Saving fraud summary to {EGINN_FRAUD_SUMMARY_CSV}")
    # (Your full saving logic would go here)


# --- Run the Validation Main Function ---
if __name__ == "__main__":
    # Note: To run this, you need all the other functions from
    # your E-GNN script (like build_graph, EdgeGNNModel, etc.)
    # to be defined in the notebook first.
    validation_main()

# edge_gnn_circular_trading_detector.py
# MODIFIED for Sensitivity Analysis (Number of Layers)
# - Replaces 'main' with 'run_sensitivity_analysis'
# - Implements a proper train/val/test split
# - Modifies GNN models to accept 'n_layers'
# - Plots F1-Score vs. Number of Layers

import os, glob, math, random
from collections import defaultdict, Counter
from datetime import datetime

import numpy as np
import pandas as pd
import networkx as nx
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score, average_precision_score
import matplotlib.pyplot as plt

# ================= DETERMINISM =================
os.environ.setdefault("PYTHONHASHSEED", "0")
os.environ.setdefault("CUBLAS_WORKSPACE_CONFIG", ":16:8")
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.set_default_dtype(torch.float64)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.use_deterministic_algorithms(True)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ================= CONFIG =====================
DATA_DIR = "/content/drive/My Drive/Capstone/B2B_CircularTrading/b2b_latest_transactions_circular_time_cleaned"
OUTPUT_PLOT_FILE = "/content/drive/My Drive/Capstone/B2B_CircularTrading/egnn_layer_sensitivity.png"

DATE_COL = "Invoice date"
RECEIVER_GSTIN_COL = "GSTIN/UIN of Recipient"
RECEIVER_NAME_COL = "Receiver Name"
SENDER_NAME_COL = "Sender Name"
INVOICE_VALUE_COL = "Invoice Value"
RATE_COL = "Rate"
TAXABLE_VALUE_COL = "Taxable Value"
CESS_COL = "Cess Amount"
REVERSE_CHARGE_COL = "Reverse Charge"
INVOICE_TYPE_COL = "Invoice Type"
PLACE_OF_SUPPLY_COL = "Place Of Supply"

EPOCHS = 100 # Increased for proper validation
LR = 1e-3
HIDDEN_DIM = 64
NEG_SAMPLING_RATIO = 1.0 # Renamed from NEGATIVE_RATIO
MAX_CYCLE_LEN = 4
TOP_K = 7

# =============== HELPERS ======================
def safe_float(x, default=0.0):
    try:
        if pd.isna(x): return default
        return float(str(x).replace(",", ""))
    except Exception:
        return default

def parse_date(s):
    if pd.isna(s): return None
    s = str(s).strip()
    fmts = [
        "%Y-%m-%d %H:%M:%S",
        "%Y-%m-%d",
        "%d-%m-%Y", "%d/%m/%Y",
        "%Y/%m/%d",
        "%d-%b-%Y", "%d %b %Y",
    ]
    for f in fmts:
        try: return datetime.strptime(s, f)
        except Exception: pass
    try:
        return pd.to_datetime(s, errors="coerce").to_pydatetime()
    except Exception:
        return None

def one_hot_invoice_type(val):
    if pd.isna(val): return [0.0, 0.0]
    return [1.0, 0.0] if "b2b" in str(val).strip().lower() else [0.0, 1.0]

def canonical_key(gstin, name, fallback):
    g = str(gstin).strip() if gstin is not None and str(gstin).strip().lower() not in ["", "nan", "none"] else None
    n = str(name).strip() if name is not None and str(name).strip().lower() not in ["", "nan", "none"] else None
    if g and len(g) == 15 and g.isalnum(): return g
    if n: return n
    return fallback

def majority_name(counter: Counter, fallback: str) -> str:
    if not counter: return fallback
    mx = max(counter.values())
    cands = [k for k, v in counter.items() if v == mx]
    return sorted(cands)[0]

# ============== GRAPH BUILD ===================
def build_graph(folder):
    G = nx.DiGraph()
    edge_agg = defaultdict(lambda: {
        "count": 0,
        "sum_inv_value": 0.0,
        "avg_rate_numer": 0.0,
        "sum_taxable": 0.0,
        "sum_cess": 0.0,
        "reverse_charge_count": 0,
        "min_date": None,
        "max_date": None,
        "place_codes": defaultdict(int),
        "inv_type_vec_sum": np.zeros(2, dtype=np.float64),
        "sum_date_ordinal": 0.0, # for avg temporal order
    })
    node_stats = defaultdict(lambda: {
        "out_count": 0, "in_count": 0,
        "out_sum": 0.0, "in_sum": 0.0,
        "partners_out": set(), "partners_in": set(),
        "place_codes": defaultdict(int),
    })
    name_votes = defaultdict(Counter)

    files = sorted(glob.glob(os.path.join(folder, "*.csv")))
    if not files:
        raise FileNotFoundError(f"No CSV files in {folder}")

    print(f"Reading {len(files)} CSVs...")
    for path in files:
        file_key = os.path.splitext(os.path.basename(path))[0].strip()
        try:
            df = pd.read_csv(path)
        except Exception:
            try:
                df = pd.read_csv(path, encoding="utf-8", engine="python")
            except Exception as e:
                print(f"Skipping {path}: {e}")
                continue
        df.columns = df.columns.str.strip()

        # Sender representative (deterministic mode)
        s_mode = None
        if SENDER_NAME_COL in df.columns and not df[SENDER_NAME_COL].empty:
            vals = df[SENDER_NAME_COL].dropna().astype(str).str.strip()
            if not vals.empty:
                s_mode = majority_name(Counter(vals.tolist()), None)

        sender_key = canonical_key(None, s_mode, file_key)
        G.add_node(sender_key)
        name_votes[sender_key][s_mode or file_key] += 1

        for _, row in df.iterrows():
            recv_gstin = row.get(RECEIVER_GSTIN_COL)
            recv_name  = row.get(RECEIVER_NAME_COL)
            recv_key = canonical_key(recv_gstin, recv_name,
                                     str(recv_gstin) if pd.notna(recv_gstin) else str(recv_name))
            if not sender_key or not recv_key or sender_key == recv_key:
                continue

            inv = safe_float(row.get(INVOICE_VALUE_COL), 0.0)
            rate = safe_float(row.get(RATE_COL), 0.0)
            tax  = safe_float(row.get(TAXABLE_VALUE_COL), 0.0)
            cess = safe_float(row.get(CESS_COL), 0.0)
            rc   = str(row.get(REVERSE_CHARGE_COL)).strip().lower() if REVERSE_CHARGE_COL in df.columns else "no"
            rc_flag = 1 if rc in ["yes", "y", "true", "1"] else 0
            inv_type_vec = np.array(one_hot_invoice_type(row.get(INVOICE_TYPE_COL)), dtype=np.float64)
            pos = str(row.get(PLACE_OF_SUPPLY_COL)).strip().lower() if PLACE_OF_SUPPLY_COL in df.columns else ""
            d = parse_date(row.get(DATE_COL))

            u, v = sender_key, recv_key
            G.add_node(u); G.add_node(v)

            s_name = row.get(SENDER_NAME_COL)
            if pd.notna(s_name) and str(s_name).strip():
                name_votes[u][str(s_name).strip()] += 1
            r_name = row.get(RECEIVER_NAME_COL)
            if pd.notna(r_name) and str(r_name).strip():
                name_votes[v][str(r_name).strip()] += 1

            b = edge_agg[(u, v)]
            b["count"] += 1
            b["sum_inv_value"] += inv
            b["avg_rate_numer"] += rate
            b["sum_taxable"] += tax
            b["sum_cess"] += cess
            b["reverse_charge_count"] += rc_flag
            if d:
                if b["min_date"] is None or d < b["min_date"]: b["min_date"] = d
                if b["max_date"] is None or d > b["max_date"]: b["max_date"] = d
                b["sum_date_ordinal"] += d.toordinal()
            if pos: b["place_codes"][pos] += 1
            b["inv_type_vec_sum"] += inv_type_vec

            nsu, nsv = node_stats[u], node_stats[v]
            nsu["out_count"] += 1; nsu["out_sum"] += inv; nsu["partners_out"].add(v)
            nsv["in_count"]  += 1; nsv["in_sum"]  += inv; nsv["partners_in"].add(u)
            if pos:
                nsu["place_codes"][pos] += 1
                nsv["place_codes"][pos] += 1

    # finalize edges
    print("Finalizing graph edges...")
    for (u, v), b in sorted(edge_agg.items()):
        c = b["count"]
        avg_rate = b["avg_rate_numer"] / c if c else 0.0
        dur = (b["max_date"] - b["min_date"]).days if b["min_date"] and b["max_date"] else 0.0
        rc_ratio = b["reverse_charge_count"] / c if c else 0.0
        pos_div = float(len(b["place_codes"]))
        inv_type_avg = (b["inv_type_vec_sum"] / max(1, c)).astype(np.float64)
        avg_ord = b["sum_date_ordinal"] / c if c else 0.0

        # edge features: magnitude, tax profile, RC, temporal span, location entropy-ish
        e = np.array([
            math.log1p(b["sum_inv_value"]),
            avg_rate,
            math.log1p(b["sum_taxable"]),
            math.log1p(b["sum_cess"]),
            rc_ratio,
            math.log1p(max(0.0, dur) + 1.0),
            pos_div,
            avg_ord / 36500.0,  # scaled ordinal to keep numeric range moderate
        ], dtype=np.float64)
        e = np.concatenate([e, inv_type_avg]) # Add one-hot avg
        G.add_edge(u, v, edge_feat=e, count=c, inv_avg=b["sum_inv_value"]/max(1,c))

    # node features + stable display names
    print("Finalizing graph nodes...")
    for n, st in node_stats.items():
        outd, ind = st["out_count"], st["in_count"]
        out_sum, in_sum = st["out_sum"], st["in_sum"]
        pratio = len(st["partners_out"]) / (len(st["partners_in"]) + 1.0)
        pdiv = len(st["place_codes"])
        nf = np.array([
            float(outd), float(ind),
            math.log1p(out_sum), math.log1p(in_sum),
            float(pratio), float(pdiv),
        ], dtype=np.float64)
        G.nodes[n]["node_feat"] = nf
        G.nodes[n]["display_name"] = majority_name(name_votes[n], str(n))

    return G

# ============== EDGE-GNN ======================
class EdgeGNNLayer(nn.Module):
    def __init__(self, in_node, in_edge, out_dim):
        super().__init__()
        self.lin = nn.Linear(in_node*2 + in_edge, out_dim)
        self.act = nn.ReLU()
        torch.manual_seed(SEED)
        nn.init.xavier_uniform_(self.lin.weight, gain=1.0)
        nn.init.zeros_(self.lin.bias)
    def forward(self, H, src, dst, E):
        x = torch.cat([H[src], H[dst], E], dim=1)
        m = self.act(self.lin(x))
        out = torch.zeros((H.shape[0], m.shape[1]), dtype=H.dtype, device=H.device)
        out.index_add_(0, dst, m)   # aggregate to dst
        return self.act(out)

# --- !!! MODIFIED GNN CLASS !!! ---
class EdgeGNN(nn.Module):
    def __init__(self, node_in, edge_in, hidden, n_layers=2): # <-- MODIFIED
        super().__init__()
        self.layers = nn.ModuleList()
        if n_layers <= 0:
             raise ValueError("n_layers must be at least 1")

        # First layer
        self.layers.append(EdgeGNNLayer(node_in, edge_in, hidden))

        # Hidden layers
        for _ in range(n_layers - 1):
            self.layers.append(EdgeGNNLayer(hidden, edge_in, hidden))

    def forward(self, X, src, dst, E):
        # We must use X for the first layer's input
        H = self.layers[0](X, src, dst, E)
        # Subsequent layers use the output of the previous layer
        for layer in self.layers[1:]:
            H = layer(H, src, dst, E)
        return H

class EdgeDecoder(nn.Module):
    def __init__(self, hidden_node, edge_in):
        super().__init__()
        self.f1 = nn.Linear(hidden_node*2 + edge_in, hidden_node)
        self.f2 = nn.Linear(hidden_node, 1)
        self.act = nn.ReLU()
        torch.manual_seed(SEED)
        nn.init.xavier_uniform_(self.f1.weight, gain=1.0)
        nn.init.zeros_(self.f1.bias)
        nn.init.xavier_uniform_(self.f2.weight, gain=1.0)
        nn.init.zeros_(self.f2.bias)
    def forward(self, Hu, Hv, E):
        z = torch.cat([Hu, Hv, E], dim=1)
        z = self.act(self.f1(z))
        return self.f2(z).squeeze(-1)

# --- !!! MODIFIED GNN MODEL CLASS !!! ---
class EdgeGNNModel(nn.Module):
    def __init__(self, node_in, edge_in, hidden, n_layers=2): # <-- MODIFIED
        super().__init__()
        # Pass n_layers to the GNN
        self.gnn = EdgeGNN(node_in, edge_in, hidden, n_layers=n_layers) # <-- MODIFIED
        self.dec = EdgeDecoder(hidden, edge_in)

    # This forward pass is used by the validation loop
    # We only need the GNN and Decoder components separately for training
    def forward(self, X, src, dst, E):
        H = self.gnn(X, src, dst, E)
        logits = self.dec(H[src], H[dst], E)
        return logits, H

# ======= GRAPH → TENSORS (stable) ===========
def to_tensors(G):
    nodes = sorted(G.nodes())
    nidx = {n:i for i,n in enumerate(nodes)}

    # Handle nodes with missing features (if any)
    default_nf = np.zeros(6, dtype=np.float64) # 6 is from build_graph
    node_feats = []
    for n in nodes:
        nf = G.nodes[n].get("node_feat")
        if nf is None:
            nf = default_nf
        node_feats.append(nf)
    X = np.stack(node_feats, axis=0)

    edges = sorted(G.edges())
    src = np.array([nidx[u] for u,v in edges], dtype=np.int64)
    dst = np.array([nidx[v] for u,v in edges], dtype=np.int64)

    # Handle edges with missing features (if any)
    default_ef = np.zeros(10, dtype=np.float64) # 8 from array + 2 from one-hot
    edge_feats = []
    if not edges: # Handle graph with no edges
        E = np.empty((0, default_ef.shape[0]), dtype=np.float64)
    else:
        for u,v in edges:
            ef = G[u][v].get("edge_feat")
            if ef is None:
                ef = default_ef
            # Ensure correct shape if some edges missed one-hot
            if len(ef) == 8:
                ef = np.concatenate([ef, [0.0, 0.0]])
            edge_feats.append(ef)
        E = np.stack(edge_feats, axis=0)


    # Check for empty features which would fail scaling
    if X.shape[0] > 0:
        X = StandardScaler().fit_transform(X)
    if E.shape[0] > 0:
        E = StandardScaler().fit_transform(E)

    return {
        "nodes": nodes, "edges": edges, "nidx": nidx,
        "X": torch.tensor(X, dtype=torch.get_default_dtype()),
        "src": torch.tensor(src, dtype=torch.long),
        "dst": torch.tensor(dst, dtype=torch.long),
        "E": torch.tensor(E, dtype=torch.get_default_dtype()),
    }



# --- NEW VALIDATION FUNCTIONS ---

def create_edge_splits(T, val_size=0.1, test_size=0.1, random_state=SEED):
    """
    Splits the positive edges into train/val/test sets.
    """
    src_pos = T['src'].cpu().numpy()
    dst_pos = T['dst'].cpu().numpy()

    edge_indices = np.arange(len(src_pos))

    # 1. Split indices into train+val and test
    train_val_idx, test_idx = train_test_split(
        edge_indices,
        test_size=test_size,
        random_state=random_state
    )

    # 2. Split train+val into train and val
    relative_val_size = val_size / (1.0 - test_size)
    train_idx, val_idx = train_test_split(
        train_val_idx,
        test_size=relative_val_size,
        random_state=random_state
    )

    return train_idx, val_idx, test_idx


def get_negative_edges(T, N, num_neg_samples):
    """
    Deterministically samples negative edges for a given set of positive edges.
    """
    existing_edges = set(zip(T['src'].cpu().numpy(), T['dst'].cpu().numpy()))

    # We use a simple deterministic sampling method
    np.random.seed(SEED)
    neg_src = np.random.choice(N, size=num_neg_samples)
    neg_dst = np.random.choice(N, size=num_neg_samples)

    neg_u_list, neg_v_list = [], []
    for i in range(num_neg_samples):
        u, v = neg_src[i], neg_dst[i]
        # Ensure it's not a self-loop and not a real edge
        while u == v or (u, v) in existing_edges:
            u = np.random.choice(N)
            v = np.random.choice(N)
        neg_u_list.append(u)
        neg_v_list.append(v)

    return torch.tensor(neg_u_list, dtype=torch.long), torch.tensor(neg_v_list, dtype=torch.long)


def prepare_data_for_split(T):
    """
    Creates data splits (pos/neg) for train, val, and test.
    """
    N = T['X'].shape[0] # Total number of nodes
    edge_dim = T['E'].shape[1] if T['E'].shape[0] > 0 else 10 # Match default_ef

    train_idx, val_idx, test_idx = create_edge_splits(T)

    data = {}

    # 1. Training Set
    num_train_pos = len(train_idx)
    data['train_src_pos'] = T['src'][train_idx]
    data['train_dst_pos'] = T['dst'][train_idx]
    data['train_E_pos'] = T['E'][train_idx]

    train_neg_src, train_neg_dst = get_negative_edges(T, N, int(num_train_pos * NEG_SAMPLING_RATIO))
    data['train_src_neg'] = train_neg_src
    data['train_dst_neg'] = train_neg_dst
    data['train_E_neg'] = torch.zeros((len(train_neg_src), edge_dim), dtype=T['E'].dtype)

    # 2. Validation Set
    num_val_pos = len(val_idx)
    data['val_src_pos'] = T['src'][val_idx]
    data['val_dst_pos'] = T['dst'][val_idx]
    data['val_E_pos'] = T['E'][val_idx]

    val_neg_src, val_neg_dst = get_negative_edges(T, N, int(num_val_pos * NEG_SAMPLING_RATIO))
    data['val_src_neg'] = val_neg_src
    data['val_dst_neg'] = val_neg_dst
    data['val_E_neg'] = torch.zeros((len(val_neg_src), edge_dim), dtype=T['E'].dtype)

    # 3. Test Set
    num_test_pos = len(test_idx)
    data['test_src_pos'] = T['src'][test_idx]
    data['test_dst_pos'] = T['dst'][test_idx]
    data['test_E_pos'] = T['E'][test_idx]

    test_neg_src, test_neg_dst = get_negative_edges(T, N, int(num_test_pos * NEG_SAMPLING_RATIO))
    data['test_src_neg'] = test_neg_src
    data['test_dst_neg'] = test_neg_dst
    data['test_E_neg'] = torch.zeros((len(test_neg_src), edge_dim), dtype=T['E'].dtype)

    return data


def train_and_validate_egnn(T, data, n_layers):
    """
    Trains and validates an E-GNN model with a specific number of layers.
    Returns the best F1-score achieved on the validation set.
    """
    X = T['X'].to(DEVICE)

    # We pass the *entire* graph structure (all original edges) to the GNN
    full_src, full_dst, full_E = T['src'].to(DEVICE), T['dst'].to(DEVICE), T['E'].to(DEVICE)

    # 1. Combine training edges (pos + neg)
    train_src = torch.cat([data['train_src_pos'], data['train_src_neg']]).to(DEVICE)
    train_dst = torch.cat([data['train_dst_pos'], data['train_dst_neg']]).to(DEVICE)
    train_E = torch.cat([data['train_E_pos'], data['train_E_neg']]).to(DEVICE)
    train_y = torch.cat([
        torch.ones(len(data['train_src_pos'])),
        torch.zeros(len(data['train_src_neg']))
    ]).to(DEVICE)

    # 2. Combine validation edges (pos + neg)
    val_src = torch.cat([data['val_src_pos'], data['val_src_neg']]).to(DEVICE)
    val_dst = torch.cat([data['val_dst_pos'], data['val_dst_neg']]).to(DEVICE)
    val_E = torch.cat([data['val_E_pos'], data['val_E_neg']]).to(DEVICE)
    val_y = torch.cat([
        torch.ones(len(data['val_src_pos'])),
        torch.zeros(len(data['val_src_neg']))
    ]).to(DEVICE)

    # --- Model Setup ---
    edge_dim = T['E'].shape[1] if T['E'].shape[0] > 0 else 10 # Default from your script

    # --- Pass n_layers to your model ---
    model = EdgeGNNModel(
        node_in=X.shape[1],
        edge_in=edge_dim,
        hidden=HIDDEN_DIM,
        n_layers=n_layers
    ).to(DEVICE)

    opt = optim.Adam(model.parameters(), lr=LR)
    loss_fn = nn.BCEWithLogitsLoss()

    best_val_f1 = -1
    best_epoch = -1

    print(f"--- Testing {n_layers}-Layer E-GNN (Train: {len(train_y)}, Val: {len(val_y)}) ---")

    for ep in range(EPOCHS):
        # --- TRAINING ---
        model.train()
        opt.zero_grad(set_to_none=True)

        # Get node embeddings from GNN
        H = model.gnn(X, full_src, full_dst, full_E)

        # Handle case where no training edges were sampled
        if len(train_src) == 0:
            if (ep + 1) % max(1, EPOCHS // 10) == 0:
                print(f"[{ep+1:03d}/{EPOCHS}] No training edges, skipping epoch.")
            continue

        # Decode only on training edges
        logits_train = model.dec(H[train_src], H[train_dst], train_E)
        loss = loss_fn(logits_train, train_y)

        loss.backward()
        opt.step()

        # --- VALIDATION ---
        model.eval()
        with torch.no_grad():
            # Get node embeddings (they are re-calculated, which is correct)
            H_val = model.gnn(X, full_src, full_dst, full_E)

            # Handle case where no validation edges were sampled
            if len(val_src) == 0:
                if (ep + 1) % max(1, EPOCHS // 10) == 0:
                    print(f"[{ep+1:03d}/{EPOCHS}] No validation edges, skipping validation.")
                continue

            # Decode only on validation edges
            logits_val = model.dec(H_val[val_src], H_val[val_dst], val_E)

            probs_val = torch.sigmoid(logits_val).cpu().numpy()
            preds_val = (probs_val > 0.5).astype(int)
            y_true_val = val_y.cpu().numpy()

            val_f1 = f1_score(y_true_val, preds_val, zero_division=0)
            val_auc = roc_auc_score(y_true_val, probs_val)

        if (ep + 1) % max(1, EPOCHS // 10) == 0:
            print(f"[{ep+1:03d}/{EPOCHS}] Train Loss: {loss.item():.4f} | Val F1: {val_f1:.2%} | Val AUC: {val_auc:.2%}")

        # --- Early Stopping ---
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_epoch = ep

    print(f"[{n_layers}-Layer Model] Best Val F1: {best_val_f1:.2%} at Epoch {best_epoch+1}")

    # Return the best validation F1
    return best_val_f1



# --- 'main' FUNCTION FOR SENSITIVITY ANALYSIS ---

def run_sensitivity_analysis():
    """
    This new 'main' function runs the sensitivity analysis loop
    and plots the results.
    """

    print("PYTHONHASHSEED =", os.environ.get("PYTHONHASHSEED", "unset"))
    print(f"Using device: {DEVICE}")
    print("Building graph from:", DATA_DIR)

    G = build_graph(DATA_DIR)

    if G.number_of_edges() < 20:
        print("Graph too small for train/val/test split. Skipping sensitivity analysis.")
        return

    print(f"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")

    T = to_tensors(G)

    # 1. Prepare data splits ONCE
    print("Preparing data splits (train/val/test)...")
    data_splits = prepare_data_for_split(T)

    # 2. Define the hyperparameter range to test
    layers_to_test = [1, 2, 3, 4, 5] # Test models with 1 to 5 layers
    f1_scores = []

    print("\n" + "="*50)
    print("     Running Hyperparameter Sensitivity Analysis")
    print("="*50)

    # 3. Loop, train, and collect scores
    for n in layers_to_test:
        f1 = train_and_validate_egnn(
            T,
            data_splits,
            n_layers=n
        )
        f1_scores.append(f1)

    print("\n" + "="*50)
    print("              Analysis Complete")
    print("="*50)
    for n, f1 in zip(layers_to_test, f1_scores):
        print(f"Total Layers: {n} | Best Validation F1: {f1:.2%}")
    print("="*50)

    # 4. Plot the results
    plt.figure(figsize=(10, 6))
    plt.plot(layers_to_test, f1_scores, marker='o', linestyle='--')
    plt.title('E-GNN Sensitivity to Number of GNN Layers', fontsize=16)
    plt.xlabel('Number of GNN Layers', fontsize=12)
    plt.ylabel('Best Validation F1-Score', fontsize=12)
    plt.xticks(layers_to_test)
    plt.grid(True, linestyle=':')

    # Save the figure to your output directory
    plt.savefig(OUTPUT_PLOT_FILE)
    print(f"Sensitivity plot saved to {OUTPUT_PLOT_FILE}")
    plt.show()


# ================== MAIN =====================
if __name__ == "__main__":
    # Call the new main function
    run_sensitivity_analysis()

"""## Creation of final Fraud Report for B2B"""

!pip install weasyprint

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
import os
import glob
import json
import base64
from io import BytesIO
from weasyprint import HTML # --- Import WeasyPrint ---

# --- This script should be placed in a new cell ---
# --- AT THE END of your Circular Trading notebook (B2B_Model_implementation2 (1).ipynb) ---
# --- It assumes you have run all cells, including the E-GNN and cycle update scripts ---
# --- AND that you have run !pip install weasyprint in a cell above ---

print("--- B2B Final Report Generator (PDF) Running ---")

# --- 1. Define File Paths ---
HSN_RESULTS_FILE = "/content/drive/My Drive/Capstone/B2B_CircularTrading/hsn_fraud_results.csv"
CT_RESULTS_FILE = "/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_companies_egnn.csv"
HEURISTIC_REPORTS_DIR = "/content/drive/My Drive/Capstone/B2B_CircularTrading/circular_detection_reports"
# --- UPDATED: File extension is now .pdf ---
FINAL_REPORT_FILE = "/content/drive/My Drive/Capstone/B2B_CircularTrading/B2B_Fraud_Report.pdf"
REPORT_DIR = os.path.dirname(FINAL_REPORT_FILE)

# Create directories if they don't exist
os.makedirs(REPORT_DIR, exist_ok=True)

# --- 2. Helper Functions for Plotting & HTML ---

def fig_to_base64(fig):
    """Converts a matplotlib figure to a base64 encoded string."""
    buf = BytesIO()
    fig.savefig(buf, format='png', bbox_inches='tight')
    plt.close(fig)
    return base64.b64encode(buf.getvalue()).decode('utf-8')

def plot_priority_pie_chart(high_count, medium_count, low_count, normal_count):
    """Creates and returns a base64 string for the priority pie chart."""
    labels = []
    sizes = []
    colors = []

    if normal_count > 0:
        labels.append(f'Normal ({normal_count})')
        sizes.append(normal_count)
        colors.append('#2ca02c') # Green
    if low_count > 0:
        labels.append(f'Low Priority ({low_count})')
        sizes.append(low_count)
        colors.append('#1f77b4') # Blue
    if medium_count > 0:
        labels.append(f'Medium Priority ({medium_count})')
        sizes.append(medium_count)
        colors.append('#ff7f0e') # Orange
    if high_count > 0:
        labels.append(f'High Priority ({high_count})')
        sizes.append(high_count)
        colors.append('#d62728') # Red

    if not sizes:
        return ""

    fig, ax = plt.subplots(figsize=(8, 6))

    wedges, texts, autotexts = ax.pie(
        sizes,
        labels=labels,
        colors=colors,
        autopct='%1.1f%%',
        startangle=90,
        pctdistance=0.85,
        textprops=dict(color="black")
    )

    centre_circle = plt.Circle((0,0),0.70,fc='white')
    fig.gca().add_artist(centre_circle)

    ax.axis('equal')
    plt.title('Breakdown of All Analyzed Companies', pad=20, fontsize=14)
    plt.tight_layout()

    return fig_to_base64(fig)

def plot_hsn_barchart(df, top_n=5, color='#d62728', title_text=None):
    """
    Creates and returns a base64 string for the HSN bar chart.
    If top_n is None, plots all companies.
    """

    # Filter out rows where mismatch_total_amount is NaN
    df_filtered = df[df['mismatch_total_amount'].notna()]

    if top_n is not None and top_n > 0:
        df_plot = df_filtered.nlargest(top_n, 'mismatch_total_amount').sort_values('mismatch_total_amount', ascending=True)
        title = title_text if title_text else f'Top {top_n} Companies by HSN Mismatch Amount'
        # Set height based on N
        fig_height = max(5, top_n * 0.8)
    else:
        # Plot all companies
        df_plot = df_filtered.sort_values('mismatch_total_amount', ascending=True)
        title = title_text if title_text else 'All Companies by HSN Mismatch Amount'
        # Set height based on total number of companies
        fig_height = max(5, len(df_plot) * 0.6)

    if df_plot.empty:
        return ""

    fig, ax = plt.subplots(figsize=(10, fig_height))
    # Use the color argument
    bars = ax.barh(df_plot['company_id'], df_plot['mismatch_total_amount'], color=color)
    ax.set_xlabel('Mismatch Total Amount (₹)')
    ax.set_ylabel('Company ID')
    ax.set_title(title) # Use dynamic title
    plt.tight_layout()
    return fig_to_base64(fig)

def plot_cycle_graph(cycle_nodes, cycle_name):
    """Creates and returns a base64 string for a single cycle graph."""
    if not cycle_nodes:
        return ""

    G = nx.DiGraph()
    for i in range(len(cycle_nodes)):
        u = cycle_nodes[i]
        v = cycle_nodes[(i + 1) % len(cycle_nodes)] # Wrap around
        G.add_edge(u, v)

    fig, ax = plt.subplots(figsize=(10, 7))
    pos = nx.circular_layout(G)
    nx.draw(G, pos, ax=ax, with_labels=True, node_size=3000, node_color='#add8e6',
            font_size=10, font_weight='bold', arrowsize=20, edge_color='gray')
    ax.set_title(f'Detected Fraud Ring: {cycle_name}', size=15)
    return fig_to_base64(fig)

def plot_combined_network_graph(df, cycle_cols):
    """Creates and returns a base64 string for the combined network graph."""
    G = nx.DiGraph()
    all_nodes = set()
    edge_colors = {}
    colors = plt.cm.tab10(np.linspace(0, 1, len(cycle_cols)))

    for i, cycle_col in enumerate(cycle_cols):
        cycle_nodes = df[df[cycle_col] == True]['company_id'].tolist()
        if len(cycle_nodes) < 2:
            continue

        all_nodes.update(cycle_nodes)
        color = colors[i]

        for j in range(len(cycle_nodes)):
            u = cycle_nodes[j]
            v = cycle_nodes[(j + 1) % len(cycle_nodes)]
            if (u, v) not in edge_colors:
                 edge_colors[(u,v)] = []
            edge_colors[(u,v)].append(color)

    G.add_nodes_from(all_nodes)

    fig, ax = plt.subplots(figsize=(14, 10))
    pos = nx.spring_layout(G, k=1.5, seed=42)

    nx.draw_networkx_nodes(G, pos, ax=ax, node_size=3000, node_color='#add8e6', alpha=0.9)
    nx.draw_networkx_labels(G, pos, ax=ax, font_size=10, font_weight='bold')

    for (u, v), colors in edge_colors.items():
        if len(colors) == 1:
            nx.draw_networkx_edges(G, pos, ax=ax, edgelist=[(u,v)], width=2, edge_color=colors[0], arrowsize=20)
        else:
            nx.draw_networkx_edges(G, pos, ax=ax, edgelist=[(u,v)], width=3, edge_color='black', style='dashed', arrowsize=20)

    ax.set_title('Combined Fraudulent Networks', size=20)
    ax.axis('off')
    return fig_to_base64(fig)

# --- UPDATED: get_html_style() function ---
def get_html_style():
    """Returns basic CSS for a professional-looking report."""
    return """
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; line-height: 1.6; }
        .container { max-width: 1200px; margin: 20px auto; padding: 20px; }
        h1, h2, h3, h4 { color: #333; border-bottom: 2px solid #f0f0f0; padding-bottom: 5px; }
        h1 { font-size: 2.5em; text-align: center; color: #1a1a1a; border-bottom: 4px solid #d62728; }
        h2 { font-size: 2em; color: #d62728; }
        h3 { font-size: 1.75em; color: #1f77b4; }
        h4 { font-size: 1.5em; }

        /* --- UPDATED: More aggressive table styles --- */
        table.report-table {
            border-collapse: collapse;
            width: 100%; /* Keep at 100% */
            margin-top: 20px;
            box-shadow: 0 2px 3px rgba(0,0,0,0.1);
            page-break-inside: auto;
            font-size: 0.8em; /* <-- UPDATED: Even smaller font */
            table-layout: fixed; /* <-- NEW: This is the key fix! Forces table to fit. */
        }
        table.report-table tr {
            page-break-inside: avoid;
            page-break-after: auto;
        }
        table.report-table th, table.report-table td {
            border: 1px solid #ddd;
            padding: 5px; /* <-- UPDATED: Even less padding */
            text-align: left;
            word-wrap: break-word; /* <-- NEW/IMPORTANT: Forces long words to break */
            overflow-wrap: break-word; /* <-- NEW: A more modern equivalent */
        }
        /* --- END OF UPDATES --- */

        table.report-table th {
            background-color: #f7f7f7; font-weight: bold;
        }
        table.report-table tr:nth-child(even) { background-color: #fdfdfd; }
        img { max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; margin-top: 15px; }
        .section {
            margin-top: 30px; padding-top: 20px; border-top: 1px solid #eee;
            page-break-inside: avoid;
        }
        .summary-box {
            background-color: #f9f9f9; border: 1px solid #e0e0e0;
            padding: 20px; border-radius: 5px; margin-bottom: 20px;
        }
        .summary-box li { list-style-type: none; font-size: 1.1em; }
        .summary-box li strong { color: #d62728; }
    </style>
    """

# --- 3. Load and Merge Data ---
print("Loading HSN Mismatch results...")
try:
    df_hsn = pd.read_csv(HSN_RESULTS_FILE, dtype={'company_id': str})
    df_hsn = df_hsn.rename(columns={'anomaly_score': 'HSN_Mismatch_Score'})
    df_hsn = df_hsn.set_index('company_id')
except FileNotFoundError:
    print(f"Warning: {HSN_RESULTS_FILE} not found. HSN data will be missing.")
    df_hsn = pd.DataFrame()

print("Loading Circular Trading results...")
try:
    df_ct = pd.read_csv(CT_RESULTS_FILE, dtype={'Company': str})
    df_ct = df_ct.rename(columns={
        'Company': 'company_id',
        'Score': 'Circular_Trading_Score',
        'FlaggedAsFraudulent': 'CT_Flag'
    })
    df_ct = df_ct.set_index('company_id')
except FileNotFoundError:
    print(f"CRITICAL: {CT_RESULTS_FILE} not found. Cannot generate report.")
    raise SystemExit("Circular Trading results file is missing.")

df_merged = pd.merge(df_ct, df_hsn, left_index=True, right_index=True, how='outer')

# --- 4. Define Risk Priority and Filter ---
df_merged.reset_index(inplace=True)
if 'index' in df_merged.columns:
    df_merged.rename(columns={'index': 'company_id'}, inplace=True)

df_merged['HSN_Flag'] = ~df_merged['HSN_Mismatch_Score'].isna()
df_merged['CT_Flag'] = df_merged['CT_Flag'].fillna(False).astype(bool)

def assign_priority(row):
    if row['CT_Flag'] and row['HSN_Flag']:
        return '1 - High'
    if row['CT_Flag']:
        return '2 - Medium'
    if row['HSN_Flag']:
        return '3 - Low'
    return '4 - Normal'

def get_fraud_types(row):
    types = []
    if row['CT_Flag']:
        types.append('Circular Trading')
    if row['HSN_Flag']:
        types.append('HSN Mismatch')
    return ', '.join(types) if types else 'N/A'

df_merged['Risk_Priority'] = df_merged.apply(assign_priority, axis=1)
df_merged['Fraud_Types_Detected'] = df_merged.apply(get_fraud_types, axis=1)

df_report = df_merged[df_merged['Risk_Priority'].isin(['1 - High', '2 - Medium', '3 - Low'])].copy()
df_report.sort_values(by='Risk_Priority', ascending=True, inplace=True)
df_report.reset_index(inplace=True)

cycle_cols = sorted([col for col in df_report.columns if col.startswith('In_Cycle_')])
df_report['Cycles_Involved'] = df_report[cycle_cols].apply(
    lambda row: ', '.join([c.replace('In_', '') for c in cycle_cols if row[c]]),
    axis=1
)
df_report['Cycles_Involved'] = df_report['Cycles_Involved'].replace('', 'N/A')

# --- 5. Generate Plots (as base64 strings) ---
print("Generating visualizations...")

# --- Calculate summary counts
count_high = len(df_report[df_report['Risk_Priority'] == '1 - High'])
count_medium = len(df_report[df_report['Risk_Priority'] == '2 - Medium'])
count_low = len(df_report[df_report['Risk_Priority'] == '3 - Low'])
count_total_flagged = len(df_report)
count_total_analyzed = len(df_merged)
count_normal = count_total_analyzed - count_total_flagged

# --- Generate the Pie Chart
priority_pie_chart_base64 = plot_priority_pie_chart(count_high, count_medium, count_low, count_normal)

# --- Generate Top 5 HSN chart
hsn_barchart_top5_base64 = plot_hsn_barchart(
    df_report,
    top_n=5,
    color='#d62728', # Red
    title_text='Top 5 Companies by HSN Mismatch Amount'
)

# --- Generate other plots
combined_network_base64 = plot_combined_network_graph(df_report, cycle_cols)

cycle_graph_base64 = {}
for cycle_col in cycle_cols:
    cycle_nodes = df_report[df_report[cycle_col] == True]['company_id'].tolist()
    if cycle_nodes:
        cycle_name = cycle_col.replace('In_', '')
        cycle_graph_base64[cycle_col] = plot_cycle_graph(cycle_nodes, cycle_name)

# --- 6. Build the HTML Report String ---
print("Building HTML report...")
report_html = f"""
<html>
<head>
    <title>B2B Fraud Detection Report</title>
    {get_html_style()}
</head>
<body>
    <div class="container">
        <h1>📈 B2B Fraud Detection Report</h1>
        <p style="text-align: center;">
            Report Generated: {pd.to_datetime('now', utc=True).strftime('%Y-%m-%d %H:%M:%S UTC')}<br>
            Models Used: Edge-GNN (Circular Trading) & Isolation Forest (HSN Mismatch)<br>
            Data Analyzed: {count_total_analyzed} B2B Companies
        </p>

        <div class="section">
            <h2>1. Executive Summary</h2>

            <div class="summary-box">
                <ul>
                    <li><strong>Total Companies Analyzed:</strong> {count_total_analyzed}</li>
                    <li><strong>Total Companies Flagged:</strong> {count_total_flagged}</li>
                    <li><strong>High Priority (CT + HSN):</strong> {count_high}</li>
                    <li><strong>Medium Priority (CT Only):</strong> {count_medium}</li>
                    <li><strong>Low Priority (HSN Only):</strong> {count_low}</li>
                </ul>
            </div>

            <div style="text-align: center; margin-top: 20px;">
                <img src="data:image/png;base64,{priority_pie_chart_base64}" alt="Priority Pie Chart" style="max-width: 650px;">
            </div>

        </div>

        <div class="section">
            <h2>2. High-Priority B2B Fraud Overview</h2>

            <h4>Detected Fraudulent Networks</h4>
            <p>Key Findings: {len(cycle_cols)} distinct collusive networks were identified, involving {df_report[df_report['CT_Flag'] == True]['company_id'].nunique()} unique companies.</p>
            <img src="data:image/png;base64,{combined_network_base64}" alt="Combined Network Graph">

            <h4>Top 5 Companies by HSN Mismatch Amount</h4>
            <p>Key Findings: The chart displays the top 5 companies based on the total value of mismatched HSN code transactions.</p>
            <img src="data:image/png;base64,{hsn_barchart_top5_base64}" alt="Top 5 HSN Mismatch Bar Chart">
        </div>

        <div class="section">
            <h2>3. Combined B2B Fraud Roster</h2>
            <p>This is the master list of all suspicious companies, prioritized by risk.</p>
"""

# Format and add the main table
report_table_cols = [
    'company_id',
    'Risk_Priority',
    'Fraud_Types_Detected',
    'Circular_Trading_Score',
    'HSN_Mismatch_Score',
    'Cycles_Involved'
]
df_table = df_report[report_table_cols].copy()
df_table['Circular_Trading_Score'] = df_table['Circular_Trading_Score'].round(3)
df_table['HSN_Mismatch_Score'] = df_table['HSN_Mismatch_Score'].round(3)
df_table.fillna('N/A', inplace=True)
df_table.reset_index(drop=True, inplace=True)
df_table.index.name = 'Rank'
df_table.index = df_table.index + 1
report_html += df_table.to_html(classes='report-table', border=0)

# --- 4. Detailed Analysis: Circular Trading (E-GNN) ---
report_html += '<div class="section"><h2>4. Detailed Analysis: Circular Trading (E-GNN)</h2>'
if not cycle_cols:
    report_html += "<p>No Circular Trading cycles were detected.</p>"
else:
    for cycle_col in cycle_cols:
        cycle_name = cycle_col.replace('In_', '')
        participants = df_report[df_report[cycle_col] == True]['company_id'].tolist()
        img_base64 = cycle_graph_base64.get(cycle_col, '')
        report_html += f"<h4>Detected {cycle_name}</h4>"
        report_html += f"<p><strong>Participants:</strong> {', '.join(participants)}</p>"
        report_html += f'<img src="data:image/png;base64,{img_base64}" alt="{cycle_name} Graph">'
report_html += '</div>'

# --- 5. Detailed Analysis: HSN/SAC Mismatch (Isolation Forest) ---
report_html += '<div class="section"><h2>5. Detailed Analysis: HSN/SAC Mismatch (Isolation Forest)</h2>'
df_hsn_report = df_report[df_report['HSN_Flag'] == True].sort_values(by='HSN_Mismatch_Score').copy()

if df_hsn_report.empty:
    report_html += "<p>No HSN Mismatch anomalies were detected.</p>"
else:
    # --- Generate the 'All Companies' bar chart here ---
    hsn_barchart_all_base64 = plot_hsn_barchart(
        df_hsn_report,
        top_n=None,  # This tells the function to plot all
        color='#1f77b4', # A nice blue color
        title_text='All Flagged Companies by HSN Mismatch Amount'
    )

    # --- Add the new 'all' chart to the HTML ---
    report_html += f'<h4>All Flagged Companies (by Mismatch Amount)</h4>'
    report_html += f'<p>This chart shows all {len(df_hsn_report)} companies flagged for HSN Mismatch, sorted by amount.</p>'
    report_html += f'<img src="data:image/png;base64,{hsn_barchart_all_base64}" alt="All HSN MMismatch Bar Chart">'

    report_html += '<h4>Anomaly Details Table</h4>'
    hsn_table_cols = [
        'company_id',
        'HSN_Mismatch_Score',
        'mismatch_count',
        'mismatch_total_amount',
        'mismatch_revenue_ratio'
    ]
    df_hsn_table = df_hsn_report[hsn_table_cols].copy()
    df_hsn_table['mismatch_revenue_ratio'] = (df_hsn_table['mismatch_revenue_ratio'] * 100).round(2).astype(str) + '%'
    df_hsn_table['mismatch_total_amount'] = df_hsn_table['mismatch_total_amount'].round(2)
    df_hsn_table['HSN_Mismatch_Score'] = df_hsn_table['HSN_Mismatch_Score'].round(3)
    df_hsn_table.reset_index(drop=True, inplace=True)
    df_hsn_table.index.name = 'Rank'
    df_hsn_table.index = df_hsn_table.index + 1

    report_html += df_hsn_table.to_html(classes='report-table', border=0)
report_html += '</div>'

# --- 7. Close HTML and Save ---
report_html += """
    </div>
</body>
</html>
"""

# --- UPDATED: Save as PDF instead of HTML ---
try:
    # This is the new part. We pass the HTML string to WeasyPrint
    HTML(string=report_html).write_pdf(FINAL_REPORT_FILE)

    print(f"\nSuccessfully generated B2B Fraud Report as PDF!")
    print(f"Report saved to: {FINAL_REPORT_FILE}")
except Exception as e:
    print(f"Error saving final PDF report: {e}")

print("--- B2B Final Report Generator Finished ---")