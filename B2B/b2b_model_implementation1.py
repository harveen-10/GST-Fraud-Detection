# -*- coding: utf-8 -*-
"""B2B_Model_implementation1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19YEKu09B51brruXvRVZi6id9kw_22USY

# Using Isolation Forest

We know that the companies in the fraudulent_transaction.csv are doing fraud since there is a mismatch in the HSN/SAC codes that the company usually deals with the the codes that the company is doing transaction in.

But to prioritize and rank them based on severity, we are using Isolation Forest.
"""

# connecting my drive to this google colab nb

from google.colab import drive
drive.mount('/content/drive')

"""###Preprocessing required for the model

Now we can start transforming this raw data into meaningful signals to detect the HSN/SAC misuse fraud (Fraudulent ITC claims)

A company can claim ITC only on the goods which it deals with. Fraudulent ITC claims is when a company buys products for its own personal use but tries to claim ITC on those products also.
"""

# HSN/SAC code misuse(Fraudulent ITC claims)
# Checking if the HSN/SAC codes the companies are doing transaction in matches the HSN/SAC codes that the company usually deals with

import os
import pandas as pd

# Define dataset paths
company_hsn_path = '/content/drive/My Drive/Capstone/B2B_CircularTrading/company hsn codes cleaned'
transaction_hsn_path = '/content/drive/My Drive/Capstone/B2B_CircularTrading/company-transaction hsn codes cleaned'
output_file = '/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_transactions.csv'

# Create a dictionary to store company HSN codes (excluding "0000")
company_hsn_lookup = {}

# Load company HSN codes
for file_name in os.listdir(company_hsn_path):
    if file_name.endswith('.csv'):
        company_id = os.path.splitext(file_name)[0]  # Extract company ID from filename
        file_path = os.path.join(company_hsn_path, file_name)
        df = pd.read_csv(file_path, dtype={"HSN": str, "SAC": str})

        # Filter out "7777" (null values) and collect valid HSN and SAC codes
        valid_hsn = df['HSN'][df['HSN'] != '7777'].tolist()
        valid_sac = df['SAC'][df['SAC'] != '7777'].tolist()
        company_hsn_lookup[company_id] = set(valid_hsn + valid_sac)  # Combine HSN and SAC into one set

# Process transaction files and detect mismatches
mismatch_data = []

for file_name in os.listdir(transaction_hsn_path):
    if file_name.endswith('.csv'):
        company_id = os.path.splitext(file_name)[0]  # Extract company ID from filename
        file_path = os.path.join(transaction_hsn_path, file_name)
        df = pd.read_csv(file_path, dtype={"HSN": str})

        # Get the company's valid HSN codes
        registered_hsn = company_hsn_lookup.get(company_id, set())

        # # Print the registered HSN codes for debugging
        # print(f"Company ID: {company_id}, Registered HSN/SAC codes: {registered_hsn}")

        # Check each transaction's HSN code
        for _, row in df.iterrows():
            hsn_code = str(row['HSN'])
            total_quantity = row['Total Quantity']
            total_value = row['Total Value']

            # Check if the HSN code is in the company's registered HSN codes
            if hsn_code not in registered_hsn:
                mismatch_data.append({
                    'company_id': company_id,
                    'Mismatch HSN Code': hsn_code,
                    'Quantity': total_quantity,
                    'Total Amount': total_value
                })

# Create a DataFrame from mismatched transactions
if mismatch_data:
    fraudulent_df = pd.DataFrame(mismatch_data)
    # Ensure HSN code remains a string
    fraudulent_df['Mismatch HSN Code'] = fraudulent_df['Mismatch HSN Code'].astype(str)
    # Save to CSV
    fraudulent_df.to_csv(output_file, index=False)
    print(f"Fraudulent transactions detected and saved to {output_file}")
else:
    print("No fraudulent transactions detected.")

"""###Model Implementation

We now want to identify and rank companies that exhibit suspicious transaction patterns.
"""

# Importing all the necessary files
import pandas as pd
import os
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from google.colab import drive


# Define File Paths
all_companies_folder = '/content/drive/My Drive/Capstone/B2B_CircularTrading/b2b transactions cleaned'
fraud_file_path = '/content/drive/My Drive/Capstone/B2B_CircularTrading/fraudulent_transactions.csv'

# Building a profile for all the companies in the dataset.
# Finding the average transaction value for all the companies by dividing the total Invoice Value by the total number of transactions.

all_profiles_list = []
try:
    all_files = os.listdir(all_companies_folder)
    csv_files = [f for f in all_files if f.endswith('.csv')]
    for filename in csv_files:
        company_id = os.path.splitext(filename)[0]
        filepath = os.path.join(all_companies_folder, filename)
        temp_df = pd.read_csv(filepath)
        total_revenue = temp_df['Invoice Value'].sum()
        total_transactions = len(temp_df)
        if total_transactions > 0:
            avg_value = total_revenue / total_transactions
            all_profiles_list.append({
                'company_id': company_id, 'avg_transaction_value': avg_value,
                'total_revenue': total_revenue, 'total_transactions': total_transactions
            })
    master_df = pd.DataFrame(all_profiles_list)
    print(f"--- Created profiles for all {len(master_df)} companies. ---")
    display(master_df.head())
except FileNotFoundError:
    print(f"Error: The directory '{all_companies_folder}' was not found.")

# Finding the ground truth values --> Finding the number of fraudulent companies and their names from the pre processing, which finds the mismatch HSN/SAC code
# for the companies.
# Created a true label column which is labelling the data as fraudulent or not
# By doing this in a way we are manually labelling the data based on the discrepencies in the HSN codes.

try:
    fraud_df = pd.read_csv(fraud_file_path)
    # Create fraud summary
    fraud_summary = fraud_df.groupby('company_id').agg(
        mismatch_count=('Mismatch HSN Code', 'count'),
        mismatch_total_amount=('Total Amount', 'sum')
    ).reset_index()

    known_fraud_count = len(fraud_summary)
    print(f"Found {known_fraud_count} unique fraudulent companies in the file.")

    # Add fraud features to master profile
    master_df = pd.merge(master_df, fraud_summary, on='company_id', how='left')

    # Use direct assignment to safely fill NaN values
    master_df['mismatch_count'] = master_df['mismatch_count'].fillna(0)
    master_df['mismatch_total_amount'] = master_df['mismatch_total_amount'].fillna(0)

    # Create the 'true_label' column
    known_fraud_ids = set(fraud_df['company_id'])
    master_df['true_label'] = np.where(master_df['company_id'].isin(known_fraud_ids), 1, 0)
    print("--- Added features and ground truth labels. ---")
    display(master_df.head())
except FileNotFoundError:
    print(f"Error: The file '{fraud_file_path}' was not found.")

# Running the Isolation Forest code

# Define the features for the model which will be used by the isolation forest to classify the company as fraudulent or not
features = master_df[['avg_transaction_value', 'total_revenue', 'total_transactions',
                      'mismatch_count', 'mismatch_total_amount']]

# Calculate contamination level dynamically --> it is the ratio of fraudulent companies to normal companies in the dataset
total_companies = len(master_df)
contamination_level = known_fraud_count / total_companies
print(f"\nTo get {known_fraud_count} outliers from {total_companies} companies, contamination is set to: {contamination_level:.4f}")

# Initialize and fit the Isolation Forest model
# The specific number, 42, holds no special significance for the model's performance in random_state
# It is a widely used convention in the data science community,
# and any other integer would have achieved the same goal of making the model's behavior deterministic.
iso_forest = IsolationForest(contamination=contamination_level, random_state=42)
iso_forest.fit(features)

# Add the model's predictions to the DataFrame
master_df['anomaly_score'] = iso_forest.decision_function(features)
master_df['is_anomaly'] = iso_forest.predict(features)
print("--- Ran Isolation Forest to get predictions. ---")

# Displaying the fraudulent companies

# Filter the DataFrame to show only the rows flagged as anomalies
anomalous_companies_df = master_df[master_df['is_anomaly'] == -1]

# Sort the results by the anomaly score to see the most suspicious at the top
ranked_anomalies = anomalous_companies_df.sort_values(by='anomaly_score')

print(f"\n--- The model detected the following {len(ranked_anomalies)} companies as anomalous: ---")
display(ranked_anomalies)

# Model evaluation
# Now since we have manually labelled the data, we can use supervised performance metrics like accuracy, precision, recall etc.
# to characterise how well our model is performing

# Get the ground truth labels from your data
y_true = master_df['true_label']
# Get the model's predictions (convert -1/1 to 1/0 for comparison)
y_pred = (master_df['is_anomaly'] == -1).astype(int)

# Calculate the metrics
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print("\n--- Model Performance Metrics ---")
print(f'Accuracy: {accuracy:.2%}')
print(f'Precision: {precision:.2%}')
print(f'Recall: {recall:.2%}')
print(f'F1 Score: {f1:.2%}')

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# 1. Get the true labels and the model's scores
y_true = master_df['true_label']
anomaly_scores = master_df['anomaly_score']

# IMPORTANT: The roc_curve function expects scores where a higher value means
# more likely to be the positive class (fraud). Isolation Forest's scores
# are the opposite (lower is more anomalous). So, we invert the scores.
y_scores = -anomaly_scores

# 2. Calculate the ROC curve points
fpr, tpr, thresholds = roc_curve(y_true, y_scores)

# 3. Calculate the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# 4. Plot the ROC curve
plt.figure(figsize=(10, 7))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (Recall)')
plt.title('Receiver Operating Characteristic (ROC) Curve for Isolation Forest')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

"""Trying a few other models which are also good for fraud detection.
# Elliptic Envelope

Elliptic Envelope is a statistical method for detecting outliers. Its core idea is based on a key assumption: that the vast majority of your "normal" data points follow a Gaussian (bell-curve) distribution.
Any point found far outside this normal distribution is considered an outlier.

Since for normal companies we dont have any mismatch (hence it is 0), so these companies form a very dense, tight cluster around the origin. Fraudulent companies which have some mismatch are pushed further away from this central cluster. This enables it to easily flag the outliers. This makes it an excellent model for finding global anomalies that are far from the main group.
"""

# Elliptic Envelope implementation

from sklearn.covariance import EllipticEnvelope
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize and fit the model
# We use the same contaimation and random_state value as used for the isolation forest.
ee_model = EllipticEnvelope(contamination=contamination_level, random_state=42)
ee_predictions = ee_model.fit_predict(features)

# Get the model's predictions (convert -1/1 to 1/0)
y_pred_ee = (ee_predictions == -1).astype(int)

# 3. Calculate and display performance metrics
accuracy_ee = accuracy_score(y_true, y_pred_ee)
precision_ee = precision_score(y_true, y_pred_ee)
recall_ee = recall_score(y_true, y_pred_ee)
f1_ee = f1_score(y_true, y_pred_ee)

print("\n--- Elliptic Envelope Performance ---")
print(f'Accuracy: {accuracy_ee:.2%}')
print(f'Precision: {precision_ee:.2%}')
print(f'Recall: {recall_ee:.2%}')
print(f'F1 Score: {f1_ee:.2%}')

"""# k-Nearest Neighbors (k-NN)

k-NN is a distance-based method. Unlike Elliptic Envelope, it makes no assumptions about the shape or distribution of your data. Its core idea is simple: anomalies are isolated from their neighbors.
For every single company in your dataset, the k-NN algorithm calculates the distance to its k closest neighbors. This distance is used as its anomaly score. A company that is far away from all its neighbors gets a high score.

k-NNs strength is to find local anomalies. When considering all the features for a fraudulent company, that combination might be unique and isolated from other normal companies.

Elliptic Envelope might miss this type of outlier if it falls within the general "shape" of the data, but k-NN will flag it because it has no close peers. This makes it powerful for finding companies with strange or unusual business patterns.
"""

# Implementation for k-NN

from sklearn.neighbors import NearestNeighbors
import numpy as np


# 1. Fit the k-NN model
# The n_neighbours determines the size of the local neighbours.
# Tried a few different values for it. But 5 gave the best result.
knn_model = NearestNeighbors(n_neighbors=5)
knn_model.fit(features)

# 2. Calculate anomaly scores
# Returns the distances to each of its 5 nearest neighbours.
distances, indices = knn_model.kneighbors(features)
# extracts only the distance to the 5th farthest neighbor. This distance becomes our anomaly score.
# A company with a high anomaly score is very isolated, meaning even its 5th closest neighbor is very far away.
# A normal company will be in a dense area and have a low score.
knn_scores = distances[:, -1]

# 3. Set a threshold to identify outliers
# Find the score value at the top percentile (determined by the contamination level)
threshold = np.quantile(knn_scores, 1 - contamination_level)

# Any company with a score greater than or equal to this threshold is an anomaly
knn_predictions = np.where(knn_scores >= threshold, -1, 1)

# 4. Get the model's predictions (convert -1/1 to 1/0)
y_pred_knn = (knn_predictions == -1).astype(int)

# 5. Calculate and display performance metrics
accuracy_knn = accuracy_score(y_true, y_pred_knn)
precision_knn = precision_score(y_true, y_pred_knn)
recall_knn = recall_score(y_true, y_pred_knn)
f1_knn = f1_score(y_true, y_pred_knn)

print("\n--- k-Nearest Neighbors Performance ---")
print(f'Accuracy: {accuracy_knn:.2%}')
print(f'Precision: {precision_knn:.2%}')
print(f'Recall: {recall_knn:.2%}')
print(f'F1 Score: {f1_knn:.2%}')

"""# Autoencoder

An Autoencoder is a neural network that learns to identify anomalies by first becoming an expert on normal data. Its trained exclusively on the normal company profiles, learning to compress them down to a small summary (encoding) and then accurately reconstruct them back to their original form (decoding).When the trained model is later shown a fraudulent company profile, its unfamiliar patterns prevent a proper reconstruction, resulting in a high reconstruction error. This error serves as a direct measure of suspicion: normal companies have a low error because the model knows them well, while fraudulent companies have a high error, effectively flagging them as anomalies.

It can learn sophisticated, non-linear relationships between the features (mismatch_count, total_revenue, etc.) that simpler models might miss.
Unlike Elliptic Envelope, it doesn't assume your data has a specific geometric shape. It learns the complex shape of your "normal" data on its own.
It creates a flexible, data-driven definition of what is "normal," which is often more accurate than a model based on stricter assumptions.
"""

!pip install tensorflow

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from sklearn.preprocessing import StandardScaler

# Define the features for the model
features_df = master_df[['avg_transaction_value', 'total_revenue', 'total_transactions',
                         'mismatch_count', 'mismatch_total_amount']]

# 1. Scale the features
# StandardScaler rescales every feature so that it has a mean of 0 and a standard deviation of 1.
# This puts all features on a level playing field, ensuring the model treats them with equal importance during training.
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_df)

# 2. Separate data for training (on normal companies only)
# The model will be trained only on the normal companies data - the ones which dont have a mismatch in the HSN codes, hence its true label showed one
# when we were making the labels for the isolation forest.
X_train = features_scaled[master_df['true_label'] == 0]

# The test data is all the companies tho- the fraudulent and the normal companies together
X_test = features_scaled
y_test = master_df['true_label'].values

print(f"Training data shape (normal companies only): {X_train.shape}")

# Define the dimensions
# This gets the number of features (columns) from your training data. The network's input layer must have one neuron for each feature.
input_dim = X_train.shape[1]

# Your model uses 5 input features. Choosing an encoding_dim of 3 is a significant compression that forces the model to learn a meaningful summary,
# but it likely still provides enough capacity to capture the essential patterns of your normal data.
encoding_dim = 3 # The size of the compressed representation

# Define the model architecture
input_layer = Input(shape=(input_dim,))

# The encoder's job is to take the input data and compress it down to the encoding_dim.

# This is the first hidden layer with 10 neurons. It takes the original company profile and starts finding patterns.
# The relu activation is a standard choice that helps the model learn complex relationships.
encoder = Dense(10, activation='relu')(input_layer)
# This is the "bottleneck" layer. It takes the 10-neuron representation from the previous layer and compresses it further into the final 3-neuron summary.
encoder = Dense(encoding_dim, activation='relu')(encoder)

# The decoder's job is to take the compressed 3-number summary and try to reconstruct the original, full company profile.

# This layer starts un-compressing the data, taking the 3-neuron summary and expanding it back out to 10 neurons.
decoder = Dense(10, activation='relu')(encoder)
# This is the final output layer. It takes the 10-neuron representation and expands it back to match the original number of features.
# The linear activation is used because you want the raw numerical output, not a classification.
decoder = Dense(input_dim, activation='linear')(decoder)

# Create and compile the autoencoder
autoencoder = Model(inputs=input_layer, outputs=decoder)
# Adam is an efficient and popular "engine" that helps the model learn and improve.
# This tells the model how to measure its own error. It will calculate the difference between the original company profile and its reconstructed copy,
# and its goal during training is to make this error as small as possible.
autoencoder.compile(optimizer='adam', loss='mean_squared_error')

autoencoder.summary()

# Train the model

# epochs=50 means the model will "see" and learn from all your normal company data a total of 50 times.
# Too few epochs could lead to an undertrained model, while too many could lead to overfitting

# Instead of processing the entire dataset at once, the model looks at the data in small chunks or "batches."
# batch_size=32 means the model will look at 32 companies, update its learning, and then move to the next 32. (32 a common default value)

# This automatically sets aside 10% of your training data to be used as a "validation set." The model does not train on this data.
# After each epoch, the model tests its performance on this validation set.
# It's a way to check if the model is actually learning general patterns or just memorizing the training data.
# It's a key tool for diagnosing a common problem called overfitting.
history = autoencoder.fit(
    X_train, X_train,
    epochs=50,
    batch_size=32,
    shuffle=True,
    validation_split=0.1,
    verbose=1
)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np

# Get the model's reconstructions of all data
reconstructions = autoencoder.predict(X_test)

# Calculate the reconstruction error (MSE) for each company
# A low score means the model made a near-perfect copy (a normal company), while a high score means the model struggled badly (a likely fraudulent company).
mse_scores = np.mean(np.power(X_test - reconstructions, 2), axis=1)

# Find a threshold to separate anomalies from normal data
contamination_level = master_df['true_label'].value_counts(normalize=True)[1]
threshold = np.quantile(mse_scores, 1 - contamination_level)

# Get predictions: if error is > threshold, it's a fraud (1)
y_pred_ae = (mse_scores > threshold).astype(int)

# --- Calculate and display all performance metrics ---
accuracy_ae = accuracy_score(y_test, y_pred_ae)
precision_ae = precision_score(y_test, y_pred_ae)
recall_ae = recall_score(y_test, y_pred_ae)
f1_ae = f1_score(y_test, y_pred_ae)

print("\n--- Autoencoder Performance ---")
print(f"Chosen Threshold: {threshold:.4f}")
print(f'Accuracy: {accuracy_ae:.2%}')
print(f'Precision: {precision_ae:.2%}')
print(f'Recall: {recall_ae:.2%}')
print(f'F1 Score: {f1_ae:.2%}')

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np

print("--- Generating Combined ROC Curve ---")

# --- 1. Get the Ground Truth Labels ---
y_true = master_df['true_label']

# --- 2. Get Anomaly Scores from Your Existing Models ---

# Isolation Forest scores (lower is more anomalous, so invert)
iso_scores = -iso_forest.decision_function(features_scaled)

# Elliptic Envelope scores (lower is more anomalous, so invert)
ee_scores = -ee_model.decision_function(features_scaled)

# k-NN scores (already calculated, higher is more anomalous)
# This assumes 'knn_scores' variable exists from your k-NN cell
# If not, recalculate it here.

# Autoencoder scores (higher reconstruction error is more anomalous)
reconstructions = autoencoder.predict(features_scaled)
ae_scores = np.mean(np.power(features_scaled - reconstructions, 2), axis=1)

# --- 3. Calculate ROC Curve and AUC for Each Model ---
fpr_iso, tpr_iso, _ = roc_curve(y_true, iso_scores)
auc_iso = auc(fpr_iso, tpr_iso)

fpr_ee, tpr_ee, _ = roc_curve(y_true, ee_scores)
auc_ee = auc(fpr_ee, tpr_ee)

fpr_knn, tpr_knn, _ = roc_curve(y_true, knn_scores)
auc_knn = auc(fpr_knn, tpr_knn)

fpr_ae, tpr_ae, _ = roc_curve(y_true, ae_scores)
auc_ae = auc(fpr_ae, tpr_ae)

# --- 4. Plot all curves on one graph for comparison ---
plt.figure(figsize=(12, 8))

plt.plot(fpr_iso, tpr_iso, lw=2, label=f'Isolation Forest (AUC = {auc_iso:.2f})')
plt.plot(fpr_ae, tpr_ae, lw=2, label=f'Autoencoder (AUC = {auc_ae:.2f})')
plt.plot(fpr_knn, tpr_knn, lw=2, label=f'k-NN (AUC = {auc_knn:.2f})')
plt.plot(fpr_ee, tpr_ee, lw=2, label=f'Elliptic Envelope (AUC = {auc_ee:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (Recall)')
plt.title('Comparison of ROC Curves for Anomaly Detection Models')
plt.legend(loc="lower right")
plt.grid(True)
plt.savefig('roc_curve_comparison.png')
plt.show()